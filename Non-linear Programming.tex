\documentclass[math]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Non-linear Programming
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Non-linear Programming Problems}
\section{Basic Terminology and Notations}
\begin{dfnbox}{General Non-linear Programming (NLP) Problems}{geNLPProb}
    Define the function $f \colon \mathbf{R}^n \to \mathbf{R}$. Let $\mathbfit{x} \in \mathbf{R}^n$ be a vector, then a general NLP problem aims to {\color{red} \textbf{optimise}} (i.e. maximise or minimise) $f(\mathbfit{x})$ subject to the constraint $\mathbfit{x} \in S \subseteq \mathbf{R}^n$, where
    \begin{itemize}
        \item $f$ is known as the {\color{red} \textbf{objective function}};
        \item $S$ is known as the {\color{red} \textbf{feasible set}};
        \item A solution (point) $\mathbfit{x} \in S$ is known as a {\color{red} \textbf{feasible solution (point)}}. Otherwise, it is known as an {\color{red} \textbf{infeasible solution(point)}}. 
    \end{itemize}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that to maximise $f(\mathbf{x})$ is equivalent to minimising $-f(\mathbf{x})$, so it suffices to only study minimisation problems.
    \end{remark}
\end{notebox}
The word ``optimal'', however, can be ambiguous due to its qualitative nature. Thus, we shall define what it means to be optimal quantitatively with more rigorous terms.
\begin{dfnbox}{Optimal Solution}{optSoln}
    Consider a minimisation problem subject to constraint $\mathbfit{x} \in S \subseteq \mathbfit{R}^n$ whose objective function is $f(\mathbfit{x})$. A feasible solution $\mathbfit{x}^*$ is called an {\color{red} \textbf{optimal solution}} if $f(\mathbfit{x}^*) \leq f(\mathbfit{x})$ for all $\mathbfit{x} \in S$. We can write
    \begin{equation*}
        \mathbfit{x}^* = \argmin_{\mathbfit{x} \in S} f(\mathbfit{x}).
    \end{equation*}
    $f(\mathbfit{x}^*)$ is then known as the {\color{red} \textbf{optimal value}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        For maximisation problems, we can write
        \begin{equation*}
            \mathbfit{x}^* = \argmax_{\mathbfit{x} \in S} f(\mathbfit{x})
        \end{equation*}
    \end{remark}
\end{notebox}
Note that not all optimisation problems have an optimal solution. We shall still expect to encounter problems for which no optimal solution nor value exists.
\begin{dfnbox}{Unboundedness}{nboundedness}
    Consider a minimisation problem subject to constraint $\mathbfit{x} \in S \subseteq \mathbfit{R}^n$ whose objective function is $f(\mathbfit{x})$. The objective value is said to be {\color{red} \textbf{unbounded}} if for all $K \in \mathbf{R}$, there exists some $\mathbfit{x} \in S$ such that $f(\mathbfit{x}) < K$.
\end{dfnbox}
\section{Unconstrained Non-linear Programs}
To introduce the notion of an unconstrained NLP, we shall first define the openness of a set.
\begin{dfnbox}{Open Set}{openSet}
    Let $S \subseteq \mathbfit{R}^n$ be a set. $S$ is called {\color{red} \textbf{open}} if for all $\mathbfit{x} \in S$ there exists $\epsilon > 0$ such that the ball
    \begin{equation*}
        B(\mathbfit{x}, \epsilon) \coloneqq \left\{\mathbfit{y} \in \mathbfit{R}^n \colon \left\lVert \mathbfit{y - x} \right\rVert < \epsilon\right\}
    \end{equation*}
    is a subset of $S$.
\end{dfnbox}
\begin{dfnbox}{Unconstrained NLP}{unconstrainedNLP}
    An {\color{red} \textbf{unconstrained}} NLP is an NLP whose feasible set $\mathcal{X}$ is an {\color{red} \textbf{open}} subset of $\mathbf{R}^n$.
\end{dfnbox}
\section{Constrained Non-linear Programs}
Similarly, to introduce the notion of a constrained NLP, we shall first define the closed-ness of a set.
\begin{dfnbox}{Closed Set}{closedSet}
    Let $S \subseteq \mathbfit{R}^n$ be a non-empty set. $S$ is said to be {\color{red} \textbf{closed}} if for all convergent sequences $\{\mathbfit{x}_i\}_{i = 1}^{\infty}$ with $\mathbfit{x}_i \in S$ for $i = 1, 2, \cdots$, the limit $\lim_{i \to \infty} \mathbfit{x}_i \in S$.
\end{dfnbox}
The empty set and Euclidean spaces $\R^n$ are both open and closed.
\begin{notebox}
    \begin{remark}
        Note that a set which is not open may not necessarily be closed. However, a set is open if and only if its complement is closed.
    \end{remark}
\end{notebox}
\begin{thmbox}{Intersection of Closed Sets}{intersecClosed}
    If $C_1$ and $C_2$ are both closed, then $C_1 \cap C_2$ is closed.
    \tcblower
    \begin{proof}
        The case where $C_1 \cap C_2 = \varnothing$ is trivial. If $C_1 \cap C_2 \neq \varnothing$, let $\{\mathbfit{x}_i\}_{i = 1}^{\infty}$ be an arbitrary convergent sequence in $C_1 \cap C_2$. Since $\{\mathbfit{x}_i\}_{i = 1}^{\infty} \in C_1$ which is closed, we have $\lim_{i \to \infty} \mathbfit{x}_i \in C_1$. Similarly, $\lim_{i \to \infty} \mathbfit{x}_i \in C_2$. Therefore, $\lim_{i \to \infty} \mathbfit{x}_i \in C_1 \cap C_2$.
        \\\\
        Therefore, $C_1 \cap C_2$ is closed.
    \end{proof}
\end{thmbox}
We then follow up by introducing three important closed sets.
\begin{thmbox}{}{}
    Let $g \colon \mathbf{R}^n \to \mathbf{R}$ be a continuous function, then the sets
    \begin{align*}
        S_1 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) \leq 0\right\}, \\
        S_2 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) \geq 0\right\}, \\
        S_3 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) = 0\right\}
    \end{align*}
    are closed.
    \tcblower
    \begin{proof}
        Consider $S_1$. Let $\left\{\mathbfit{x}_i\right\}_{i = 1}^\infty$ be any convergent sequence with $\mathbfit{x}_i \in S_1$ for $i = 1, 2, \cdots$, then
        \begin{equation*}
            g\left(\lim_{i \to \infty}\mathbfit{x}_i\right) \leq 0
        \end{equation*}
        since $\mathbfit{x}_i \leq 0$. Therefore, $\lim_{i \to \infty}\mathbfit{x}_i \in S_1$ and so $S_1$ is closed.

        $S_2$ and $S_3$ can be proved similarly.
    \end{proof}
\end{thmbox}
By Theorem \ref{thm:intersecClosed}, we know that $S_1 \cup S_2 \cup S_3$ is closed, which motivates the following definition:
\begin{dfnbox}{Constrained NLP}{constrainedNLP}
    A {\color{red} \textbf{constrained}} NLP is an NLP whose feasible set
    \begin{displaymath}
        S \coloneqq \left\{\mathbfit{x} \in \mathbf{R}^n \colon g_i(\mathbfit{x}) = 0, i = 1, 2, \cdots, p, h_j(\mathbfit{x}) \leq 0, j = 1, 2, \cdots, q\right\}
    \end{displaymath}
    is {\color{red} \textbf{closed}}, where each of the $g_i$'s is known as an equality constraint and each of the $h_j$'s is known as an inequality constraint.
\end{dfnbox}

\chapter{Convex Functions}
\section{Convexity of Sets and Functions}
Intuitively, we describe two types of shapes in natural languages: the shapes which, if you choose any of its edges, lies in the same side of that edge, and the shapes which span across both sides from some chosen edge of its.

Graphically, this means that some shapes are ``convex'' to all directions, where as some other shapes are ``concave''. We shall define this rigorously as follows:
\begin{dfnbox}{Convex Set}{convexSet}
    A set $D \subseteq \mathbf{R}^n$ is said to be {\color{red} \textbf{convex}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        \lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y} \in D.
    \end{displaymath}
\end{dfnbox}
We can define convexity over functions as follows:
\begin{dfnbox}{Convex Function}{convexFunc}
    A function $f \colon D \to \mathbf{R}^n$ is said to be {\color{red} \textbf{convex}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        f\left(\lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}).
    \end{displaymath}
\end{dfnbox}
\begin{dfnbox}{Concave Function}{concaveFunc}
    A function $f \colon D \to \mathbf{R}^n$ is said to be {\color{red} \textbf{concave}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        f\left(\lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) \geq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}).
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A function which is not convex must be concave. However, a function which is convex may not be non-concave (consider $f(x) = x$).
    \end{remark}
\end{notebox}
We may derive the following relationship between a convex set and a convex function:
\begin{probox}{Relations between Convex Sets and Convex Functions}{cSetRCFunc}
    Let $D \subseteq \mathbf{R}^n$ be a convex set and let $f \colon D \to \mathbf{R}$ be a convex function, then for all $\alpha \in \mathbf{R}$, the set
    \begin{displaymath}
        S_\alpha \coloneqq \left\{\mathbfit{x} \in D \colon f(\mathbfit{x}) \leq \alpha\right\}
    \end{displaymath}
    is convex.
    \tcblower   
    \begin{proof}
        Take $\mathbfit{x}, \mathbfit{y} \in S_\alpha$, then $f(\mathbfit{x}) \leq \alpha$ and $f(\mathbfit{y}) \leq \alpha$. Note that for any $\lambda \in [0, 1]$, we have
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}) \\
            & \leq \lambda\alpha + (1 - \lambda)\alpha \\
            & = \alpha.
        \end{align*}
        Therefore, $\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y} \in S_\alpha$, and so $S_\alpha$ is convex.
    \end{proof}
\end{probox}
Next, we introduce the notion of an \textit{epigraph}.
\begin{dfnbox}{Epigraph}{epigraph}
    Let $f \colon D \to \mathbf{R}$ be a function over a convex set $D \subseteq \mathbf{R}^n$. The {\color{red} \textbf{epigraph}} of $f$ is the set~$E_f \subseteq \mathbf{R}^{n + 1}$ defined by
    \begin{displaymath}
        E_f \coloneqq \left\{(\mathbfit{x}, \alpha) \colon \mathbfit{x} \in D, \alpha \in \mathbf{R}, f(\mathbfit{x}) \leq \alpha\right\}.
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A trivial result: $D \times \mathrm{range}(f) \subseteq E_f$.
    \end{remark}
\end{notebox}
Note that graphically, the epigraph of a function is just the region above the graph of the function.
\begin{probox}{Convexity of Epigraph}{convexEpi}
    Let $f \colon D \to \mathbf{R}$ be a function over a convex set $D \subseteq \mathbf{R}^n$. The epigraph $E_f$ is convex if and only if $f$ is convex.
    \tcblower   
    \begin{proof}
        Suppose $E_f$ is convex. Take any $\mathbfit{x}, \mathbfit{y} \in D$, then $(\mathbfit{x}, f(\mathbfit{x})), (\mathbfit{y}, f(\mathbfit{y})) \in E_f$. Let $\lambda \in [0, 1]$, we have
        \begin{displaymath}
            \lambda(\mathbfit{x}, f(\mathbfit{x})) + (1 - \lambda)(\mathbfit{y}, f(\mathbfit{y})) = (\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y})) \in E_f.
        \end{displaymath}
        Therefore, 
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}),
        \end{align*}
        and so $f$ is convex.
        \\\\
        Suppose conversely that $f$ is convex. For any $\mathbfit{x}, \mathbfit{y} \in D$ and any $\alpha, \beta \in \mathbf{R}$ such that $f(\mathbfit{x}) \leq \alpha$ and $f(\mathbfit{y}) \leq \beta$, we have $(\mathbfit{x}, \alpha), (\mathbfit{y}, \beta) \in E_f$. For all $\lambda \in [0, 1]$, consider
        \begin{align*}
            \lambda(\mathbfit{x}, \alpha) + (1 - \lambda)(\mathbfit{y}, \beta) = (\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda\alpha + (1 - \lambda)\beta).
        \end{align*}
        Since $f$ is convex, we have
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}) \\
            & \leq \lambda\alpha + (1 - \lambda)\beta
        \end{align*}
        for all $\lambda \in [0, 1]$. Therefore, $(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda\alpha + (1 - \lambda)\beta) \in E_f$, and so $E_f$ is convex.
    \end{proof}
\end{probox}
Lastly, we generalise the notion of a \textit{convex combination}.
\begin{probox}{Generalised Convex Combination}{convexCombi}
    Let $k \in \N^+$ and let $f \colon S \to \mathbf{R}$ be a convex function on the convex set $S \subseteq \mathbf{R}^n$ and let $\mathbfit{x}_1, \mathbfit{x}_2, \cdots, \mathbfit{x}_k \in S$, then 
    \begin{equation*}       
        f\left(\sum_{i = 1}^{k}\lambda_i\mathbfit{x}_i\right) \leq \sum_{i = 1}^{k}\lambda_i f(\mathbfit{x}_i),
    \end{equation*}     
    where $\sum_{i = 1}^{k}\lambda_i = 1$ and $\lambda_i \geq 0$ for $i = 1, 2, \cdots, k$.
    \tcblower
    \begin{proof}
        The case where $k = 1$ is trivial.
        \\\\
        Suppose that there exists some $n \in \N^+$ such that
        \begin{equation*}
            f\left(\sum_{i = 1}^{n}\lambda_i\mathbfit{x}_i\right) \leq \sum_{i = 1}^{n}\lambda_i f(\mathbfit{x}_i),
        \end{equation*}
    \end{proof}
\end{probox}
\section{Gradient Vectors}
\begin{probox}{An Alternative Expression for Directional Derivatives}{directionalDerivative}
    Let $f$ be a function over $D \subseteq \mathbf{R}^n$ and let $\mathbfit{d} \in \mathbf{R}^n$ be non-zero, then
    \begin{equation*}
        \nabla f(\mathbfit{x})^{\mathrm{T}}\mathbfit{d} = \lim_{\lambda \to 0}\frac{f(\mathbfit{x + \lambda\mathbfit{d}}) - f(\mathbfit{x})}{\lambda}.
    \end{equation*}
\end{probox}
\begin{probox}{Tangent Plane Characterisation of Convex Functions}{convexTan}
    Let $f$ be a function over an open convex set $S \subseteq \mathbf{R}^n$ with continuous first partial derivatives, then $f$ is convex if and only if 
    \begin{equation*}
        f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y})
    \end{equation*}
    for all $\mathbfit{x}, \mathbfit{y} \in S$. In particular, $f$ is strictly convex if and only if the above inequality is strict.
\end{probox}
\begin{probox}{Global Minimiser of Convex Functions}{convexFuncMin}
    Let $f \colon C \to \mathbf{R}$ be a convex and continuously differentiable function over a convex set~$C \subseteq \mathbf{R}^n$. Then $\mathbfit{x}^* \in C$ is a global minimiser for the minimisation problem 
    \begin{displaymath}
        \min\left\{f(\mathbfit{x}) \colon \mathbfit{x} \in C\right\}
    \end{displaymath}
    if and only if
    \begin{equation*}
        \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \geq 0
    \end{equation*}
    for all $\mathbfit{x} \in C$.
\end{probox}
\chapter{Unconstrained NLPs}
\section{Coercive Functions}
\begin{dfnbox}{Coercive Function}{coerciveFunc}
    A {\color{red} \textbf{continuous}} function $f \colon \mathbf{R}^n \to \mathbf{R}$ is said to be {\color{red} \textbf{coercive}} if
    \begin{equation*}
        \lim_{\norm{\mathbfit{x}} \to \infty} f(\mathbfit{x}) = +\infty.
    \end{equation*}
    More formally, $f$ is coercive if and only if for all $M > 0$, there is some $r > 0$ such that~$f(\mathbfit{x}) > M$ whenever $\norm{\mathbfit{x}} > r$.
\end{dfnbox}
\begin{thmbox}{Global Minimiser of Coercive Functions}{globalMinCoerciveFunc}
    If a function $f \colon \mathbf{R}^n \to \mathbf{R}$ is coercive, then $f$ has at least one global minimiser.
    \tcblower   
    \begin{proof}
        Take $M = \abs{f(\mathbfit{0})} + 1 > 0$. Since $f$ is coercive, there exists some $r > 0$ such that $f(\mathbfit{x}) > M > f(\mathbfit{0})$ whenever $\norm{\mathbfit{x}} > r$. Consider
        \begin{displaymath}
            B(\mathbfit{0}, r) = \left\{\mathbfit{x} \in \mathbf{R}^n \colon \norm{\mathbfit{x}} \leq r\right\}
        \end{displaymath}
        which is a compact set. So by Weiestrass' Theorem, there exists some $\mathbfit{x}^* \in B(\mathbfit{0}, r)$ such that $f(\mathbfit{x}^*) \leq f(\mathbfit{x}) \leq f(\mathbfit{0})$ for all $\mathbfit{x} \in B(\mathbfit{0}, r)$. Now, this means that for all $\mathbfit{x} \in \mathbf{R}^n$, we have $(\mathbfit{x}^*) \leq f(\mathbfit{x})$, which means that $\mathbfit{x}^*$ is a global minimiser for $f$.
    \end{proof}
\end{thmbox}
\begin{dfnbox}{Stationary (Critical) Point}{stationaryPt}
    Let $X \subseteq \mathbf{R}^n$ be an open set and let $f \colon X \to \mathbf{R}$ be a function. An interior point $\mathbfit{x}^*$ is called a {\color{red} \textbf{stationary point}} of $f$ if $\nabla f(\mathbfit{x}^*) = \mathbfit{0}$.
\end{dfnbox}
\begin{dfnbox}{Saddle Point}{Saddle}
    A stationary point $\mathbfit{x}^*$ of a function $f$ which is neither a local minimisr nor a local maximiser is called a {\color{red} \textbf{saddle point}}.
\end{dfnbox}
\begin{corbox}{}{saddleCor}
    Let $\mathbfit{x}^*$ be a stationary point of $f$. If $H_f(\mathbfit{x}^*)$ is indefinite, then $\mathbfit{x}^*$ is a saddle point.
\end{corbox}
\end{document}