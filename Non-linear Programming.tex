\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Non-linear Programming
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Non-linear Programming Problems}
\section{Basic Terminology and Notations}
\begin{dfnbox}{General Non-linear Programming (NLP) Problems}{geNLPProb}
    Define the function $f \colon \mathbf{R}^n \to \mathbf{R}$. Let $\mathbfit{x} \in \mathbf{R}^n$ be a vector, then a general NLP problem aims to {\color{red} \textbf{optimise}} (i.e. maximise or minimise) $f(\mathbfit{x})$ subject to the constraint $\mathbfit{x} \in S \subseteq \mathbf{R}^n$, where
    \begin{itemize}
        \item $f$ is known as the {\color{red} \textbf{objective function}};
        \item $S$ is known as the {\color{red} \textbf{feasible set}};
        \item A solution (point) $\mathbfit{x} \in S$ is known as a {\color{red} \textbf{feasible solution (point)}}. Otherwise, it is known as an {\color{red} \textbf{infeasible solution(point)}}. 
    \end{itemize}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that to maximise $f(\mathbf{x})$ is equivalent to minimising $-f(\mathbf{x})$, so it suffices to only study minimisation problems.
    \end{remark}
\end{notebox}
The word ``optimal'', however, can be ambiguous due to its qualitative nature. Thus, we shall define what it means to be optimal quantitatively with more rigorous terms.
\begin{dfnbox}{Optimal Solution}{optSoln}
    Consider a minimisation problem subject to constraint $\mathbfit{x} \in S \subseteq \mathbfit{R}^n$ whose objective function is $f(\mathbfit{x})$. A feasible solution $\mathbfit{x}^*$ is called an {\color{red} \textbf{optimal solution}} if $f(\mathbfit{x}^*) \leq f(\mathbfit{x})$ for all $\mathbfit{x} \in S$. We can write
    \begin{equation*}
        \mathbfit{x}^* = \argmin_{\mathbfit{x} \in S} f(\mathbfit{x}).
    \end{equation*}
    $f(\mathbfit{x}^*)$ is then known as the {\color{red} \textbf{optimal value}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        For maximisation problems, we can write
        \begin{equation*}
            \mathbfit{x}^* = \argmax_{\mathbfit{x} \in S} f(\mathbfit{x})
        \end{equation*}
    \end{remark}
\end{notebox}
Note that not all optimisation problems have an optimal solution. We shall still expect to encounter problems for which no optimal solution nor value exists.
\begin{dfnbox}{Unboundedness}{nboundedness}
    Consider a minimisation problem subject to constraint $\mathbfit{x} \in S \subseteq \mathbfit{R}^n$ whose objective function is $f(\mathbfit{x})$. The objective value is said to be {\color{red} \textbf{unbounded}} if for all $K \in \mathbf{R}$, there exists some $\mathbfit{x} \in S$ such that $f(\mathbfit{x}) < K$.
\end{dfnbox}
\section{Unconstrained Non-linear Programs}
To introduce the notion of an unconstrained NLP, we shall first define the openness of a set.
\begin{dfnbox}{Open Set}{openSet}
    Let $S \subseteq \mathbfit{R}^n$ be a set. $S$ is called {\color{red} \textbf{open}} if for all $\mathbfit{x} \in S$ there exists $\epsilon > 0$ such that the ball
    \begin{equation*}
        B(\mathbfit{x}, \epsilon) \coloneqq \left\{\mathbfit{y} \in \mathbfit{R}^n \colon \left\lVert \mathbfit{y - x} \right\rVert < \epsilon\right\}
    \end{equation*}
    is a subset of $S$.
\end{dfnbox}
\begin{dfnbox}{Unconstrained NLP}{unconstrainedNLP}
    An {\color{red} \textbf{unconstrained}} NLP is an NLP whose feasible set $\mathcal{X}$ is an {\color{red} \textbf{open}} subset of $\mathbf{R}^n$.
\end{dfnbox}
\section{Constrained Non-linear Programs}
Similarly, to introduce the notion of a constrained NLP, we shall first define the closed-ness of a set.
\begin{dfnbox}{Closed Set}{closedSet}
    Let $S \subseteq \mathbfit{R}^n$ be a non-empty set. $S$ is said to be {\color{red} \textbf{closed}} if for all convergent sequences $\{\mathbfit{x}_i\}_{i = 1}^{\infty}$ with $\mathbfit{x}_i \in S$ for $i = 1, 2, \cdots$, the limit $\lim_{i \to \infty} \mathbfit{x}_i \in S$.
\end{dfnbox}
The empty set and Euclidean spaces $\R^n$ are both open and closed.
\begin{notebox}
    \begin{remark}
        Note that a set which is not open may not necessarily be closed. However, a set is open if and only if its complement is closed.
    \end{remark}
\end{notebox}
\begin{thmbox}{Intersection of Closed Sets}{intersecClosed}
    If $C_1$ and $C_2$ are both closed, then $C_1 \cap C_2$ is closed.
    \tcblower
    \begin{proof}
        The case where $C_1 \cap C_2 = \varnothing$ is trivial. If $C_1 \cap C_2 \neq \varnothing$, let $\{\mathbfit{x}_i\}_{i = 1}^{\infty}$ be an arbitrary convergent sequence in $C_1 \cap C_2$. Since $\{\mathbfit{x}_i\}_{i = 1}^{\infty} \in C_1$ which is closed, we have $\lim_{i \to \infty} \mathbfit{x}_i \in C_1$. Similarly, $\lim_{i \to \infty} \mathbfit{x}_i \in C_2$. Therefore, $\lim_{i \to \infty} \mathbfit{x}_i \in C_1 \cap C_2$.
        \\\\
        Therefore, $C_1 \cap C_2$ is closed.
    \end{proof}
\end{thmbox}
We then follow up by introducing three important closed sets.
\begin{probox}{}{}
    Let $g \colon \mathbf{R}^n \to \mathbf{R}$ be a continuous function, then the sets
    \begin{align*}
        S_1 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) \leq 0\right\}, \\
        S_2 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) \geq 0\right\}, \\
        S_3 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) = 0\right\}
    \end{align*}
    are closed.
    \tcblower
    \begin{proof}
        Consider $S_1$. Let $\left\{\mathbfit{x}_i\right\}_{i = 1}^\infty$ be any convergent sequence with $\mathbfit{x}_i \in S_1$ for $i = 1, 2, \cdots$, then
        \begin{equation*}
            g\left(\lim_{i \to \infty}\mathbfit{x}_i\right) \leq 0
        \end{equation*}
        since $\mathbfit{x}_i \leq 0$. Therefore, $\lim_{i \to \infty}\mathbfit{x}_i \in S_1$ and so $S_1$ is closed.

        $S_2$ and $S_3$ can be proved similarly.
    \end{proof}
\end{probox}
By Theorem \ref{thm:intersecClosed}, we know that $S_1 \cup S_2 \cup S_3$ is closed, which motivates the following definition:
\begin{dfnbox}{Constrained NLP}{constrainedNLP}
    A {\color{red} \textbf{constrained}} NLP is an NLP whose feasible set
    \begin{displaymath}
        S \coloneqq \left\{\mathbfit{x} \in \mathbf{R}^n \colon g_i(\mathbfit{x}) = 0, i = 1, 2, \cdots, p, h_j(\mathbfit{x}) \leq 0, j = 1, 2, \cdots, q\right\}
    \end{displaymath}
    is {\color{red} \textbf{closed}}, where each of the $g_i$'s is known as an equality constraint and each of the $h_j$'s is known as an inequality constraint.
\end{dfnbox}
If the feasible set of a constraind NLP is \textit{bounded}, then it will be easy to find an optimal solution.
\begin{dfnbox}{Boundedness}{bounded}
    Let $S \subseteq \mathbf{R}^n$. $S$ is said to be {\color{red} \textbf{bounded}} if there exists some $M \in \R^+$ such that $\norm{\mathbfit{x}} \leq M$ for all $\mathbfit{x} \in S$.
\end{dfnbox}
Note that a closed set may not be bounded and a bounded set may not be closed, so we are motivated to define a \textit{closed and bounded set} rigorously.
\begin{dfnbox}{Compact Set}{compactSet}
    A set $S \subseteq \mathbf{R}^n$ is {\color{red} \textbf{compact}} if it is closed and bounded.
\end{dfnbox}
It turns out that, on a compact feasible set, the existence of global minimiser is guaranteed!
\begin{thmbox}{Weiestrass Theorem}{weiestrass}
    Let $f$ be a continuous function on some non-empty $S \subseteq \mathbf{R}^n$. If $S$ is compact, then $f$ has a global minimiser and a global maximiser.
\end{thmbox}
\chapter{Convex Functions}
\section{Convexity of Sets and Functions}
Intuitively, we describe two types of shapes in natural languages: the shapes which, if you choose any of its edges, lies in the same side of that edge, and the shapes which span across both sides from some chosen edge of its.

Graphically, this means that some shapes are ``convex'' to all directions, where as some other shapes are ``concave''. We shall define this rigorously as follows:
\begin{dfnbox}{Convex Set}{convexSet}
    A set $D \subseteq \mathbf{R}^n$ is said to be {\color{red} \textbf{convex}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        \lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y} \in D.
    \end{displaymath}
\end{dfnbox}
We can define convexity over functions as follows:
\begin{dfnbox}{Convex Function}{convexFunc}
    A function $f \colon D \to \mathbf{R}^n$ is said to be {\color{red} \textbf{convex}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        f\left(\lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}).
    \end{displaymath}
\end{dfnbox}
\begin{dfnbox}{Concave Function}{concaveFunc}
    A function $f \colon D \to \mathbf{R}^n$ is said to be {\color{red} \textbf{concave}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        f\left(\lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) \geq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}).
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A function which is not convex must be concave. However, a function which is convex may not be non-concave (consider $f(x) = x$).
    \end{remark}
\end{notebox}
We may derive the following relationship between a convex set and a convex function:
\begin{probox}{Relations between Convex Sets and Convex Functions}{cSetRCFunc}
    Let $D \subseteq \mathbf{R}^n$ be a convex set and let $f \colon D \to \mathbf{R}$ be a convex function, then for all $\alpha \in \mathbf{R}$, the set
    \begin{displaymath}
        S_\alpha \coloneqq \left\{\mathbfit{x} \in D \colon f(\mathbfit{x}) \leq \alpha\right\}
    \end{displaymath}
    is convex.
    \tcblower   
    \begin{proof}
        Take $\mathbfit{x}, \mathbfit{y} \in S_\alpha$, then $f(\mathbfit{x}) \leq \alpha$ and $f(\mathbfit{y}) \leq \alpha$. Note that for any $\lambda \in [0, 1]$, we have
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}) \\
            & \leq \lambda\alpha + (1 - \lambda)\alpha \\
            & = \alpha.
        \end{align*}
        Therefore, $\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y} \in S_\alpha$, and so $S_\alpha$ is convex.
    \end{proof}
\end{probox}
Next, we introduce the notion of an \textit{epigraph}.
\begin{dfnbox}{Epigraph}{epigraph}
    Let $f \colon D \to \mathbf{R}$ be a function over a convex set $D \subseteq \mathbf{R}^n$. The {\color{red} \textbf{epigraph}} of $f$ is the set~$E_f \subseteq \mathbf{R}^{n + 1}$ defined by
    \begin{displaymath}
        E_f \coloneqq \left\{(\mathbfit{x}, \alpha) \colon \mathbfit{x} \in D, \alpha \in \mathbf{R}, f(\mathbfit{x}) \leq \alpha\right\}.
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A trivial result: $D \times \mathrm{range}(f) \subseteq E_f$.
    \end{remark}
\end{notebox}
Note that graphically, the epigraph of a function is just the region above the graph of the function.
\begin{probox}{Convexity of Epigraph}{convexEpi}
    Let $f \colon D \to \mathbf{R}$ be a function over a convex set $D \subseteq \mathbf{R}^n$. The epigraph $E_f$ is convex if and only if $f$ is convex.
    \tcblower   
    \begin{proof}
        Suppose $E_f$ is convex. Take any $\mathbfit{x}, \mathbfit{y} \in D$, then $(\mathbfit{x}, f(\mathbfit{x})), (\mathbfit{y}, f(\mathbfit{y})) \in E_f$. Let $\lambda \in [0, 1]$, we have
        \begin{displaymath}
            \lambda(\mathbfit{x}, f(\mathbfit{x})) + (1 - \lambda)(\mathbfit{y}, f(\mathbfit{y})) = (\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y})) \in E_f.
        \end{displaymath}
        Therefore, 
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}),
        \end{align*}
        and so $f$ is convex.
        \\\\
        Suppose conversely that $f$ is convex. For any $\mathbfit{x}, \mathbfit{y} \in D$ and any $\alpha, \beta \in \mathbf{R}$ such that $f(\mathbfit{x}) \leq \alpha$ and $f(\mathbfit{y}) \leq \beta$, we have $(\mathbfit{x}, \alpha), (\mathbfit{y}, \beta) \in E_f$. For all $\lambda \in [0, 1]$, consider
        \begin{align*}
            \lambda(\mathbfit{x}, \alpha) + (1 - \lambda)(\mathbfit{y}, \beta) = (\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda\alpha + (1 - \lambda)\beta).
        \end{align*}
        Since $f$ is convex, we have
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}) \\
            & \leq \lambda\alpha + (1 - \lambda)\beta
        \end{align*}
        for all $\lambda \in [0, 1]$. Therefore, $(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda\alpha + (1 - \lambda)\beta) \in E_f$, and so $E_f$ is convex.
    \end{proof}
\end{probox}
Lastly, we generalise the notion of a \textit{convex combination}.
\begin{probox}{Generalised Convex Combination}{convexCombi}
    Let $k \in \N^+$ and let $f \colon S \to \mathbf{R}$ be a convex function on the convex set $S \subseteq \mathbf{R}^n$ and let $\mathbfit{x}_1, \mathbfit{x}_2, \cdots, \mathbfit{x}_k \in S$, then 
    \begin{equation*}       
        f\left(\sum_{i = 1}^{k}\lambda_i\mathbfit{x}_i\right) \leq \sum_{i = 1}^{k}\lambda_i f(\mathbfit{x}_i),
    \end{equation*}     
    where $\sum_{i = 1}^{k}\lambda_i = 1$ and $\lambda_i \geq 0$ for $i = 1, 2, \cdots, k$.
    \tcblower
    \begin{proof}
        The case where $k = 1$ is trivial.
        \\\\
        Suppose that there exists some $n \in \N^+$ such that
        \begin{equation*}
            f\left(\sum_{i = 1}^{n}\lambda_i\mathbfit{x}_i\right) \leq \sum_{i = 1}^{n}\lambda_i f(\mathbfit{x}_i),
        \end{equation*}
    \end{proof}
\end{probox}
\section{Tangent Plane Characterisation}
Recall that for a function $f \colon \mathbf{R}^n \to \mathbf{R}$, the \textit{gradient vector} of $f$ at $\mathbf{x}$ is given by
\begin{equation*}
    \nabla f(\mathbf{x}) = \begin{bmatrix}
        \frac{\partial}{\partial x_1}f(\mathbfit{x}) \\
        \frac{\partial}{\partial x_2}f(\mathbfit{x}) \\
        \vdots \\
        \frac{\partial}{\partial x_n}f(\mathbfit{x}) \\
    \end{bmatrix}.
\end{equation*}
\begin{probox}{Directional Derivative}{directionalDerivative}
    Let $f$ be a function over $D \subseteq \mathbf{R}^n$ and let $\mathbfit{d} \in \mathbf{R}^n$ be non-zero, then the directional derivative of $f$ at $\mathbfit{x}$ along $\mathbfit{d}$ is
    \begin{equation*}
        \nabla f(\mathbfit{x})^{\mathrm{T}}\mathbfit{d} = \lim_{\lambda \to 0}\frac{f(\mathbfit{x} + \lambda\mathbfit{d}) - f(\mathbfit{x})}{\lambda}.
    \end{equation*}
    In particular, if $\norm{\mathbf{d}} = 1$, then the above gives the rate of change of $f$ in the direction of $\mathbfit{d}$.
\end{probox}
\begin{notebox}
    \begin{remark}
        $f$ increases the fastest along the direction of $\nabla f(\mathbfit{x})$ and decreases the fastest along the direction of $-\nabla f(\mathbfit{x})$ for any $\mathbfit{x} \in \mathbf{R}^n$.
    \end{remark}
\end{notebox}
The gradient vector of a function allows us to establish its tangent plane at a given point. Intuitively, we can see that a function is convex if its tangent plane lies below its graph. This can be described rigorously as follows:
\begin{probox}{Tangent Plane Characterisation of Convex Functions}{convexTan}
    Let $f$ be a function over an open convex set $S \subseteq \mathbf{R}^n$ with continuous first partial derivatives, then $f$ is convex if and only if 
    \begin{equation*}
        f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y})
    \end{equation*}
    for all $\mathbfit{x}, \mathbfit{y} \in S$. In particular, $f$ is strictly convex if and only if the above inequality is strict.
    \tcblower   
    \begin{proof}
        Suppose that $f$ is convex. Let $\mathbfit{x}, \mathbfit{y} \in S$ and let $\lambda \in [0, 1]$, then we have
        \begin{equation*}
            f\left(\mathbfit{x} + \lambda(\mathbfit{y} - \mathbfit{x})\right) = f\left(\lambda\mathbfit{y} + (1 - \lambda)\mathbfit{x}\right) \leq \lambda f(\mathbfit{y}) + (1 - \lambda)f(\mathbfit{x}).
        \end{equation*} 
        Therefore,
        \begin{equation*}
            \frac{f\left(\mathbfit{x} + \lambda(\mathbfit{y} - \mathbfit{x})\right) - f(\mathbfit{x})}{\lambda} \leq f(\mathbfit{y}) - f(\mathbfit{x}).
        \end{equation*}
        Taking the limit as $\lambda \to 0$ on both sides, we have
        \begin{equation*}
            \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) = \lim_{\lambda \to 0}\frac{f\left(\mathbfit{x} + \lambda(\mathbfit{y} - \mathbfit{x})\right) - f(\mathbfit{x})}{\lambda} \leq f(\mathbfit{y}) - f(\mathbfit{x}),
        \end{equation*}
        and so
        \begin{equation*}
            f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y}).
        \end{equation*}
        Suppose conversely that 
        \begin{equation*}
            f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y}).
        \end{equation*}
        Take $\mathbfit{u}, \mathbfit{v} \in S$ and let $\mathbfit{w} = \lambda\mathbfit{u} + (1 - \lambda)\mathbfit{v}$ for some $\lambda \in [0, 1]$, then
        \begin{displaymath}
            \mathbfit{u - w} = (1 - \lambda)(\mathbfit{u - v}), \qquad \mathbfit{v - w} = -\lambda(\mathbfit{u - v}).
        \end{displaymath}
        Therefore, we have
        \begin{align*}
            f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{u - w}) & \leq f(\mathbfit{u}), \\
            f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{v - w}) & \leq f(\mathbfit{v}).
        \end{align*}
        Note that
        \begin{align*}
            \lambda\left(f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{u - w})\right) + (1 - \lambda)\left(f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{v - w})\right) = f(\mathbfit{w}),
        \end{align*}
        so we have
        \begin{align*}
            f(\lambda\mathbfit{u} + (1 - \lambda)\mathbfit{v}) & = f(\mathbfit{w}) \\
            & \leq \lambda f(\mathbfit{u}) + (1 - \lambda)f(\mathbfit{v}).
        \end{align*}
        Hence, $f$ is convex.
        \\\\
        We can similarly prove that $f$ is strictly convex if $f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) < f(\mathbfit{y})$. Now suppose that $f$ is strictly convex but there exists $\mathbfit{x}, \mathbfit{y} \in S$ with $\mathbfit{x} \neq \mathbfit{y}$ such that
        \begin{equation*}
            f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \geq f(\mathbfit{y}).
        \end{equation*}
        Take $\mathbfit{z} = \frac{1}{2}\mathbfit{x} + \frac{1}{2}\mathbfit{y}$. We have
        \begin{align*}
            \frac{1}{2}f(\mathbfit{x}) + \frac{1}{2}f(\mathbfit{y}) & > f(\mathbfit{z}) \\
            & \geq f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{z - x}) \\
            & = f(\mathbfit{x}) + \frac{1}{2}\nabla f(\mathbfit{x})^{\mathrm{T}}\left(\mathbfit{y - x}\right) \\
            & \geq f(\mathbfit{x}) + \frac{1}{2}\left(f(\mathbfit{y}) - f(\mathbfit{x})\right) \\
            & = \frac{1}{2}f(\mathbfit{x}) + \frac{1}{2}f(\mathbfit{y}),
        \end{align*}
        which is a contradiction.
    \end{proof}
\end{probox}
Proposition \ref{pro:convexTan} helps us determine whether a point is the global minimiser of a certain convex function.
\begin{thmbox}{Global Minimiser of Convex Functions}{convexFuncMin}
    Let $f \colon C \to \mathbf{R}$ be a convex and continuously differentiable function over a convex set~$C \subseteq \mathbf{R}^n$. Then $\mathbfit{x}^* \in C$ is a global minimiser for the minimisation problem 
    \begin{displaymath}
        \min\left\{f(\mathbfit{x}) \colon \mathbfit{x} \in C\right\}
    \end{displaymath}
    if and only if
    \begin{equation*}
        \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \geq 0
    \end{equation*}
    for all $\mathbfit{x} \in C$.
    \tcblower   
    \begin{proof}
        Suppose $\nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \geq 0$. By Proposition \ref{pro:convexTan}, we have 
        \begin{equation*}
            \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \leq f(\mathbfit{x}) - f(\mathbfit{x}^*).
        \end{equation*}
        This means that $f(\mathbfit{x}) \geq f(\mathbfit{x}^*)$ for all $\mathbfit{x} \in C$, and so $\mathbfit{x}^*$ is a global minimiser.
        \\\\
        Suppose that $\mathbfit{x}^*$ is a global minimiser for $f$ but there exists some $\mathbfit{x}_0 \in C$ such that
        \begin{equation*}
            \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x}_0 - \mathbfit{x}^*) < 0.
        \end{equation*}
        Consider $\mathbfit{y} = \lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*$ for $\lambda \in (0, 1)$. Notice that $f(\mathbfit{y}) \geq f(\mathbfit{x}^*)$, so by using Proposition \ref{pro:convexTan} we have
        \begin{align*}
            0 & \leq f(\mathbfit{y}) - f(\mathbfit{x}^*) \\
            & \leq -\nabla f(\mathbfit{y})^{\mathrm{T}}(\mathbfit{y - x^*}) \\
            & \leq \nabla f(\mathbfit{y})^{\mathrm{T}}(\mathbfit{y - x^*}) \\
            & = \lambda\nabla f\left(\lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*\right)^{\mathrm{T}}(\mathbfit{x_0 - x^*}) \\
            & \leq \nabla f\left(\lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*\right)^{\mathrm{T}}(\mathbfit{x_0 - x^*}).
        \end{align*}
        Therefore,
        \begin{align*}
            \lim_{\lambda \to 0^+}\left[\nabla f\left(\lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*\right)^{\mathrm{T}}(\mathbfit{x_0 - x^*})\right] & = \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x_0 - x^*}) \\
            & \geq 0,
        \end{align*}
        which is a contradiction.
    \end{proof}
\end{thmbox}

\section{Hessian Matrices}
So far we have learnt how to determine a function's convexity by either Definition \ref{dfn:convexFunc} or Proposition \ref{pro:convexTan}. However, there are certain functions which are difficult to manipulate and apply those methods algebraically. Therefore, we introduce another method to determine convexity by \textit{Hessian Matrices}.
\begin{dfnbox}{Hessian Matrix}{hessian}
    Let $f \colon S \to \mathbf{R}$ where $S \subseteq \mathbf{R}^n$ is non-empty and let $\mathbfit{x}$ be an {\color{red} \textbf{interior point}} of $S$. The {\color{red} \textbf{Hessian}} of $f$ at $\mathbfit{x}$ is the $n \times n$ matrix
    \begin{displaymath}
        H_f(\mathbfit{x}) = \begin{bmatrix}
            \frac{\partial^2f}{\partial x_1 \partial x_1}(\mathbfit{x}) & \frac{\partial^2f}{\partial x_1 \partial x_2}(\mathbfit{x}) & \cdots & \frac{\partial^2f}{\partial x_1 \partial x_n}(\mathbfit{x}) \\
            \frac{\partial^2f}{\partial x_2 \partial x_1}(\mathbfit{x}) & \frac{\partial^2f}{\partial x_2 \partial x_2}(\mathbfit{x}) & \cdots & \frac{\partial^2f}{\partial x_2 \partial x_n}(\mathbfit{x}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2f}{\partial x_n \partial x_1}(\mathbfit{x}) & \frac{\partial^2f}{\partial x_n \partial x_2}(\mathbfit{x}) & \cdots & \frac{\partial^2f}{\partial x_n \partial x_n}(\mathbfit{x})
        \end{bmatrix}
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        If $f$ has continuous second order derivatives, then $H_f(\mathbfit{x})$ is symmetric.
    \end{remark}
\end{notebox}
To use Hessian Matrices in convexity test, we also need to introduce the following notion of \textit{(semi)definiteness}:
\begin{dfnbox}{(Semi)Definiteness}
    Let $\mathbfit{A}$ be a real square matrix.
    \begin{itemize}
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{positive semidefinite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} \geq 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{positive definite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} > 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{negative semidefinite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} \leq 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{negative definite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} < 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{indefinite}} if it is neither positive semidefinite nor negative semidefinite.
    \end{itemize}
\end{dfnbox}
To determine the definiteness of a matrix, we may use the following eigenvalue tests:
\begin{thmbox}{Eigenvalue Test}{eigenTest}
    If $\mathbfit{A}$ is a symmetric real square matrix, then:
    \begin{itemize}
        \item $\mathbfit{A}$ is positive semidefinite if and only if all eigenvalues of $\mathbfit{A}$ are non-negative.
        \item $\mathbfit{A}$ is positive definite if and only if all eigenvalues of $\mathbfit{A}$ are positive.
        \item $\mathbfit{A}$ is negative semiefinite if and only if all eigenvalues of $\mathbfit{A}$ are non-positive.
        \item $\mathbfit{A}$ is negative definite if and only if all eigenvalues of $\mathbfit{A}$ are negative.
        \item $\mathbfit{A}$ is indefinite if and only if it has at least one positive eigenvalue and at least one negative eigenvalue.
    \end{itemize}
    \tcblower
    \begin{proof}
        We will only prove that $\mathbfit{A}$ is positive semidefinite if and only if all eigenvalues of $\mathbfit{A}$ are non-negative. The rest of the tests can be proven similarly.
        \\\\
        Suppose $\mathbfit{A}$ is positive semidefinite. Let $\lambda$ be any eigenvalue of $\mathbfit{A}$ and let $\mathbfit{x}$ be a corresponding eigenvector, then $\mathbfit{Ax} = \lambda\mathbfit{x}$. Therefore,
        \begin{equation*}
            \lambda\norm{\mathbfit{x}}^2 = \lambda\mathbfit{x}^{\mathrm{T}}\mathbfit{x} = \mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} \geq 0.
        \end{equation*}
        Since $\norm{\mathbfit{x}}^2 \geq 0$, this implies that $\lambda \geq 0$.
        \\\\
        Suppose conversely that all eigenvalues of $\mathbfit{A}$ are non-negative. Consider the diagonal matrix
        \begin{displaymath}
            \mathbfit{D} = \begin{bmatrix}
                \lambda_1 & 0 & \cdots & 0 \\
                0 & \lambda_2 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \lambda_n
            \end{bmatrix}
        \end{displaymath}
        where $\lambda_i$ is the $i$-th eigenvalue of $\mathbfit{A}$. Then there exists some orthogonal square matrix $\mathbfit{Q}$ such that 
        \begin{equation*}
            \mathbfit{A} = \mathbfit{QDQ}^{\mathrm{T}}.
        \end{equation*}
        Let $\mathbfit{x} \in \mathbf{R}^n$ be an arbitrary vector, then 
        \begin{align*}
            \mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} & = \mathbfit{x}^{\mathrm{T}}\mathbfit{QDQ}^{\mathrm{T}}\mathbfit{x} \\
            & = \left(\mathbfit{Q}^{\mathrm{T}}\mathbfit{x}\right)^{\mathrm{T}}\mathbfit{D}\left(\mathbfit{Q}^{\mathrm{T}}\mathbfit{x}\right) \\
            & = \sum_{i = 1}^{n}\left(\lambda_i\norm{\mathbfit{Q}^{\mathrm{T}}\mathbfit{x}}^2\right) \\
            & \geq 0.
        \end{align*}
        Therefore, $\mathbfit{A}$ is positive semidefinite.
    \end{proof}
\end{thmbox}
However, eigenvalues can be troublesome to compute. Thus for \textbf{small matrices}, we may use the following test with the \textit{principal minors}:
\begin{dfnbox}{Principal Minor}{prinMinor}
    Let $\mathbfit{A}$ be an $n \times n$ matrix. The $k$-th {\color{red} \textbf{principal minor}} $\Delta_k$ of $\mathbfit{A}$ is defined to be the determinant of the $k$th principal submatrix of $\mathbfit{A}$, i.e.,
    \begin{equation*}
        \Delta_k = \begin{vmatrix}
            a_{11} & a_{12} & \cdots & a_{1k} \\
            a_{21} & a_{22} & \cdots & a_{2k} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{k1} & a_{k2} & \cdots & a_{kk} \\
        \end{vmatrix}
    \end{equation*}
\end{dfnbox}
\begin{thmbox}{Definiteness Test Using Principal Minors}{prinMinorDefTest}
    Let $\mathbfit{A}$ be a symmetric $n \times n$ matrix, then $\mathbfit{A}$ is positive definite if and only if $\Delta_k > 0$, and negative definite if and only if $(-1)^k\Delta_k > 0$, for all $k = 1, 2, \cdots, n$.
\end{thmbox}
\subsection{Taylor's Theorem}
Note that we have not given the proof to Theorem \ref{thm:convexTest}. Now to prove it, we need to first introduce \textit{Taylor's Theorem}.
\begin{thmbox}{Taylor's Theorem}{taylorThm}
    Suppose that $f \colon S \to \mathbf{R}$ is a function with continuous second partial derivatives. Consider the set
    \begin{displaymath}
        [\mathbfit{x}, \mathbfit{y}] \coloneqq \left\{\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y} \colon \mathbfit{x}, \mathbfit{y} \in \mathbf{R}^n, \lambda \in [0, 1]\right\}.
    \end{displaymath}
    If $[\mathbfit{x}, \mathbfit{y}]$ is contained in the interior of $S$, then there exists some $\mathbfit{z} \in [\mathbfit{x}, \mathbfit{y}]$ such that
    \begin{equation*}
        f(\mathbfit{y}) = f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) + \frac{1}{2}(\mathbfit{y - x})^{\mathrm{T}}H_f(\mathbfit{z})(\mathbfit{y - x}).
    \end{equation*}
\end{thmbox}
Now, we are able to apply the following tests:
\begin{thmbox}{Convexity Test for Differentiable Functions}{convexTest}
    Let a function $f$ have continuous second order derivatives on an open convex set $D \subseteq \mathbf{R}^n$.
    \begin{itemize}
        \item $H_f(\mathbfit{x})$ is positive semidefinite for all $\mathbfit{x} \in D \iff f$ is convex on $D$.
        \item $H_f(\mathbfit{x})$ is positive definite for all $\mathbfit{x} \in D \implies f$ is strictly convex on $D$.
        \item $H_f(\mathbfit{x})$ is negative semidefinite for all $\mathbfit{x} \in D \iff f$ is concave on $D$.
        \item $H_f(\mathbfit{x})$ is negative definite for all $\mathbfit{x} \in D \implies f$ is strictly concave on $D$.
        \item $H_f(\mathbfit{x})$ is indefinite for some $\mathbfit{x} \in D \implies f$ is neither convex nor concave on $D$.
    \end{itemize}
\end{thmbox}
\chapter{Unconstrained Optimisation}
\section{Coercive Functions}
\begin{dfnbox}{Coercive Function}{coerciveFunc}
    A {\color{red} \textbf{continuous}} function $f \colon \mathbf{R}^n \to \mathbf{R}$ is said to be {\color{red} \textbf{coercive}} if
    \begin{equation*}
        \lim_{\norm{\mathbfit{x}} \to \infty} f(\mathbfit{x}) = +\infty.
    \end{equation*}
    More formally, $f$ is coercive if and only if for all $M > 0$, there is some $r > 0$ such that~$f(\mathbfit{x}) > M$ whenever $\norm{\mathbfit{x}} > r$.
\end{dfnbox}
\begin{thmbox}{Global Minimiser of Coercive Functions}{globalMinCoerciveFunc}
    If a function $f \colon \mathbf{R}^n \to \mathbf{R}$ is coercive, then $f$ has at least one global minimiser.
    \tcblower   
    \begin{proof}
        Take $M = \abs{f(\mathbfit{0})} + 1 > 0$. Since $f$ is coercive, there exists some $r > 0$ such that $f(\mathbfit{x}) > M > f(\mathbfit{0})$ whenever $\norm{\mathbfit{x}} > r$. Consider
        \begin{displaymath}
            B(\mathbfit{0}, r) = \left\{\mathbfit{x} \in \mathbf{R}^n \colon \norm{\mathbfit{x}} \leq r\right\}
        \end{displaymath}
        which is a compact set. So by Weiestrass' Theorem, there exists some $\mathbfit{x}^* \in B(\mathbfit{0}, r)$ such that $f(\mathbfit{x}^*) \leq f(\mathbfit{x}) \leq f(\mathbfit{0})$ for all $\mathbfit{x} \in B(\mathbfit{0}, r)$. Now, this means that for all $\mathbfit{x} \in \mathbf{R}^n$, we have $(\mathbfit{x}^*) \leq f(\mathbfit{x})$, which means that $\mathbfit{x}^*$ is a global minimiser for $f$.
    \end{proof}
\end{thmbox}
\section{Critical Points}
\begin{dfnbox}{Stationary (Critical) Point}{stationaryPt}
    Let $X \subseteq \mathbf{R}^n$ be an open set and let $f \colon X \to \mathbf{R}$ be a function. An interior point $\mathbfit{x}^*$ is called a {\color{red} \textbf{stationary point}} of $f$ if $\nabla f(\mathbfit{x}^*) = \mathbfit{0}$.
\end{dfnbox}
\begin{dfnbox}{Saddle Point}{Saddle}
    A stationary point $\mathbfit{x}^*$ of a function $f$ which is neither a local minimisr nor a local maximiser is called a {\color{red} \textbf{saddle point}}.
\end{dfnbox}
\begin{corbox}{}{saddleCor}
    Let $\mathbfit{x}^*$ be a stationary point of $f$. If $H_f(\mathbfit{x}^*)$ is indefinite, then $\mathbfit{x}^*$ is a saddle point.
\end{corbox}

\chapter{Numerical Methods}
\section{One-Dimensional Methods}
\subsection{Bisection}
Recall the \textit{Intermediate Value Theorem}:
\begin{thmbox}{Intermediate Value Theorem}{interVal}
    Let $f$ be a continuous function on $[a, b]$ with $f(a)f(b) < 0$, then there exists some $r \in (a, b)$ such that $f(r) = 0$.
\end{thmbox}
Note that $f'(x) = 0$ if $x$ is a local optimiser for $f$. The bisection method aims to find the solution to $f'(x) = 0$ to determine an approximation for the local optimiser of $f$.
\begin{tecbox}{Bisection Search Algorithm}{bisect}
    Let $f$ be a one-dimensional function such that $f(a)f(b) < 0$. We initialise bisection search with the interval $[a_1, b_1] = [a, b]$ and a tolerance level $\epsilon > 0$.
    \\\\
    At the $k$-th iteration, set $x_k = \frac{a_k + b_k}{2}$ as the current approximation. 
    \\\\
    If $b_k - a_k \leq 2\epsilon$ or $f(x_k) = 0$, we can stop the search and output $x_k$ as an approximated root of $f(x) = 0$.
    \\\\
    Else, we update the interval by
    \begin{equation*}
        [a_{k + 1}, b_{k + 1}] = \begin{cases}
            [a_k, x_k], & \quad\textrm{if } f(a_k)f(x_k) < 0 \\
            [x_k, b_k], & \quad\textrm{if } f(b_k)f(x_k) < 0
        \end{cases}.
    \end{equation*} 
\end{tecbox}
Let $x^* \in [a, b]$ be the true root of $f(x) = 0$. Note that if the algorithm terminates at the $k$-th iteration, it means that $\abs{b_k - a_k} \leq 2\epsilon$. Since $x^* \in [a_k, b_k]$ and $x_k = \frac{b_k + a_k}{2}$, we have
\begin{equation*}
    \abs{x_k - x^*} \leq \frac{\abs{b_k - a_k}}{2} \leq \epsilon.
\end{equation*}
Furthermore, notice that the size of the interval $[a_i, b_i]$ is halved after each iteration, so we have $\abs{b_k - a_k} = \frac{\abs{b_1 - a_1}}{2^{k - 1}}$. This means to have $\abs{b_k - a_k} \leq 2\epsilon$, we need at least
\begin{equation*}
    k = \left\lceil\log_2\frac{\abs{b_1 - a_1}}{\epsilon}\right\rceil
\end{equation*}
iterations. In other words, bisection search with tolerance level $\epsilon$ and initial interval width $n$ is an $\bigO\left(\log\frac{n}{\epsilon}\right)$ algorithm.

\begin{codebox}{Bisection Search Algorithm in Matlab}{}
    \begin{amzcode}{matlab}
function [x, k] = bisection(f, a, b, t)
    fa = feval(f, a);
    fb = feval(f, b);
    if (sign(fa) == sign(fb))
        error("f(a), f(b) have same sign.");
    end
    k = 1;
    while abs(b - a) > t
        x = (a + b) / 2;
        fx = feval(f, x);
        if (fx == 0)
            break;
        end
        if (sign(fx) == sign(fa))
            a = x; 
            fa = fx;
        else
            b = x; 
            fb = fx;
        end
        k += 1;
    end
end
    \end{amzcode}
\end{codebox}

\subsection{Newton's Method}
Let $f(x)$ be a function, then by Taylor expansion we can approximate $f$ by
\begin{equation*}
    f(x) \approx q(x) = f(x_k) + f'(x_k)(x - x_k) + \frac{1}{2}f''(x_k)(x - x_k)^2,
\end{equation*}
where $x_k$ is some approximation of the global minimiser of $f$. Let $x^*$ be the global minimiser for $q$, then
\begin{equation*}
    x^* = x_k - \frac{f'(x_k)}{f''(x_k)}.
\end{equation*}
This motivates the following algorithm:
\begin{tecbox}{Newton's Method Algorithm}{newton}
    Let $f$ be a one-dimensional function. We initialise Newton's method with an initial approximation $x_0$ and a tolerance level $\epsilon > 0$.
    \\\\
    At the $k$-th iteration, set $x_k = x_{k - 1} - \frac{f'(x_k)}{f''(x_k)}$. If $\abs{f'(x_k)} \leq \epsilon$, we can stop the algorithm and output $x_k$ as an approximated minimiser of $f$.
\end{tecbox}
\begin{codebox}{Newton's Method Algorithm in Matlab}{}
    \begin{amzcode}{matlab}
function [x, k] = newton(df, ddf, x0, t)
    k = 0;
    x = x0;
    while abs(feval(df, x)) > t
        x -= feval(df, x) / feval(ddf, x);
        k += 1;
    end
end
    \end{amzcode}
\end{codebox}
The rate of convergence of Newton's method is generally very fast, but for non-convex functions, there is a risk that the algorithm fails to find the approximation.

\subsection{Golden Section Method}
In some cases, we may encounter a function which is not differentiable (and thus we cannot use Newton's method to our advantage). It turns out, however, that if the function is \textit{unimodal}, then we can easily approximate its global minimiser.
\begin{dfnbox}{Unimodal Function}{unimodal}
    A function $f$ is said to be {\color{red} \textbf{unimodal}} on $[a, b]$ if it has exactly one global minimiser (maximiser) $x^*$ in $[a, b]$ and is strictly decreasing (increasing) on $[a, x^*]$ and strictly increasing (decreasing) on $[x^*, b]$.
\end{dfnbox}
\begin{tecbox}{Golden Section Method}{goldeSect}
    Let $f$ be a unimodal function. Consider $[a_0, b_0]$ as the initial interval. For each iteration, we consider
    \begin{align*}
        \lambda_n & = b_n - \Phi(b_n - a_n) \\
        \mu_n & = a_n + \Phi(b_n - a_n)
    \end{align*}
    where $\Phi = \frac{\sqrt{5} - 1}{2}$ is the Golden Ratio constant. Then we update the interval by:
    \begin{equation*}
        [a_{n + 1}, b_{n + 1}] = \begin{cases}
            [\lambda_n, b_n], \quad \textrm{if } f(\lambda_n) > f(\mu_n)\\
            [a_n, \mu_n], \quad \textrm{if } f(\lambda_n) < f(\mu_n)\\
            [\lambda_n, \mu_n], \quad \textrm{if } f(\lambda_n) = f(\mu_n)
        \end{cases}.
    \end{equation*}
\end{tecbox}
The reason why we use $\Phi$ to generate the intervals recursively is because that it reduces the number of computations. Consider that $[a_k, b_k] = [\lambda_{k - 1}, b_{k - 1}]$, then
\begin{align*}
    \lambda_k & = b_k - \Phi(b_k - a_k) \\
    & = b_{k - 1} - \Phi(b_{k - 1} - \lambda_{k - 1}) \\
    & = b_{k - 1} - \Phi(b_{k - 1} - b_{k - 1} + \Phi(b_{k - 1} - a_{k - 1})) \\
    & = b_{k - 1} - \Phi^2(b_{k - 1} - a_{k - 1}) \\
    & = b_{k - 1} - (1 - \Phi)(b_{k - 1} - a_{k - 1}) \\
    & = a_{k - 1} + \Phi(b_{k - 1} - a_{k - 1}) \\
    & = \mu_{k - 1}.
\end{align*}
However, note that we have already computed $\mu_{k - 1}$ in the previous iteration! Therefore, by applying ideas of \textit{dynamic programming}, this eliminates the need to re-compute its value. Similarly, we can show that $\mu_k = \lambda_{k - 1}$ if $f(\lambda_k) < f(\mu_k)$.
\begin{codebox}{Golden Section Algorithm in Matlab}{}
    \begin{amzcode}{matlab}
function [lhs, rhs, k] = golden_section(f, a, b, t)
    lhs = a;
    rhs = b;
    k = 0;
    phi = (sqrt(5) - 1) / 2;
    lambda = b - phi * (b - a);
    mu = a + phi * (b - a);
    while abs(b - a) > t
        f_lambda = feval(f, lambda);
        f_mu = feval(f, mu);
        if (f_lambda > f_mu) 
            a = lambda;
            lambda = mu;
        elseif (f_lambda < f_mu)
            b = mu;
            mu = lambda;
        else
            a = lambda;
            b = mu;
            lambda = b - phi * (b - a);
            mu = a + phi * (b - a);
        end
        lhs = a;
        rhs = b;
        k += 1;
    end
end
    \end{amzcode}
\end{codebox}
\section{Multivariable Methods}
Consider the following general framework of optimisation algorithms:
\begin{itemize}
    \item Initialise with an initial guess $\mathbfit{x}_0$.
    \item At each iteration:
        \begin{itemize}
            \item If $\mathbfit{x}_k$ is considered optimal, stop the search and output.
            \item Else, set $\mathbfit{x}_{k + 1} = \mathbfit{x}_k + \alpha_k\mathbfit{p}_k$.
        \end{itemize}
\end{itemize}
$\mathbfit{p}_k$ is the search direction and $\alpha_k$ is the step size.
\subsection{Newton's Method}
Similar to the one-dimensional case, let $f$ be a multivariable function with continuous second order partial derivatives. Given $\mathbfit{x_k} \in \mathbf{R}^n$ as an approximation of the global minimiser of $f$, by Taylor expansion there is some quadratic function $q$ such that
\begin{equation*}
    f(\mathbfit{x}) \approx q(\mathbfit{x}) = f(\mathbfit{x}_k) + \nabla f(\mathbfit{x}_k)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}_k) + \frac{1}{2}(\mathbfit{x} - \mathbfit{x}_k)^{\mathrm{T}}H_f(\mathbfit{x}_k)(\mathbfit{x} - \mathbfit{x}_k).
\end{equation*}
The global minimiser of $q$ is given by
\begin{equation*}
    \hat{\mathbfit{x}} = \mathbfit{x}_k - H_f(\mathbfit{x}_k)^{-1}\nabla f(\mathbfit{x}_k).
\end{equation*}
However, for the above equation to hold, we require two important assumptions: first, $H_f$ must be non-singular at $\hat{\mathbfit{x}}$; second, $H_f(\mathbfit{x})$ is \textit{Lipschitz continuous} in a neighbourhood of $\hat{\mathbfit{x}}$. To formally introduce \textit{Lipschitz continuity}, we state the following definitions:
\begin{dfnbox}{Frobenius Norm}{frobenius}
    Let $\mathbfit{A} \in \mathbf{R}^{n \times n}$ be a square matrix. The {\color{red} \textbf{Frobenius norm}} of $\mathbfit{A}$ is defined by
    \begin{equation*}
        \norm{\mathbfit{A}}^2_F = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}^2.
    \end{equation*}
\end{dfnbox}
We can show that $\norm{\mathbfit{Ax}} \leq \norm{\mathbfit{A}}_F\norm{\mathbfit{x}}$ for any vector $\mathbfit{x} \in \mathbf{R}^n$ and that $\norm{\mathbfit{AB}} \leq \norm{\mathbfit{A}}_F\norm{\mathbfit{B}}_F$ for any $n \times n$ matrix $\mathbfit{B}$.
\begin{dfnbox}{Lipschitz Continuity}{lipschitz}
    Let $f \colon \mathbf{R}^n \to \mathbf{R}^{n \times n}$ be a function. $f$ is {\color{red} \textbf{Lipschitz continuous}} at $\mathbfit{x}^*$ if there exists some $\delta, L > 0$ such that for all $\mathbfit{x}, \mathbfit{y} \in B(\mathbfit{x}^*, \delta)$,
    \begin{equation*}
        \norm{f(\mathbfit{x}) - f(\mathbfit{y})}_F \leq L\norm{\mathbfit{x - y}}.
    \end{equation*}
\end{dfnbox}
In practice when using Newton's method, we choose a neighbourhood of $\hat{\mathbfit{x}}$ which is small enough by setting $\delta = \frac{1}{\norm{H_f(\hat{\mathbfit{x}})^{-1}}_F}$.
\begin{tecbox}{Multi-dimensional Newton's Method Algorithm}{multiDimNewton}
    Let $f$ be a function. We initialise Newton's method with an initial approximation $\mathbfit{x}_0$ and a tolerance level $\epsilon > 0$.
    \\\\
    At the $k$-th iteration, set $\mathbfit{x}_k = \mathbfit{x}_{k - 1} - H_f(\mathbfit{x}_{k - 1})^{-1}\nabla f(\mathbfit{x}_{k - 1})$. If $\norm{\nabla f(\mathbfit{x}_k)} \leq \epsilon$, we can stop the algorithm and output $\mathbfit{x}_k$ as an approximated minimiser of $f$.
\end{tecbox}
Note that in the multi-dimensional case, although Newton's method still retain a fast rate of convergence, the computational cost per iteration is very high.
\begin{probox}{Convergence of the Newton Method}{newtonConverge}
    If $\mathbfit{x}_0$ is sufficiently close to $\mathbfit{x}^*$, then the sequence $\{\mathbfit{x}_k\}$ generated by the Newton Method converges to $\mathbfit{x}^*$ quadratically, i.e., there exists some $M \in \R$ such that
    \begin{equation*}
        \norm{\mathbfit{x}_{k + 1} - \mathbfit{x}^*} \leq M\norm{\mathbfit{x}_k - \mathbfit{x}^*}^2.
    \end{equation*}
\end{probox}
Note that the search direction of Newton's method is $-H_f(\mathbfit{x}_k)^{-1}\nabla f(\mathbfit{x}_k)$ and the step size is $1$. However, we can choose a better step size $\alpha_k$ given by
\begin{equation*}
    \alpha_k = \argmin_{\alpha \geq 0}f(\mathbfit{x}_{k + 1}) = \argmin_{\alpha \geq 0}f\left(\mathbfit{x}_k - \alpha_kH_f(\mathbfit{x}_k)^{-1}\nabla f(\mathbfit{x}_k)\right).
\end{equation*}
\subsection{Armijo Line Search}
Let $\mathbfit{x}_0$ be an approximation of some optimisation algorithm with search direction $\mathbfit{p}_0$ and step size $\alpha$. We can obtain the next approximation as
\begin{equation*}
    \hat{\mathbfit{x}} = \mathbfit{x}_0 + \alpha\mathbfit{p}_0.
\end{equation*} 
Correspondingly, the average change in the function value is given by
\begin{equation*}
    \frac{f(\hat{\mathbfit{x}}) - f(\mathbfit{x}_0)}{\alpha} = \frac{f(\mathbfit{x}_0 + \alpha\mathbfit{p}_0) - f(\mathbfit{x}_0)}{\alpha}.
\end{equation*}
Note that the rate of change along $\mathbfit{p}_0$ is given by $\nabla f(\mathbfit{x}_0)^{\mathrm{T}}\mathbfit{p}_0$. Suppose we wish $\alpha$ to be sufficiently small, then we can fix some $\sigma \in (0, 1)$ such that
\begin{equation*}
    \frac{f(\mathbfit{x}_0 + \alpha\mathbfit{p}_0) - f(\mathbfit{x}_0)}{\alpha} \leq \sigma\nabla f(\mathbfit{x}_0)^{\mathrm{T}}\mathbfit{p}_0.
\end{equation*}
If the above inequality holds true, it means that the average rate of decrease in the value of $f$ along $\mathbfit{p}_0$ with step size $\alpha$ is at least $\sigma$ of the instantaneous rate of decrease. If the choice of $\alpha$ is not ideal enough, we can then try $\beta\alpha$ for some $\beta \in (0, 1)$. This leads to the \textit{Armijo line search} algorithm. 
\begin{tecbox}{Armijo Line Search Algorithm}{armijo}
    Take $\sigma \in (0, 0.5)$ and $\beta \in (0, 1)$. Initialise $\alpha_0$ with a potentially large value.
    \\\\
    At the $k$-th iteration, consider $\alpha_k = \beta\alpha_{k - 1}$. If
    \begin{equation*}
        f(\mathbfit{x}_0 + \alpha_k\mathbfit{p}_0) \leq f(\mathbfit{x}_0) + \sigma\alpha_k\nabla f(\mathbfit{x}_0)^{\mathrm{T}}\mathbfit{p}_0,
    \end{equation*}
    we can output $\alpha_k$ as an sufficiently optimal choice of $\alpha$.
\end{tecbox}
We can then combine the Armijo line search with Newton's method as follows:
\begin{codebox}{Newton's Method with Armijo Line Search in Matlab}{}
    \begin{amzcode}{matlab}
function [alpha, k] = armijo(f, fx, nabla, x, p, sigma, beta, init_a)
    alpha = init_a;
    dec = nabla' * p;
    k = 0;
    while feval(f, x + alpha * p) > sigma * alpha * dec 
        alpha *= beta;
        k += 1;
    end
end

function [x, k] = newton_armijo(f, df, ddf, x0, t)
    x = x0;
    i = 0;
    alpha = 2;
    df_val = feval(df, x);
    while norm(df_val) > t
        p = -feval(ddf, x) \ df_val;
        [alpha, i] = armijo(f, feval(f, x), df_val, x, p, 0.48, 0.95, alpha);
        x += alpha * p;
    end
end
    \end{amzcode}
\end{codebox}
\section{Steepest Descent}
Recall that $f$ decreases most rapidly along the direction of $-\nabla f$. So we can take
\begin{equation*}
    \hat{\mathbfit{d}} = -\frac{\nabla f(\mathbfit{x})}{\norm{\nabla f(\mathbfit{x})}}
\end{equation*}
at any $\mathbfit{x}$, where $-\nabla f(\mathbfit{x})$ is known as the \textit{steepest descent direction}. Recall \ref{pro:directionalDerivative}, so 
\begin{align*}
    f(\mathbfit{x} + \delta\hat{\mathbfit{d}}) & = f(\mathbfit{x}) + \delta\nabla f(\mathbfit{x})^{\mathrm{T}}\hat{\mathbfit{d}} \\
    & = f(\mathbfit{x}) - \delta\frac{\norm{\nabla f(\mathbfit{x})}^2}{\norm{\nabla f(\mathbfit{x})}} \\
    & = f(\mathbfit{x}) - \delta\norm{\nabla f(\mathbfit{x})}.
\end{align*}
\begin{tecbox}{Steepest Descent Method}{steepestDescent}
    Let $f$ be an objective function and $\mathbfit{x}_0$ be an initial guess. Let $\epsilon > 0$ be the tolerance level.
    \\\\
    At the $k$-th iteration, we evaluate the steepest descent direction
    \begin{equation*}
        \mathbfit{d}_k = -\nabla f\left(\mathbfit{x}_k\right).
    \end{equation*}
    If $\norm{\mathbfit{d}_k} < \epsilon$, $\mathbfit{x}_k$ is an approximate minimiser. Otherwise, compute
    \begin{equation*}
        \mathbfit{x}_{k + 1} = \mathbfit{x}_k + t_k\mathbfit{d}_k,
    \end{equation*}
    where $t_k$ is such that $f(\mathbfit{x}_k + t_k\mathbfit{d}_k)$ is minimised and is known as the step size.
\end{tecbox}
\begin{codebox}{Steepest Descent Algorithm in Matlab}{}
    \begin{amzcode}{matlab}
function [x, d, k] = steepest_descent(f, nabla, x0, t)
    x = x0;
    grad = feval(nabla, x);
    k = 0;
    while norm(grad) > t
        d = -grad;
        fx = feval(f, x);
        [step, i] = armijo(f, fx, nabla, x, d, 0.48, 0.95, 2);
        x += step * d;
        grad = feval(nabla, x);
        k += 1;
    end
end
    \end{amzcode}
\end{codebox}
Intuitively, suppose $\mathbfit{x}_0$ is some initial approximation located at the level set $f(\mathbfit{x}) = k$. Using the steepest descent method, we would search along the direction that is \textbf{orthogonal} to the level set at $\mathbfit{x}_0$. We denote this direction as $\mathbfit{d}$. Note that we will only find the next approximation when $f(\mathbfit{x})$ \textbf{cannot decrease further}, i.e., we encounter some other level set $f(\mathbfit{x}) = h$ to which $\mathbfit{d}$ is a tangential direction.

With a graphical illustration, one might observe that the approximations obtained via the steepest descent method trace out a ``zig-zag'' path with an orthogonal turn between any two consecutive search directions. This behaviour is rigorously described as follows:
\begin{probox}{Zig-zag Behaviour of Steepest Descent}{zigzag}
    If $(\mathbfit{x}_k)$ be a sequence of approximations for the function $f(\mathbfit{x})$ using steepest descent method, then
    \begin{equation*}
        (\mathbfit{x}_{k + 2} - \mathbfit{x}_{k + 1})^{\mathrm{T}}(\mathbfit{x}_{k + 1} - \mathbfit{x}_k) = 0
    \end{equation*}
    for all $k \in \N$.
\end{probox}

\section{Conjugate Gradient Method}
Consider the quadratic function
\begin{equation*}
    \phi(\mathbfit{x}) = \frac{1}{2}\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} - \mathbfit{b}^{\mathrm{T}}\mathbfit{x}
\end{equation*}
where $\mathbfit{A}$ is symmetric positive definite. Let $\mathbfit{x}_0$ be an initial approximation. Suppose we obtain $\mathbfit{x}_1$ along $\mathbfit{p}_0$ with exact line search, i.e.,
\begin{equation*}
    \mathbfit{x}_1 = \mathbfit{x}_0 + \alpha_0\mathbfit{p}_0, \qquad \alpha_0 = \argmin_{\alpha > 0} \phi(\mathbfit{x}_0 + \alpha\mathbfit{p}_0).
\end{equation*}
Notice that this means that $\alpha_0$ is a global minimiser to $g_0(\alpha) = \phi(\mathbfit{x}_0 + \alpha\mathbfit{p}_0)$, and so $g'_0(\alpha_0) = 0$. Let $\mathbfit{y}_0 = \mathbfit{x}_0 + \alpha\mathbfit{p}_0$, then
\begin{align*}
    g'_0(\alpha) & = \frac{\partial \phi}{\partial \mathbfit{y}_0}\cdot\frac{\partial \mathbfit{y}_0}{\partial \alpha} \\
    & = \nabla \phi(\mathbfit{x}_0 + \alpha\mathbfit{p}_0)^{\mathrm{T}}\mathbfit{p}_0.
\end{align*}
This implies that $\nabla \phi(\mathbfit{x}_1)^{\mathrm{T}}\mathbfit{p}_0 = 0$. Let $\mathbfit{x}^*$ be the optimal point, we now wish to find some $\mathbfit{p}_1$ and $\alpha^*$ such that 
\begin{equation*}
    \mathbfit{x}_1 + \alpha^*\mathbfit{p}_1 = \mathbfit{x}^*.
\end{equation*}
Notice that $\nabla\phi(\mathbfit{x}) = \mathbfit{Ax - b}$, so $\nabla\phi(\mathbfit{x}^*) = \mathbfit{Ax}^* - \mathbfit{b} = 0$ and $\nabla \phi(\mathbfit{x}_1)^{\mathrm{T}}\mathbfit{p}_0 = (\mathbfit{Ax}_1 - \mathbfit{b})^{\mathrm{T}}\mathbfit{p}_0 = 0$, so we have
\begin{align*}
    0 & = (\mathbfit{Ax}_1 - \mathbfit{b})^{\mathrm{T}}\mathbfit{p}_0 \\
    & = (\mathbfit{Ax}_1 - \mathbfit{Ax}^*)^{\mathrm{T}}\mathbfit{p}_0 \\
    & = (\mathbfit{x}_1 - \mathbfit{x}^*)^{\mathrm{T}}\mathbfit{A}^{\mathrm{T}}\mathbfit{p}_0 \\
    & = -\alpha^*\mathbfit{p}_1^{\mathrm{T}}\mathbfit{A}\mathbfit{p}_0.
\end{align*}
The above concludes that if there is $\mathbfit{p}_0, \mathbfit{p}_1$ and $\alpha^* \in \mathbf{R}$ such that for some initial approximation $\mathbfit{x}_0$ of the optimiser $\mathbfit{x}^*$ to a quadratic function $\phi$, we have
\begin{align*}
    \mathbfit{x}^* & = \mathbfit{x}_1 + \alpha^*\mathbfit{p}_1 \\
    & = \mathbfit{x}_0 + \alpha_0\mathbfit{p}_0 + \alpha^*\mathbfit{p}_1
\end{align*}
where $\alpha_0 = \argmin_{\alpha > 0} \phi(\mathbfit{x}_0 + \alpha\mathbfit{p}_0)$, then necessarily $\mathbfit{p}_1^{\mathrm{T}}\mathbfit{Ap}_0 = 0$.
\begin{dfnbox}{Conjugate Vectors}{conjVecs}
    A set of vectors $\left\{\mathbfit{p}_0, \mathbfit{p}_1, \cdots, \mathbfit{p}_n\right\}$ is {\color{red} \textbf{conjugate}} with respect to $\mathbfit{A}$, where $\mathbfit{A} \succ 0$, if
    \begin{equation*}
        \mathbfit{p}_i^{\mathrm{T}}\mathbfit{Ap}_j = 0 \qquad \textrm{for all } i \neq j.
    \end{equation*}
\end{dfnbox}
\begin{tecbox}{Conjugate Gradient Method Algorithm}{conjGrad}
    Let 
    \begin{equation*}
        \phi(\mathbfit{x}) = \frac{1}{2}\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} - \mathbfit{b}^{\mathrm{T}}\mathbfit{x}
    \end{equation*}
    where $\mathbfit{A} \succ 0$. Take $\mathbfit{x}_0$ be the initial approximation for the global minimiser of $\phi$ and $\left\{\mathbfit{p}_0, \mathbfit{p}_1, \cdots, \mathbfit{p}_{n - 1}\right\}$ be a set of directional vectors conjugate to $\mathbfit{A}$. Set
    \begin{equation*}
        \mathbfit{r}_n = \mathbfit{Ax}_n - \mathbfit{b} = \nabla \phi(\mathbfit{x}_n)
    \end{equation*}
    as the {\color{red} \textbf{residual}}.
    \\\\
    At the $k$-th iteration, consider
    \begin{align*}
        0 & = \phi(\mathbfit{x}_k + \alpha\mathbfit{p}_k)^{\mathrm{T}}\mathbfit{p}_k \\
        & = \left[\mathbfit{A}(\mathbfit{x}_k + \alpha\mathbfit{p}_k) - \mathbfit{b}\right]^{\mathrm{T}}\mathbfit{p}_k \\
        & = \left(\mathbfit{r}_k + \alpha\mathbfit{Ap}_k\right)^{\mathrm{T}}\mathbfit{p}_k \\
        & = \mathbfit{r}_k^{\mathrm{T}}\mathbfit{p}_k + \alpha\mathbfit{p}_k^{\mathrm{T}}\mathbfit{A}^{\mathrm{T}}\mathbfit{p}_k.
    \end{align*}
    Therefore,
    \begin{equation*}
        \alpha_k = \argmin_{\alpha > 0}\phi(\mathbfit{x}_k + \alpha\mathbfit{p}_k) = -\frac{\mathbfit{r}_k^{\mathrm{T}}\mathbfit{p}_k + \mathbfit{p}_k}{\mathbfit{p}_k^{\mathrm{T}}\mathbfit{A}^{\mathrm{T}}\mathbfit{p}_k}.
    \end{equation*}
    Take $\mathbfit{x}_{k + 1} = \mathbfit{x}_k + \alpha_k\mathbfit{p}_k$.
\end{tecbox}
\begin{notebox}
    \begin{remark}
        Note that we can verify
        \begin{align*}
            \mathbfit{r}_{k + 1} & = \mathbfit{Ax}_{k + 1} - \mathbfit{b} \\
            & = \mathbfit{A}(\mathbfit{x}_k + \alpha_k\mathbfit{p}_k) - \mathbfit{b} \\
            & = \mathbfit{Ax}_{k} - \mathbfit{b} - \alpha_k\mathbfit{Ap}_k \\
            & = \mathbfit{r}_k - \alpha_k\mathbfit{Ap}_k
        \end{align*}
    \end{remark}
\end{notebox}
To analyse the rate of convergence of conjugate gradient method, we consider the following theorem:
\begin{thmbox}{Expanding Subspace Minimisation}{expandSubspaceMin}
    Let $\mathbfit{x}_k$ be the $k$-th approximation generated by the conjugate gradient method with $\left\{\mathbfit{p}_0, \mathbfit{p}_1, \cdots, \mathbfit{p}_{k - 1}\right\}$ for the quadratic function
    \begin{equation*}
        \phi(\mathbfit{x}) = \frac{1}{2}\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} - \mathbfit{b}^{\mathrm{T}}\mathbfit{x},
    \end{equation*}
    then $\mathbfit{x}_k$ minimises $\phi$ over the set
    \begin{equation*}
        \mathbfit{x}_0 + \mathrm{span}\left\{\mathbfit{p}_0, \mathbfit{p}_1, \cdots, \mathbfit{p}_{k - 1}\right\}.
    \end{equation*}
\end{thmbox}
\begin{corbox}{Convergence of Conjugate Gradient Method}{convergeConjGrad}
    For any $\mathbfit{x}_0$ as the initial approximation for the minimiser of some quadratic function $\phi$, the sequence $(\mathbfit{x}_k)$ generated by the conjugate gradient method converges to the global minimiser of $\phi$ in at most $n$ steps.
\end{corbox}
\begin{probox}{}{}
    At the $(k + 1)$-th iteration of the conjugate gradient method, 
    \begin{equation*}
        \mathbfit{p}_{k + 1} = -\mathbfit{r}_{k + 1} + \beta_{k + 1}\mathbfit{p}_k,
    \end{equation*}
    where
    \begin{align*}
        \beta_{k + 1} = \frac{\mathbfit{r}_{k + 1}^{\mathrm{T}}\mathbfit{Ap}_k}{\mathbfit{p}_k^{\mathrm{T}}\mathbfit{Ap}_k} = \frac{\mathbfit{r}_{k + 1}^{\mathrm{T}}\mathbfit{r}_{k + 1}}{\mathbfit{r}_{k}^{\mathrm{T}}\mathbfit{r}_{k}}, \qquad \mathbfit{p}_0 = -\mathbfit{r}_0.
    \end{align*}
\end{probox}
\begin{codebox}{Conjugate Gradient Method Algorithm in Matlab}{}
    \begin{amzcode}{matlab}
function x = conj_grad(x0, A, b, t)
    x = x0;
    r = A * x - b;
    p = -r;
    while norm(r) > t
        alpha = -(r' * p + p) / (p' * A' * p);
        x += alpha * p;
        r_new = r - alpha * A * p;
        beta = (r_new' * r_new) / (r' * r);
        p = -r_new + beta * p;
    end
end
    \end{amzcode}
\end{codebox}
\chapter{Constrained Optimisation}
\section{Dual Optimisation Problems}
Consider the non-linear programming problem
\begin{align*}
    \min_{\mathbfit{x} \in \mathbf{R}^n} & f(\mathbfit{x}) \\
    \textrm{s.t. } & g_i(\mathbfit{x}) = 0 \quad\textrm{for } i = 1, 2, \cdots, m \\
    & h_j(\mathbfit{x}) \leq 0 \quad\textrm{for } j = 1, 2, \cdots, p.
\end{align*}
Previously, we have learnt how to optimise a function with no constraint. In non-rigorous language, if we were to convert this constrained problem to an unconstrained case, we would want to transform the objective function such that any points outside of the feasible set can never possibly be an optimal solution, i.e., choosing such a point will impose a penalty to our function value. To ``get rid of'' the constraints here, we may attempt to consider penalty functions
\begin{equation*}
    \min_{\mathbfit{x} \in \mathbf{R}^n} \left[f(\mathbfit{x}) + \sum_{i = 1}^{m}P_i\bigl(g_i(\mathbfit{x})\bigr) + \sum_{j = 1}^{p}Q_j\bigl(h_j(\mathbfit{x})\bigr)\right],
\end{equation*}
where 
\begin{align*}
    P_i\bigl(g_i(\mathbfit{x})\bigr) & = \begin{cases}
        \infty, & \quad\textrm{if } g_i(\mathbfit{x}) \neq 0 \\
        0, & \quad\textrm{otherwise}
    \end{cases}, \\
    Q_j\bigl(h_j(\mathbfit{x})\bigr) & = \begin{cases}
        \infty, & \quad\textrm{if } h_j(\mathbfit{x}) > 0 \\
        0, & \quad\textrm{otherwise}
    \end{cases}.
\end{align*}
However, notice that by doing so our objective function has become \textbf{discontinuous}! To fix this, we may instead try linear functions like
\begin{equation*}
    P_i\bigl(g_i(\mathbfit{x})\bigr) = \abs{\lambda_ig_i(\mathbfit{x})}, \qquad Q_j\bigl(h_j(\mathbfit{x})\bigr) = \mu_jh_j(\mathbfit{x}),
\end{equation*}
where $\lambda_i \in \mathbf{R}$ and $\mu_j \in \mathbfit{R}^+_0$. One way to interpret this construct is that when our constraints are not satisfied, the $P_i$'s and $Q_j$'s will make our function value larger than $f(\mathbfit{x})$. On the other hand, if our constraints are satisfied, the function value is even smaller than $f(\mathbfit{x})$.

So an obvious pitfall of this transformation is that our optimal value will be altered. However, notice that if we take
\begin{equation*}
    P_i\bigl(g_i(\mathbfit{x})\bigr) = \max_{\lambda_i \in \mathbf{R}}\lambda_ig_i(\mathbfit{x}), \qquad Q_j\bigl(h_j(\mathbfit{x})\bigr) = \max_{\mu_j \geq 0}\mu_jh_j(\mathbfit{x}),
\end{equation*}
then we essentially get back to the first group of penalty functions! 
\begin{dfnbox}{Lagrangian Dual Problem}{lagrangianDual}
    Let
    \begin{align*}
        \min_{\mathbfit{x} \in X} & f(\mathbfit{x}) \\
        \textrm{s.t. } & g_i(\mathbfit{x}) = 0 \quad\textrm{for } i = 1, 2, \cdots, m \\
        & h_j(\mathbfit{x}) \leq 0 \quad\textrm{for } j = 1, 2, \cdots, p
    \end{align*}
    be a constrained optimisation problem known as the {\color{red} \textbf{primal}} problem. The function
    \begin{equation*}
        \mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}, \mathbfit{\mu}) = f(\mathbfit{x}) + \mathbfit{\lambda}^{\mathrm{T}}\mathbfit{g}(\mathbfit{x}) + \mathbfit{\mu}^{\mathrm{T}}\mathbfit{h}(\mathbfit{x}) = f(\mathbfit{x}) + \sum_{i = 1}^{m}\lambda_i g_i(\mathbfit{x}) + \sum_{j = 1}^{p}\mu_j h_j(\mathbfit{x})
    \end{equation*}
    is known as the {\color{red} \textbf{Lagrangian}} of the optimisation problem, where
    \begin{equation*}
        \mathbfit{\lambda} = \begin{bmatrix}
            \lambda_1 \\
            \lambda_2 \\
            \vdots \\
            \lambda_m
        \end{bmatrix}, \qquad \mathbfit{g} = \begin{bmatrix}
            g_1 \\
            g_2 \\
            \vdots \\
            g_m
        \end{bmatrix}, \qquad \mathbfit{\mu} = \begin{bmatrix}
            \mu_1 \\
            \mu_2 \\
            \vdots \\
            \mu_p
        \end{bmatrix}, \qquad \mathbfit{h} = \begin{bmatrix}
            h_1 \\
            h_2 \\
            \vdots \\
            h_p
        \end{bmatrix}.
    \end{equation*}
    The {\color{red} \textbf{Lagrangian dual function}} is defined as
    \begin{equation*}
        \theta(\mathbfit{\lambda}, \mathbfit{\mu}) \coloneqq \inf_{\mathbfit{x} \in X}\mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}, \mathbfit{\mu}).
    \end{equation*}
    The {\color{red} \textbf{Lagrangian dual}} problem is the optimisation problem
    \begin{align*}
        \max_{\lambda_i \in \mathbf{R}, \mu_j \geq 0}\theta(\mathbfit{\lambda}, \mathbfit{\mu}).
    \end{align*}
\end{dfnbox}
The Lagrangian dual problem can be interpreted as follows: we first choose an $\mathbfit{x}$ such that~$\mathcal{L}$ is as small as possible for any $\mathbfit{\lambda}$ and $\mathbfit{\mu}$. With respect to this chosen $\mathbfit{x}$, we then try to make~$\mathcal{L}$ as large as possible by choosing appropriate $\mathbfit{\lambda}$ and $\mathbfit{u}$. However, note that if our choice of $\mathbfit{x}$ is feasible, then in the most ideal case, the penalty functions evaluate to $0$ and we restore our original optimal value!

Moreover, it turns out that the dual problem can be easy to solve.
\begin{thmbox}{Concavity of Lagrangian Dual Functions}{concaveDual}
    Let $\theta(\mathbfit{\lambda}, \mathbfit{\mu}) = \min\left\{\mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}, \mathbfit{\mu}) \colon \mathbfit{x} \in X\right\}$ be finite for all $(\mathbfit{\lambda}, \mathbfit{\mu})$, then $\theta$ is concave.
\end{thmbox}
\subsection{Duality Theorems}
We would like to investigate the relation between the dual problem and the primal. Intuitively, since we construct the dual function $\theta$ as the infimum of $\mathcal{L}$, its value would be bounded above by $f$.
\begin{thmbox}{Weak Duality Theorem}{weakDuality}
    Let (P) and (D) be a pair of primal and dual problems. Let $\mathbfit{x}$ be a feasible solution to (P) and $(\mathbfit{\lambda}, \mathbfit{\mu})$ be a feasible solution to (D), then
    \begin{equation*}
        f(\mathbfit{x}) \geq \theta(\mathbfit{\lambda}, \mathbfit{\mu}).
    \end{equation*}
\end{thmbox}
Note that \ref{thm:weakDuality} holds for any $\mathbfit{x}$ and $(\mathbfit{\lambda}, \mathbfit{\mu})$, which leads to the following corollary:
\begin{corbox}{}{lowerBoundOptimal}
    Let (P) and (D) be a pair of primal and dual problems, then
    \begin{equation*}
        \min f(\mathbfit{x}) \geq \max \theta(\mathbfit{\lambda}, \mathbfit{\mu}).
    \end{equation*}
    Therefore, if $\mathbfit{x}^*$ is feasible to (P) and $(\mathbfit{\lambda}^*, \mathbfit{\mu}^*)$ is feasible to (D) such that
    \begin{equation*}
        f(\mathbfit{x}^*) = \theta(\mathbfit{\lambda}^*, \mathbfit{\mu}^*),
    \end{equation*}
    then they are the optimal solutions to (P) and (D) respectively.
\end{corbox}
\begin{notebox}
    \begin{remark}
        Note that the converse of \ref{cor:lowerBoundOptimal} does not hold in general!
    \end{remark}
\end{notebox}
Let $\mathbfit{x}^*$ be optimal to (P) whose Lagrangian dual (D) has the optimal solution $(\mathbfit{\lambda}^*, \mathbfit{\mu}^*)$, then by \ref{thm:weakDuality} and \ref{cor:lowerBoundOptimal} we have
\begin{equation*}
    f(\mathbfit{x}) \geq f(\mathbfit{x}^*) \geq \theta(\mathbfit{\lambda}^*, \mathbfit{\mu}^*) \geq \theta(\mathbfit{\lambda}, \mathbfit{\mu}).
\end{equation*}
Therefore, under weak duality, the Lagrangian dual problem seeks the \textbf{greatest lower bound} for $f(\mathbfit{x}^*)$. We may wish to measure the tightness of this lower bound using \textit{duality gap}.
\begin{dfnbox}{Duality Gap}{dualityGap}
    Let (P) and (D) be a pair of primal and Lagrangian dual problems, then
    \begin{equation*}
        \min f(\mathbfit{x}) - \max \theta(\mathbfit{\lambda}, \mathbfit{\mu}),
    \end{equation*}
    i.e., the difference between the optimal values of the primal and dual problems, is known as the {\color{red} \textbf{duality gap}}.
\end{dfnbox}
It is easy to see that when the duality gap is $0$, the primal and dual problems become equivalent. This situation is known as \textit{strong duality}. For strong duality to hold, we first need to state an important pre-requisite known as the \textit{Slater's Condition}.
\begin{dfnbox}{Slater's Condition}{slater}
    For a convex programming problem with at least one inequality constraint and feasible set $S$, the {\color{red} \textbf{Slater's Condition}} holds if there is some $\hat{\mathbfit{x}} \in S$ such that $h_j(\hat{\mathbfit{x}}) < 0$ for all $j = 1, 2, \cdots, p$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        In other words, Slater's condition verifies that there is at least one point which is {\color{red} \textbf{strictly feasible}}, i.e., the interior of the feasible set is {\color{red} \textbf{non-empty}}.
    \end{remark}
\end{notebox}
\begin{thmbox}{Strong Duality Theorem}{strongDuality}
    Let (P) be a primal problem. If (P) is convex and Slater's condition holds for (P), then
    \begin{equation*}
        \inf f(\mathbfit{x}) = \sup \theta(\mathbfit{\lambda}, \mathbfit{\mu}).
    \end{equation*}
\end{thmbox}
\subsection{Saddle Points}
Let $\mathbfit{x}^*$ be the optimal solution to some primal problem (P) and let $(\mathbfit{\lambda}^*, \mathbfit{\mu}^*)$ be the optimal solution to its dual problem (D). Consider the Lagrangian function $\mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}, \mathbfit{\mu})$. By definition, 
\begin{equation*}
    \theta(\mathbfit{\lambda}^*, \mathbfit{\mu}^*) = \inf \mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}^*, \mathbfit{\mu}^*) \leq \mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}^*, \mathbfit{\mu}^*).
\end{equation*}
Note that $\mathbfit{\lambda}^*$ and $\mathbfit{\mu}^*$ also correspond to the maximal penalty function values, so 
\begin{equation*}
    \mathcal{L}(\mathbfit{x}^*, \mathbfit{\lambda}, \mathbfit{\mu}) \leq \mathcal{L}(\mathbfit{x}^*, \mathbfit{\lambda}^*, \mathbfit{\mu}^*).
\end{equation*}
The above can be summarised as the following definition:
\begin{dfnbox}{Saddle Point of Lagrangian Functions}{LSaddlePt}
    Let $\mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}, \mathbfit{\mu})$ be the Lagrangian of some non-linear programming problem. A point $(\mathbfit{x}^*, \mathbfit{\lambda}^*, \mathbfit{\mu}^*)$ is known as the {\color{red} \textbf{saddle point}} of $\mathcal{L}$ if
    \begin{equation*}
        \mathcal{L}(\mathbfit{x}^*, \mathbfit{\lambda}, \mathbfit{\mu}) \leq \mathcal{L}(\mathbfit{x}^*, \mathbfit{\lambda}^*, \mathbfit{\mu}^*) \leq \mathcal{L}(\mathbfit{x}, \mathbfit{\lambda}^*, \mathbfit{\mu}^*)
    \end{equation*}
    for all feasible $\mathbfit{x}$ and $(\mathbfit{\lambda}, \mathbfit{\mu})$.
\end{dfnbox}
From the above definition, we see that for a fixed $\mathbfit{x}^*$, $(\mathbfit{\lambda}^*, \mathbfit{\mu}^*)$ maximises $\mathcal{L}$ over all $(\mathbfit{\lambda}, \mathbfit{\mu})$ and for a fixed $(\mathbfit{\lambda}^*, \mathbfit{\mu}^*)$, $\mathbfit{x}^*$ minimises $\mathcal{L}$ over all $\mathbfit{x}$.
\begin{thmbox}{Saddle Point Optimality}{saddlePtOptimal}
    Let (P) be a primal problem with Lagrangian function $\mathcal{L}$ and correponding dual problem (D). If $(\mathbfit{x}^*, \mathbfit{\lambda}^*, \mathbfit{\mu}^*)$ is a saddle point of $\mathcal{L}$, then $\mathbfit{x}^*$ and $(\mathbfit{\lambda}^*, \mathbfit{\mu}^*)$ are the optimal solution of (P) and (D) respectively.
\end{thmbox}
Therefore, we can solve a constrained NLP by finding the saddle points of its Lagrangian. It turns out that these saddle points conform to some special conditions known as the \textit{Karush-Kuhn-Tucker Conditions}.

\section{Karush-Kuhn-Tucker Conditions}
A useful tool to find saddle points is the gradient vector of a function, but in a constrained case, the gradient vector is undefined along the boundary of the feasible set. However, notice that along the ``boundary'' of the feasible set, the inequality constraints will be reduced into equality constraints. This motivates us to consider the feasible points along the boundary first.

In the following section, we shall define rigorously the meaning of a point being ``on the boundary'' of the feasible set.
\subsection{Linear Independence Constraint Qualification}
\begin{dfnbox}{Active Constraint}{activeConstraint}
    An inequality constraint $h_j(\mathbfit{x}) \leq 0$ is said to be {\color{red} \textbf{active}} at $\mathbfit{x}^* \in S$ if $h_j(\mathbfit{x}^*) = 0$. Otherwise, it is said to be {\color{red} \textbf{inactive}} or {\color{red} \textbf{slack}} at $\mathbfit{x}^*$.
\end{dfnbox}
The above definition essentially defines the notion of ``being on the boundary'' for any feasible point to a constrained NLP. Note that for any any $\mathbfit{x}^*$ in the feasible set, any equality constraint $g_i(\mathbfit{x}) = 0$ is always active.

Now, take any point $\mathbfit{x}^*$ in the feasible set. If $\left\{h_1, h_2, \cdots, h_p\right\}$ are the set of inequality constraints of the NLP, then we can always find some subset $J \subseteq \left\{1, 2, \cdots, p\right\}$ such that $h_j$ where $j \in J$ is active at $\mathbfit{x}^*$. This $J$ is known as an \textit{index set}.
\begin{dfnbox}{Regular Point}{regPt}
    Let $\mathbfit{x}^* \in S$ and let
    \begin{equation*}
        J(\mathbfit{x}^*) = \left\{j \in \left\{1, \cdots, p\right\} \colon h_j(\mathbfit{x}^*) = 0\right\}
    \end{equation*}
    be the index set of active ineqaulity constraints at $
    \mathbfit{x}^*$. If the set of gradient vectors
    \begin{equation*}
        \left\{\nabla g_i(\mathbfit{x}^*) \colon i = 1, 2, \cdots, m\right\} \cup \left\{\nabla h_j(\mathbfit{x}^*) \colon j \in J(\mathbfit{x}^*)\right\}
    \end{equation*}
    is linearly independent, then we say that $\mathbfit{x}^*$ is a {\color{red} \textbf{regular point}}. Alternatively, we say that the {\color{red} \textbf{Regularity Condition}} or the {\color{red} \textbf{Linear Independence Constraint Qualification (LICQ)}} holds at $\mathbfit{x}^*$.
\end{dfnbox}

\subsubsection{Special Case: Only Equality Constraints}
If there is no inequality constraints, then $\mathbfit{x}^*$ is a regular point if and only if the set 
\begin{equation*}
    \left\{\nabla g_i(\mathbfit{x}^*) \colon i = 1, 2, \cdots, m\right\}
\end{equation*}
is linearly independent. In practice, this means we can just check 
\begin{equation*}
    \mathrm{rank}\left(\begin{bmatrix}
        \nabla g_1(\mathbfit{x}^*), \cdots, \nabla g_m(\mathbfit{x}^*)
    \end{bmatrix}\right)
\end{equation*}
to determine the regularity. In particular, if there is only one equality constraint, i.e., $m = 1$, then we only need to check that $\nabla g_1(\mathbfit{x}^*) \neq \mathbfit{0}$. If there are two equality constraints, i.e., $m = 2$, then we check that there is no $\lambda \in \R$ such that $\nabla g_1(\mathbfit{x}^*) = \lambda \nabla g_2(\mathbfit{x}^*)$.

\subsubsection{Special Case: Only Inequality Constraints}
If there is no equality constraints, then $\mathbfit{x}^*$ is a regular point if and only if 
\begin{equation*}
    \left\{\nabla h_j(\mathbfit{x}^*) \colon j \in J(\mathbfit{x}^*)\right\}
\end{equation*}
is linearly independent. In particular, notice that if $\mathbfit{x}^*$ is an interior point of the feasible set, then none of the inequality constraints is active, which means the set $\left\{\nabla h_j(\mathbfit{x}^*) \colon j \in J(\mathbfit{x}^*)\right\}$ is empty. However, recall that $\varnothing$ is linearly independent, so any interior point $\mathbfit{x}^*$ is always a regular point.

\subsection{Karush-Kuhn-Tucker Points}
We are now ready to generalise the Lagrange multiplier method into the KKT conditions.
\begin{dfnbox}{Karush-Kuhn-Tucker (KKT) Points}{kktPt}
    Let $\mathbfit{x}^*$ be a regular point. $\mathbfit{x}^*$ is a {\color{red} \textbf{KKT point}} if:
    \begin{itemize}
        \item there are scalars $\lambda_1, \cdots, \lambda_m$ and $\mu_1, \cdots, \mu_p$, called the {\color{red} \textbf{KKT multipliers}}, such that
        \begin{equation*}
            \nabla f(\mathbfit{x}^*) + \sum_{i = 1}^{m}\lambda_i\nabla g_i(\mathbfit{x}^*) + \sum_{j = 1}^{p}\mu_j\nabla h_j(\mathbfit{x}^*) = \mathbf{0};
        \end{equation*}
        \item $g_i(\mathbfit{x}^*) = 0$ for $i = 1, 2, \cdots, m$ and $h_i(\mathbfit{x}^*) \leq 0$ for $i = 1, 2, \cdots, p$;
        \item $\mu_i \geq 0$ for $i = 1, 2, \cdots, p$;
        \item $\mu_i = 0$ for $i \notin J(\mathbfit{x}^*)$,
    \end{itemize}
    where $J(\mathbfit{x}^*)$ is the index set of active inequality constraints at $\mathbfit{x}^*$.
\end{dfnbox}
\subsubsection{Special Case: Only One Inequality Constraint}
If there is only one inequality constraint $h_1$ and no equality constraint, then $\mathbfit{x}^*$ is a KKT Point if
\begin{equation*}
    \nabla f(\mathbfit{x}^*) + \mu_1\nabla h_1(\mathbfit{x}^*) = 0
\end{equation*}
for some $\mu_1 \geq 0$. This means that $-\nabla f(\mathbfit{x}^*)$ is in exactly the same direction as $\nabla h_1(\mathbfit{x}^*)$.
\subsubsection{Special Case: Only Two Inequality Constraints}
If there is only two inequality constraints $h_1$ and $h_2$ and no equality constraint, then $\mathbfit{x}^*$ is a KKT Point if
\begin{equation*}
    \nabla f(\mathbfit{x}^*) + \mu_1\nabla h_1(\mathbfit{x}^*) + \mu_2\nabla h_2(\mathbfit{x}^*) = 0
\end{equation*}
for some $\mu_1 \geq 0$. This means that $-\nabla f(\mathbfit{x}^*) \in \mathrm{span}\left\{\nabla h_1(\mathbfit{x}^*), \nabla h_2(\mathbfit{x}^*)\right\}$. Note that this space is a cone spanned by the outward normals to the curves $h_1(\mathbfit{x}) = 0$ and~$h_2(\mathbfit{x}) = 0$.

\subsubsection{Special Case: Interior Points}
If the local minimiser $\mathbfit{x}^*$ is an interior point of the feasible set and there is no equality constraint, then $\mathbfit{x}^*$ is a KKT Point if 
\begin{equation*}
    \nabla f(\mathbfit{x}^*) = 0.
\end{equation*}
Therefore, similar to an unconstrained NLP, $\mathbfit{x}^*$ is a stationary point.

\subsection{Karush-Kuhn-Tucker Conditions}
Before we state the KKT conditions formally, we first consider the following preliminary definition:
\begin{dfnbox}{Critical Cone}{critCone}
    Let $\mathbfit{x}^*$ be a KKT point such that
    \begin{equation*}
        \nabla f(\mathbfit{x}^*) + \sum_{i = 1}^{m}\lambda_i\nabla g_i(\mathbfit{x}^*) + \sum_{j = 1}^{p}\mu_j\nabla h_j(\mathbfit{x}^*) = \mathbfit{0},
    \end{equation*}
    then the set
    \begin{equation*}
        C(\mathbfit{x}^*, \lambda, \mu) \coloneqq \left\{\mathbfit{y} \in \mathbf{R}^n \colon \begin{array}{l}
            \nabla g_i(\mathbfit{x}^*)^{\mathrm{T}}\mathbfit{y} = 0 \quad\textrm{for } i = 1, 2, \cdots, m \\
            \nabla h_j(\mathbfit{x}^*)^{\mathrm{T}}\mathbfit{y} = 0 \quad\textrm{for } j \in J(\mathbfit{x^*}), \mu_j > 0 \\
            \nabla h_j(\mathbfit{x}^*)^{\mathrm{T}}\mathbfit{y} \leq 0 \quad\textrm{for } j \in J(\mathbfit{x^*}), \mu_j = 0
        \end{array}\right\}
    \end{equation*}
    is known as the {\color{red} \textbf{critical cone}} at $\mathbfit{x}^*$.
\end{dfnbox}
Observe that
\begin{equation*}
    \nabla f(\mathbfit{x}^*) + \sum_{i = 1}^{m}\lambda_i\nabla g_i(\mathbfit{x}^*) + \sum_{j = 1}^{p}\mu_j\nabla h_j(\mathbfit{x}^*)
\end{equation*}
is essentially $\nabla \mathcal{L}(\mathbfit{x}^*, \mathbfit{\lambda}, \mathbfit{\mu})$. Therefore, we can naturally find the Hessian of $\mathcal{L}$ as
\begin{equation*}
    H_{\mathcal{L}}(\mathbfit{x}^*) = H_f(\mathbfit{x}^*) + \sum_{i = 1}^{m}\lambda_i H_{g_i}(\mathbfit{x}^*) + \sum_{j = 1}^{p}\mu_jH_{h_j}(\mathbfit{x}^*).
\end{equation*}
\begin{thmbox}{Kurash-Kuhn-Tucker (KKT) Conditions}{kkt}
    Consider the optimisation problem
    \begin{align*}
        \min & \quad f(\mathbfit{x}) \\
        \mathrm{s.t.} & \quad g_i(\mathbfit{x}) = 0 \quad\textrm{for } i = 1, 2, \cdots, m \\
        & \quad h_j(\mathbfit{x}) \leq 0 \quad\textrm{for } j = 1, 2, \cdots, p.
    \end{align*}
    Suppose that $f$ and each of the $g_i$'s and $h_j$'s have continuous first order partial derivatives. Let $\mathbfit{x}^*$ be a regular point.
    \\\\
    The {\color{red} \textbf{Kurash-Kuhn-Tucker First Order Necessary Condition}} states that if $\mathbfit{x}^*$ is a local minimiser, then $\mathbfit{x}^*$ is a KKT point.
    \\\\
    The {\color{red} \textbf{Kurash-Kuhn-Tucker Second Order Necessary Condition}} states that if $f$ and each of the $g_i$'s and $h_j$'s have continuous second order partial derivatives, then $\mathbfit{x}^*$ is a KKT point and 
    \begin{equation*}
        \mathbfit{y}^{\mathrm{T}}H_{\mathcal{L}}(\mathbfit{x}^*)\mathbfit{y} \geq 0
    \end{equation*}
    for all $\mathbfit{y} \in C(\mathbfit{x}^*, \lambda, \mu)$, where
    \begin{equation*}
        \mathcal{L}(\mathbfit{x}^*) = \nabla f(\mathbfit{x}^*) + \sum_{i = 1}^{m}\lambda_i\nabla g_i(\mathbfit{x}^*) + \sum_{j = 1}^{p}\mu_j\nabla h_j(\mathbfit{x}^*).
    \end{equation*}
\end{thmbox}
We first justify the first order condition. Notice that the condition $\mu_j \geq 0$ can be argued in the following manner:

If $\mathbfit{x}^*$ is an interior point, then the problem is reduced to an optimisation problem with only equality constraints. By the Lagragian multiplier method, $\nabla f(\mathbfit{x}^*) + \sum_{i = 1}^{m}\lambda_i g_i(\mathbfit{x}^*) = \mathbfit{0}$. Since $\mathbfit{x}^*$ is a regular point, the set
\begin{displaymath}
    \left\{\nabla h_j(\mathbfit{x}^*) \colon j = 1, 2, \cdots, p\right\}
\end{displaymath}
is linearly independent. Therefore, all of $\mu_j$'s must be identically $0$.

If $\mathbfit{x}^*$ is on the boundary of the feasible set and there is some $\mu_j < 0$, this means that the vector $\nabla f(\mathbfit{x}^*)$ contains some component in the outwards direction of $h_j(\mathbfit{x}^*) = 0$. However, this means $f$ decreases along the inward direction from the boundary of the feasible set, so $\mathbfit{x}^*$ is not a local minimiser, which is a contradiction.

Note that the condition $\mu_i = 0$ for $i \in J(\mathbfit{x}^*)$ is equivalent to $\mu_jh_j = 0$ for aall $j \in \N^+$. The latter is known as the \textbf{Complementary Slackness Condition}. The reasoning is as follows:

If $h_j$ is active at $\mathbfit{x}^*$, then $h_j(\mathbfit{x}^*) = 0$. Otherwise, $h_j$ is slack and so we have $\mu_j = 0$. Either way, $\mu_jh_j(\mathbfit{x}^*) = 0$.

For the second order necessary condition, we may wish to argue why it suffices to consider the critical cone only.

Notice that for all $\mathbfit{z} \notin C(\mathbfit{x}^*, \lambda, \mu)$, either of the following is satisfied:

\textbf{Case I}: $\nabla g_i(\mathbfit{x}^*)^{\mathrm{T}}\mathbfit{z} \neq 0$ for some $i$. In this case, the direction of $\mathbfit{z}$ is not along the constraint curve $g_i(\mathbfit{x}) = 0$, so it is irrelevant to the NLP. 

\textbf{Case II}: $\mu_j > 0$ for some $j \in J(\mathbfit{x}^*)$ but $\nabla h_j(\mathbfit{x}^*)^{\mathrm{T}}\mathbfit{z} \neq 0$. In this case, the direction of $\mathbfit{z}$ is either towards the interior of the feasible set, along which the value of $f$ increases, or outwards from the boundary of the feasible set. Either way, there cannot be an optimal solution along the direction of $\mathbfit{z}$.

\textbf{Case III}: $\mu_j = 0$ for some $j \in J(\mathbfit{x}^*)$ but $\nabla h_j(\mathbfit{x}^*)^{\mathrm{T}}\mathbfit{z} > 0$. In this case, the direction of $\mathbfit{z}$ is outwards from the feasible set and so it is irrelevant to consider any point along the direction of $\mathbfit{z}$.

\section{Constrained Convex Optimisation Problems}
\begin{dfnbox}{Constrained Convex Non-linear Programming Problem}{constrainConvexNLP}
    A {\color{red} \textbf{constrained convex non-linear programming problem}} is defined as
    \begin{align*}
        \min & \quad f(\mathbfit{x}) \\
        \mathrm{s.t.} & \quad g_i(\mathbfit{x}) = \mathbfit{a}_i^{\mathrm{T}}\mathbfit{x} - \mathbfit{b} = 0 \quad\textrm{for } i = 1, 2, \cdots, m \\
        & \quad h_j(\mathbfit{x}) \leq 0 \quad\textrm{for } j = 1, 2, \cdots, p,
    \end{align*}
    where $f$ and the $h_j$'s are convex functions and the $g_i$'s are linear functions. 
\end{dfnbox}
\begin{thmbox}{Optimum of Constrained Convex NLPs}{constrainConvexOpt}
    Let $f$ be an objective function with equality constraints $g_i(\mathbfit{x}) = 0$ for $i = 1, 2, \cdots, m$ and ineqaulity constraints $h_j(\mathbfit{x}) \leq 0$ for $j = 1, 2, \cdots, p$. If $f$ and the $h_j$'s are differentiable convex functions and $g_i(\mathbfit{x}) = \mathbfit{a}_i^{\mathrm{T}}\mathbfit{x} - \mathbfit{b}$ is linear for $i = 1, 2, \cdots, m$. If $\mathbfit{x}^*$ is a feasible KKT point, then it is a global minimiser for $f$.
\end{thmbox}
\end{document}