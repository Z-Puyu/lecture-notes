\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Non-linear Programming
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Non-linear Programming Problems}
\section{Basic Terminology and Notations}
\begin{dfnbox}{General Non-linear Programming (NLP) Problems}{geNLPProb}
    Define the function $f \colon \mathbf{R}^n \to \mathbf{R}$. Let $\mathbfit{x} \in \mathbf{R}^n$ be a vector, then a general NLP problem aims to {\color{red} \textbf{optimise}} (i.e. maximise or minimise) $f(\mathbfit{x})$ subject to the constraint $\mathbfit{x} \in S \subseteq \mathbf{R}^n$, where
    \begin{itemize}
        \item $f$ is known as the {\color{red} \textbf{objective function}};
        \item $S$ is known as the {\color{red} \textbf{feasible set}};
        \item A solution (point) $\mathbfit{x} \in S$ is known as a {\color{red} \textbf{feasible solution (point)}}. Otherwise, it is known as an {\color{red} \textbf{infeasible solution(point)}}. 
    \end{itemize}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that to maximise $f(\mathbf{x})$ is equivalent to minimising $-f(\mathbf{x})$, so it suffices to only study minimisation problems.
    \end{remark}
\end{notebox}
The word ``optimal'', however, can be ambiguous due to its qualitative nature. Thus, we shall define what it means to be optimal quantitatively with more rigorous terms.
\begin{dfnbox}{Optimal Solution}{optSoln}
    Consider a minimisation problem subject to constraint $\mathbfit{x} \in S \subseteq \mathbfit{R}^n$ whose objective function is $f(\mathbfit{x})$. A feasible solution $\mathbfit{x}^*$ is called an {\color{red} \textbf{optimal solution}} if $f(\mathbfit{x}^*) \leq f(\mathbfit{x})$ for all $\mathbfit{x} \in S$. We can write
    \begin{equation*}
        \mathbfit{x}^* = \argmin_{\mathbfit{x} \in S} f(\mathbfit{x}).
    \end{equation*}
    $f(\mathbfit{x}^*)$ is then known as the {\color{red} \textbf{optimal value}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        For maximisation problems, we can write
        \begin{equation*}
            \mathbfit{x}^* = \argmax_{\mathbfit{x} \in S} f(\mathbfit{x})
        \end{equation*}
    \end{remark}
\end{notebox}
Note that not all optimisation problems have an optimal solution. We shall still expect to encounter problems for which no optimal solution nor value exists.
\begin{dfnbox}{Unboundedness}{nboundedness}
    Consider a minimisation problem subject to constraint $\mathbfit{x} \in S \subseteq \mathbfit{R}^n$ whose objective function is $f(\mathbfit{x})$. The objective value is said to be {\color{red} \textbf{unbounded}} if for all $K \in \mathbf{R}$, there exists some $\mathbfit{x} \in S$ such that $f(\mathbfit{x}) < K$.
\end{dfnbox}
\section{Unconstrained Non-linear Programs}
To introduce the notion of an unconstrained NLP, we shall first define the openness of a set.
\begin{dfnbox}{Open Set}{openSet}
    Let $S \subseteq \mathbfit{R}^n$ be a set. $S$ is called {\color{red} \textbf{open}} if for all $\mathbfit{x} \in S$ there exists $\epsilon > 0$ such that the ball
    \begin{equation*}
        B(\mathbfit{x}, \epsilon) \coloneqq \left\{\mathbfit{y} \in \mathbfit{R}^n \colon \left\lVert \mathbfit{y - x} \right\rVert < \epsilon\right\}
    \end{equation*}
    is a subset of $S$.
\end{dfnbox}
\begin{dfnbox}{Unconstrained NLP}{unconstrainedNLP}
    An {\color{red} \textbf{unconstrained}} NLP is an NLP whose feasible set $\mathcal{X}$ is an {\color{red} \textbf{open}} subset of $\mathbf{R}^n$.
\end{dfnbox}
\section{Constrained Non-linear Programs}
Similarly, to introduce the notion of a constrained NLP, we shall first define the closed-ness of a set.
\begin{dfnbox}{Closed Set}{closedSet}
    Let $S \subseteq \mathbfit{R}^n$ be a non-empty set. $S$ is said to be {\color{red} \textbf{closed}} if for all convergent sequences $\{\mathbfit{x}_i\}_{i = 1}^{\infty}$ with $\mathbfit{x}_i \in S$ for $i = 1, 2, \cdots$, the limit $\lim_{i \to \infty} \mathbfit{x}_i \in S$.
\end{dfnbox}
The empty set and Euclidean spaces $\R^n$ are both open and closed.
\begin{notebox}
    \begin{remark}
        Note that a set which is not open may not necessarily be closed. However, a set is open if and only if its complement is closed.
    \end{remark}
\end{notebox}
\begin{thmbox}{Intersection of Closed Sets}{intersecClosed}
    If $C_1$ and $C_2$ are both closed, then $C_1 \cap C_2$ is closed.
    \tcblower
    \begin{proof}
        The case where $C_1 \cap C_2 = \varnothing$ is trivial. If $C_1 \cap C_2 \neq \varnothing$, let $\{\mathbfit{x}_i\}_{i = 1}^{\infty}$ be an arbitrary convergent sequence in $C_1 \cap C_2$. Since $\{\mathbfit{x}_i\}_{i = 1}^{\infty} \in C_1$ which is closed, we have $\lim_{i \to \infty} \mathbfit{x}_i \in C_1$. Similarly, $\lim_{i \to \infty} \mathbfit{x}_i \in C_2$. Therefore, $\lim_{i \to \infty} \mathbfit{x}_i \in C_1 \cap C_2$.
        \\\\
        Therefore, $C_1 \cap C_2$ is closed.
    \end{proof}
\end{thmbox}
We then follow up by introducing three important closed sets.
\begin{thmbox}{}{}
    Let $g \colon \mathbf{R}^n \to \mathbf{R}$ be a continuous function, then the sets
    \begin{align*}
        S_1 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) \leq 0\right\}, \\
        S_2 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) \geq 0\right\}, \\
        S_3 & = \left\{\mathbfit{x} \in \mathbf{R}^n \colon g(\mathbfit{x}) = 0\right\}
    \end{align*}
    are closed.
    \tcblower
    \begin{proof}
        Consider $S_1$. Let $\left\{\mathbfit{x}_i\right\}_{i = 1}^\infty$ be any convergent sequence with $\mathbfit{x}_i \in S_1$ for $i = 1, 2, \cdots$, then
        \begin{equation*}
            g\left(\lim_{i \to \infty}\mathbfit{x}_i\right) \leq 0
        \end{equation*}
        since $\mathbfit{x}_i \leq 0$. Therefore, $\lim_{i \to \infty}\mathbfit{x}_i \in S_1$ and so $S_1$ is closed.

        $S_2$ and $S_3$ can be proved similarly.
    \end{proof}
\end{thmbox}
By Theorem \ref{thm:intersecClosed}, we know that $S_1 \cup S_2 \cup S_3$ is closed, which motivates the following definition:
\begin{dfnbox}{Constrained NLP}{constrainedNLP}
    A {\color{red} \textbf{constrained}} NLP is an NLP whose feasible set
    \begin{displaymath}
        S \coloneqq \left\{\mathbfit{x} \in \mathbf{R}^n \colon g_i(\mathbfit{x}) = 0, i = 1, 2, \cdots, p, h_j(\mathbfit{x}) \leq 0, j = 1, 2, \cdots, q\right\}
    \end{displaymath}
    is {\color{red} \textbf{closed}}, where each of the $g_i$'s is known as an equality constraint and each of the $h_j$'s is known as an inequality constraint.
\end{dfnbox}

\chapter{Convex Functions}
\section{Convexity of Sets and Functions}
Intuitively, we describe two types of shapes in natural languages: the shapes which, if you choose any of its edges, lies in the same side of that edge, and the shapes which span across both sides from some chosen edge of its.

Graphically, this means that some shapes are ``convex'' to all directions, where as some other shapes are ``concave''. We shall define this rigorously as follows:
\begin{dfnbox}{Convex Set}{convexSet}
    A set $D \subseteq \mathbf{R}^n$ is said to be {\color{red} \textbf{convex}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        \lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y} \in D.
    \end{displaymath}
\end{dfnbox}
We can define convexity over functions as follows:
\begin{dfnbox}{Convex Function}{convexFunc}
    A function $f \colon D \to \mathbf{R}^n$ is said to be {\color{red} \textbf{convex}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        f\left(\lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}).
    \end{displaymath}
\end{dfnbox}
\begin{dfnbox}{Concave Function}{concaveFunc}
    A function $f \colon D \to \mathbf{R}^n$ is said to be {\color{red} \textbf{concave}} if for all $\mathbfit{x}, \mathbfit{y} \in D$ and for all $\lambda \in [0, 1]$, 
    \begin{displaymath}
        f\left(\lambda \mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) \geq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}).
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A function which is not convex must be concave. However, a function which is convex may not be non-concave (consider $f(x) = x$).
    \end{remark}
\end{notebox}
We may derive the following relationship between a convex set and a convex function:
\begin{probox}{Relations between Convex Sets and Convex Functions}{cSetRCFunc}
    Let $D \subseteq \mathbf{R}^n$ be a convex set and let $f \colon D \to \mathbf{R}$ be a convex function, then for all $\alpha \in \mathbf{R}$, the set
    \begin{displaymath}
        S_\alpha \coloneqq \left\{\mathbfit{x} \in D \colon f(\mathbfit{x}) \leq \alpha\right\}
    \end{displaymath}
    is convex.
    \tcblower   
    \begin{proof}
        Take $\mathbfit{x}, \mathbfit{y} \in S_\alpha$, then $f(\mathbfit{x}) \leq \alpha$ and $f(\mathbfit{y}) \leq \alpha$. Note that for any $\lambda \in [0, 1]$, we have
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}) \\
            & \leq \lambda\alpha + (1 - \lambda)\alpha \\
            & = \alpha.
        \end{align*}
        Therefore, $\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y} \in S_\alpha$, and so $S_\alpha$ is convex.
    \end{proof}
\end{probox}
Next, we introduce the notion of an \textit{epigraph}.
\begin{dfnbox}{Epigraph}{epigraph}
    Let $f \colon D \to \mathbf{R}$ be a function over a convex set $D \subseteq \mathbf{R}^n$. The {\color{red} \textbf{epigraph}} of $f$ is the set~$E_f \subseteq \mathbf{R}^{n + 1}$ defined by
    \begin{displaymath}
        E_f \coloneqq \left\{(\mathbfit{x}, \alpha) \colon \mathbfit{x} \in D, \alpha \in \mathbf{R}, f(\mathbfit{x}) \leq \alpha\right\}.
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A trivial result: $D \times \mathrm{range}(f) \subseteq E_f$.
    \end{remark}
\end{notebox}
Note that graphically, the epigraph of a function is just the region above the graph of the function.
\begin{probox}{Convexity of Epigraph}{convexEpi}
    Let $f \colon D \to \mathbf{R}$ be a function over a convex set $D \subseteq \mathbf{R}^n$. The epigraph $E_f$ is convex if and only if $f$ is convex.
    \tcblower   
    \begin{proof}
        Suppose $E_f$ is convex. Take any $\mathbfit{x}, \mathbfit{y} \in D$, then $(\mathbfit{x}, f(\mathbfit{x})), (\mathbfit{y}, f(\mathbfit{y})) \in E_f$. Let $\lambda \in [0, 1]$, we have
        \begin{displaymath}
            \lambda(\mathbfit{x}, f(\mathbfit{x})) + (1 - \lambda)(\mathbfit{y}, f(\mathbfit{y})) = (\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y})) \in E_f.
        \end{displaymath}
        Therefore, 
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}),
        \end{align*}
        and so $f$ is convex.
        \\\\
        Suppose conversely that $f$ is convex. For any $\mathbfit{x}, \mathbfit{y} \in D$ and any $\alpha, \beta \in \mathbf{R}$ such that $f(\mathbfit{x}) \leq \alpha$ and $f(\mathbfit{y}) \leq \beta$, we have $(\mathbfit{x}, \alpha), (\mathbfit{y}, \beta) \in E_f$. For all $\lambda \in [0, 1]$, consider
        \begin{align*}
            \lambda(\mathbfit{x}, \alpha) + (1 - \lambda)(\mathbfit{y}, \beta) = (\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda\alpha + (1 - \lambda)\beta).
        \end{align*}
        Since $f$ is convex, we have
        \begin{align*}
            f\left(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\right) & \leq \lambda f(\mathbfit{x}) + (1 - \lambda)f(\mathbfit{y}) \\
            & \leq \lambda\alpha + (1 - \lambda)\beta
        \end{align*}
        for all $\lambda \in [0, 1]$. Therefore, $(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}, \lambda\alpha + (1 - \lambda)\beta) \in E_f$, and so $E_f$ is convex.
    \end{proof}
\end{probox}
Lastly, we generalise the notion of a \textit{convex combination}.
\begin{probox}{Generalised Convex Combination}{convexCombi}
    Let $k \in \N^+$ and let $f \colon S \to \mathbf{R}$ be a convex function on the convex set $S \subseteq \mathbf{R}^n$ and let $\mathbfit{x}_1, \mathbfit{x}_2, \cdots, \mathbfit{x}_k \in S$, then 
    \begin{equation*}       
        f\left(\sum_{i = 1}^{k}\lambda_i\mathbfit{x}_i\right) \leq \sum_{i = 1}^{k}\lambda_i f(\mathbfit{x}_i),
    \end{equation*}     
    where $\sum_{i = 1}^{k}\lambda_i = 1$ and $\lambda_i \geq 0$ for $i = 1, 2, \cdots, k$.
    \tcblower
    \begin{proof}
        The case where $k = 1$ is trivial.
        \\\\
        Suppose that there exists some $n \in \N^+$ such that
        \begin{equation*}
            f\left(\sum_{i = 1}^{n}\lambda_i\mathbfit{x}_i\right) \leq \sum_{i = 1}^{n}\lambda_i f(\mathbfit{x}_i),
        \end{equation*}
    \end{proof}
\end{probox}
\section{Tangent Plane Characterisation}
Recall that for a function $f \colon \mathbf{R}^n \to \mathbf{R}$, the \textit{gradient vector} of $f$ at $\mathbf{x}$ is given by
\begin{equation*}
    \nabla f(\mathbf{x}) = \begin{bmatrix}
        \frac{\partial}{\partial x_1}f(\mathbfit{x}) \\
        \frac{\partial}{\partial x_2}f(\mathbfit{x}) \\
        \vdots \\
        \frac{\partial}{\partial x_n}f(\mathbfit{x}) \\
    \end{bmatrix}.
\end{equation*}
\begin{probox}{Directional Derivative}{directionalDerivative}
    Let $f$ be a function over $D \subseteq \mathbf{R}^n$ and let $\mathbfit{d} \in \mathbf{R}^n$ be non-zero, then the directional derivative of $f$ at $\mathbfit{x}$ along $\mathbfit{d}$ is
    \begin{equation*}
        \nabla f(\mathbfit{x})^{\mathrm{T}}\mathbfit{d} = \lim_{\lambda \to 0}\frac{f(\mathbfit{x + \lambda\mathbfit{d}}) - f(\mathbfit{x})}{\lambda}.
    \end{equation*}
    In particular, if $\norm{\mathbf{d}} = 1$, then the above gives the rate of change of $f$ in the direction of $\mathbfit{d}$.
\end{probox}
\begin{notebox}
    \begin{remark}
        $f$ increases the fastest along the direction of $\nabla f(\mathbfit{x})$ and decreases the fastest along the direction of $-\nabla f(\mathbfit{x})$ for any $\mathbfit{x} \in \mathbf{R}^n$.
    \end{remark}
\end{notebox}
The gradient vector of a function allows us to establish its tangent plane at a given point. Intuitively, we can see that a function is convex if its tangent plane lies below its graph. This can be described rigorously as follows:
\begin{probox}{Tangent Plane Characterisation of Convex Functions}{convexTan}
    Let $f$ be a function over an open convex set $S \subseteq \mathbf{R}^n$ with continuous first partial derivatives, then $f$ is convex if and only if 
    \begin{equation*}
        f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y})
    \end{equation*}
    for all $\mathbfit{x}, \mathbfit{y} \in S$. In particular, $f$ is strictly convex if and only if the above inequality is strict.
    \tcblower   
    \begin{proof}
        Suppose that $f$ is convex. Let $\mathbfit{x}, \mathbfit{y} \in S$ and let $\lambda \in [0, 1]$, then we have
        \begin{equation*}
            f\left(\mathbfit{x} + \lambda(\mathbfit{y} - \mathbfit{x})\right) = f\left(\lambda\mathbfit{y} + (1 - \lambda)\mathbfit{x}\right) \leq \lambda f(\mathbfit{y}) + (1 - \lambda)f(\mathbfit{x}).
        \end{equation*} 
        Therefore,
        \begin{equation*}
            \frac{f\left(\mathbfit{x} + \lambda(\mathbfit{y} - \mathbfit{x})\right) - f(\mathbfit{x})}{\lambda} \leq f(\mathbfit{y}) - f(\mathbfit{x}).
        \end{equation*}
        Taking the limit as $\lambda \to 0$ on both sides, we have
        \begin{equation*}
            \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) = \lim_{\lambda \to 0}\frac{f\left(\mathbfit{x} + \lambda(\mathbfit{y} - \mathbfit{x})\right) - f(\mathbfit{x})}{\lambda} \leq f(\mathbfit{y}) - f(\mathbfit{x}),
        \end{equation*}
        and so
        \begin{equation*}
            f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y}).
        \end{equation*}
        Suppose conversely that 
        \begin{equation*}
            f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \leq f(\mathbfit{y}).
        \end{equation*}
        Take $\mathbfit{u}, \mathbfit{v} \in S$ and let $\mathbfit{w} = \lambda\mathbfit{u} + (1 - \lambda)\mathbfit{v}$ for some $\lambda \in [0, 1]$, then
        \begin{displaymath}
            \mathbfit{u - w} = (1 - \lambda)(\mathbfit{u - v}), \qquad \mathbfit{v - w} = -\lambda(\mathbfit{u - v}).
        \end{displaymath}
        Therefore, we have
        \begin{align*}
            f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{u - w}) & \leq f(\mathbfit{u}), \\
            f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{v - w}) & \leq f(\mathbfit{v}).
        \end{align*}
        Note that
        \begin{align*}
            \lambda\left(f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{u - w})\right) + (1 - \lambda)\left(f(\mathbfit{w}) + \nabla f(\mathbfit{w})^{\mathrm{T}}(\mathbfit{v - w})\right) = f(\mathbfit{w}),
        \end{align*}
        so we have
        \begin{align*}
            f(\lambda\mathbfit{u} + (1 - \lambda)\mathbfit{v}) & = f(\mathbfit{w}) \\
            & \leq \lambda f(\mathbfit{u}) + (1 - \lambda)f(\mathbfit{v}).
        \end{align*}
        Hence, $f$ is convex.
        \\\\
        We can similarly prove that $f$ is strictly convex if $f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) < f(\mathbfit{y})$. Now suppose that $f$ is strictly convex but there exists $\mathbfit{x}, \mathbfit{y} \in S$ with $\mathbfit{x} \neq \mathbfit{y}$ such that
        \begin{equation*}
            f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) \geq f(\mathbfit{y}).
        \end{equation*}
        Take $\mathbfit{z} = \frac{1}{2}\mathbfit{x} + \frac{1}{2}\mathbfit{y}$. We have
        \begin{align*}
            \frac{1}{2}f(\mathbfit{x}) + \frac{1}{2}f(\mathbfit{y}) & > f(\mathbfit{z}) \\
            & \geq f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{z - x}) \\
            & = f(\mathbfit{x}) + \frac{1}{2}\nabla f(\mathbfit{x})^{\mathrm{T}}\left(\mathbfit{y - x}\right) \\
            & \geq f(\mathbfit{x}) + \frac{1}{2}\left(f(\mathbfit{y}) - f(\mathbfit{x})\right) \\
            & = \frac{1}{2}f(\mathbfit{x}) + \frac{1}{2}f(\mathbfit{y}),
        \end{align*}
        which is a contradiction.
    \end{proof}
\end{probox}
Proposition \ref{pro:convexTan} helps us determine whether a point is the global minimiser of a certain convex function.
\begin{thmbox}{Global Minimiser of Convex Functions}{convexFuncMin}
    Let $f \colon C \to \mathbf{R}$ be a convex and continuously differentiable function over a convex set~$C \subseteq \mathbf{R}^n$. Then $\mathbfit{x}^* \in C$ is a global minimiser for the minimisation problem 
    \begin{displaymath}
        \min\left\{f(\mathbfit{x}) \colon \mathbfit{x} \in C\right\}
    \end{displaymath}
    if and only if
    \begin{equation*}
        \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \geq 0
    \end{equation*}
    for all $\mathbfit{x} \in C$.
    \tcblower   
    \begin{proof}
        Suppose $\nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \geq 0$. By Proposition \ref{pro:convexTan}, we have 
        \begin{equation*}
            \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}^*) \leq f(\mathbfit{x}) - f(\mathbfit{x}^*).
        \end{equation*}
        This means that $f(\mathbfit{x}) \geq f(\mathbfit{x}^*)$ for all $\mathbfit{x} \in C$, and so $\mathbfit{x}^*$ is a global minimiser.
        \\\\
        Suppose that $\mathbfit{x}^*$ is a global minimiser for $f$ but there exists some $\mathbfit{x}_0 \in C$ such that
        \begin{equation*}
            \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x}_0 - \mathbfit{x}^*) < 0.
        \end{equation*}
        Consider $\mathbfit{y} = \lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*$ for $\lambda \in (0, 1)$. Notice that $f(\mathbfit{y}) \geq f(\mathbfit{x}^*)$, so by using Proposition \ref{pro:convexTan} we have
        \begin{align*}
            0 & \leq f(\mathbfit{y}) - f(\mathbfit{x}^*) \\
            & \leq -\nabla f(\mathbfit{y})^{\mathrm{T}}(\mathbfit{y - x^*}) \\
            & \leq \nabla f(\mathbfit{y})^{\mathrm{T}}(\mathbfit{y - x^*}) \\
            & = \lambda\nabla f\left(\lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*\right)^{\mathrm{T}}(\mathbfit{x_0 - x^*}) \\
            & \leq \nabla f\left(\lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*\right)^{\mathrm{T}}(\mathbfit{x_0 - x^*}).
        \end{align*}
        Therefore,
        \begin{align*}
            \lim_{\lambda \to 0^+}\left[\nabla f\left(\lambda\mathbfit{x}_0 + (1 - \lambda)\mathbfit{x}^*\right)^{\mathrm{T}}(\mathbfit{x_0 - x^*})\right] & = \nabla f(\mathbfit{x}^*)^{\mathrm{T}}(\mathbfit{x_0 - x^*}) \\
            & \geq 0,
        \end{align*}
        which is a contradiction.
    \end{proof}
\end{thmbox}

\section{Hessian Matrices}
So far we have learnt how to determine a function's convexity by either Definition \ref{dfn:convexFunc} or Proposition \ref{pro:convexTan}. However, there are certain functions which are difficult to manipulate and apply those methods algebraically. Therefore, we introduce another method to determine convexity by \textit{Hessian Matrices}.
\begin{dfnbox}{Hessian Matrix}{hessian}
    Let $f \colon S \to \mathbf{R}$ where $S \subseteq \mathbf{R}^n$ is non-empty and let $\mathbfit{x}$ be an {\color{red} \textbf{interior point}} of $S$. The {\color{red} \textbf{Hessian}} of $f$ at $\mathbfit{x}$ is the $n \times n$ matrix
    \begin{displaymath}
        H_f(\mathbfit{x}) = \begin{bmatrix}
            \frac{\partial^2f}{\partial x_1 \partial x_1}(\mathbfit{x}) & \frac{\partial^2f}{\partial x_1 \partial x_2}(\mathbfit{x}) & \cdots & \frac{\partial^2f}{\partial x_1 \partial x_n}(\mathbfit{x}) \\
            \frac{\partial^2f}{\partial x_2 \partial x_1}(\mathbfit{x}) & \frac{\partial^2f}{\partial x_2 \partial x_2}(\mathbfit{x}) & \cdots & \frac{\partial^2f}{\partial x_2 \partial x_n}(\mathbfit{x}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2f}{\partial x_n \partial x_1}(\mathbfit{x}) & \frac{\partial^2f}{\partial x_n \partial x_2}(\mathbfit{x}) & \cdots & \frac{\partial^2f}{\partial x_n \partial x_n}(\mathbfit{x})
        \end{bmatrix}
    \end{displaymath}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        If $f$ has continuous second order derivatives, then $H_f(\mathbfit{x})$ is symmetric.
    \end{remark}
\end{notebox}
To use Hessian Matrices in convexity test, we also need to introduce the following notion of \textit{(semi)definiteness}:
\begin{dfnbox}{(Semi)Definiteness}
    Let $\mathbfit{A}$ be a real square matrix.
    \begin{itemize}
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{positive semidefinite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} \geq 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{positive definite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} > 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{negative semidefinite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} \leq 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{negative definite}} if $\mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} < 0$ for all $\mathbfit{x} \in \mathbfit{R}$.
        \item $\mathbfit{A}$ is said to be {\color{red} \textbf{indefinite}} if it is neither positive semidefinite nor negative semidefinite.
    \end{itemize}
\end{dfnbox}
Now, we are able to apply the following tests:
\begin{thmbox}{Convexity Test for Differentiable Functions}{convexTest}
    Let a function $f$ have continuous second order derivatives on an open convex set $D \subseteq \mathbf{R}^n$.
    \begin{itemize}
        \item $H_f(\mathbfit{x})$ is positive semidefinite for all $\mathbfit{x} \in D \iff f$ is convex on $D$.
        \item $H_f(\mathbfit{x})$ is positive definite for all $\mathbfit{x} \in D \implies f$ is strictly convex on $D$.
        \item $H_f(\mathbfit{x})$ is negative semidefinite for all $\mathbfit{x} \in D \iff f$ is concave on $D$.
        \item $H_f(\mathbfit{x})$ is negative definite for all $\mathbfit{x} \in D \implies f$ is strictly concave on $D$.
        \item $H_f(\mathbfit{x})$ is indefinite for some $\mathbfit{x} \in D \implies f$ is neither convex nor concave on $D$.
    \end{itemize}
\end{thmbox}
To determine the definiteness of a matrix, we may use the following eigenvalue tests:
\begin{thmbox}{Eigenvalue Test}{eigenTest}
    If $\mathbfit{A}$ is a symmetric real square matrix, then:
    \begin{itemize}
        \item $\mathbfit{A}$ is positive semidefinite if and only if all eigenvalues of $\mathbfit{A}$ are non-negative.
        \item $\mathbfit{A}$ is positive definite if and only if all eigenvalues of $\mathbfit{A}$ are positive.
        \item $\mathbfit{A}$ is negative semiefinite if and only if all eigenvalues of $\mathbfit{A}$ are non-positive.
        \item $\mathbfit{A}$ is negative definite if and only if all eigenvalues of $\mathbfit{A}$ are negative.
        \item $\mathbfit{A}$ is indefinite if and only if it has at least one positive eigenvalue and at least one negative eigenvalue.
    \end{itemize}
    \tcblower
    \begin{proof}
        We will only prove that $\mathbfit{A}$ is positive semidefinite if and only if all eigenvalues of $\mathbfit{A}$ are non-negative. The rest of the tests can be proven similarly.
        \\\\
        Suppose $\mathbfit{A}$ is positive semidefinite. Let $\lambda$ be any eigenvalue of $\mathbfit{A}$ and let $\mathbfit{x}$ be a corresponding eigenvector, then $\mathbfit{Ax} = \lambda\mathbfit{x}$. Therefore,
        \begin{equation*}
            \lambda\norm{\mathbfit{x}}^2 = \lambda\mathbfit{x}^{\mathrm{T}}\mathbfit{x} = \mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} \geq 0.
        \end{equation*}
        Since $\norm{\mathbfit{x}}^2 \geq 0$, this implies that $\lambda \geq 0$.
        \\\\
        Suppose conversely that all eigenvalues of $\mathbfit{A}$ are non-negative. Consider the diagonal matrix
        \begin{displaymath}
            \mathbfit{D} = \begin{bmatrix}
                \lambda_1 & 0 & \cdots & 0 \\
                0 & \lambda_2 & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \lambda_n
            \end{bmatrix}
        \end{displaymath}
        where $\lambda_i$ is the $i$-th eigenvalue of $\mathbfit{A}$. Then there exists some orthogonal square matrix $\mathbfit{Q}$ such that 
        \begin{equation*}
            \mathbfit{A} = \mathbfit{QDQ}^{\mathrm{T}}.
        \end{equation*}
        Let $\mathbfit{x} \in \mathbf{R}^n$ be an arbitrary vector, then 
        \begin{align*}
            \mathbfit{x}^{\mathrm{T}}\mathbfit{Ax} & = \mathbfit{x}^{\mathrm{T}}\mathbfit{QDQ}^{\mathrm{T}}\mathbfit{x} \\
            & = \left(\mathbfit{Q}^{\mathrm{T}}\mathbfit{x}\right)^{\mathrm{T}}\mathbfit{D}\left(\mathbfit{Q}^{\mathrm{T}}\mathbfit{x}\right) \\
            & = \sum_{i = 1}^{n}\left(\lambda_i\norm{\mathbfit{Q}^{\mathrm{T}}\mathbfit{x}}^2\right) \\
            & \geq 0.
        \end{align*}
        Therefore, $\mathbfit{A}$ is positive semidefinite.
    \end{proof}
\end{thmbox}
However, eigenvalues can be troublesome to compute. Thus for \textbf{small matrices}, we may use the following test with the \textit{principal minors}:
\begin{dfnbox}{Principal Minor}{prinMinor}
    Let $\mathbfit{A}$ be an $n \times n$ matrix. The $k$-th {\color{red} \textbf{principal minor}} $\Delta_k$ of $\mathbfit{A}$ is defined to be the determinant of the $k$th principal submatrix of $\mathbfit{A}$, i.e.,
    \begin{equation*}
        \Delta_k = \begin{vmatrix}
            a_{11} & a_{12} & \cdots & a_{1k} \\
            a_{21} & a_{22} & \cdots & a_{2k} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{k1} & a_{k2} & \cdots & a_{kk} \\
        \end{vmatrix}
    \end{equation*}
\end{dfnbox}
\begin{thmbox}{Definiteness Test Using Principal Minors}{prinMinorDefTest}
    Let $\mathbfit{A}$ be a symmetric $n \times n$ matrix, then $\mathbfit{A}$ is positive definite if and only if $\Delta_k > 0$, and negative definite if and only if $(-1)^k\Delta_k > 0$, for all $k = 1, 2, \cdots, n$.
\end{thmbox}
\subsection{Taylor's Theorem}
Note that we have not given the proof to Theorem \ref{thm:convexTest}. Now to prove it, we need to first introduce \textit{Taylor's Theorem}.
\begin{thmbox}{Taylor's Theorem}{taylorThm}
    Suppose that $f \colon S \to \mathbf{R}$ is a function with continuous second partial derivatives. Consider the set
    \begin{displaymath}
        [\mathbfit{x}, \mathbfit{y}] \coloneqq \left\{\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y} \colon \mathbfit{x}, \mathbfit{y} \in \mathbf{R}^n, \lambda \in [0, 1]\right\}.
    \end{displaymath}
    If $[\mathbfit{x}, \mathbfit{y}]$ is contained in the interior of $S$, then there exists some $\mathbfit{z} \in [\mathbfit{x}, \mathbfit{y}]$ such that
    \begin{equation*}
        f(\mathbfit{y}) = f(\mathbfit{x}) + \nabla f(\mathbfit{x})^{\mathrm{T}}(\mathbfit{y - x}) + \frac{1}{2}(\mathbfit{y - x})^{\mathrm{T}}H_f(\mathbfit{z})(\mathbfit{y - x}).
    \end{equation*}
\end{thmbox}
\chapter{Unconstrained NLPs}
\section{Coercive Functions}
To verify the existence of global minimisers for unbounded functions, we introduce the notion of \textit{coercive functions}.
\begin{dfnbox}{Coercive Function}{coerciveFunc}
    A {\color{red} \textbf{continuous}} function $f \colon \mathbf{R}^n \to \mathbf{R}$ is said to be {\color{red} \textbf{coercive}} if
    \begin{equation*}
        \lim_{\norm{\mathbfit{x}} \to \infty} f(\mathbfit{x}) = +\infty.
    \end{equation*}
    More formally, $f$ is coercive if and only if for all $M > 0$, there is some $r > 0$ such that~$f(\mathbfit{x}) > M$ whenever $\norm{\mathbfit{x}} > r$.
\end{dfnbox}
\begin{thmbox}{Global Minimiser of Coercive Functions}{globalMinCoerciveFunc}
    If a function $f \colon \mathbf{R}^n \to \mathbf{R}$ is coercive, then $f$ has at least one global minimiser.
    \tcblower   
    \begin{proof}
        Take $M = \abs{f(\mathbfit{0})} + 1 > 0$. Since $f$ is coercive, there exists some $r > 0$ such that $f(\mathbfit{x}) > M > f(\mathbfit{0})$ whenever $\norm{\mathbfit{x}} > r$. Consider
        \begin{displaymath}
            B(\mathbfit{0}, r) = \left\{\mathbfit{x} \in \mathbf{R}^n \colon \norm{\mathbfit{x}} \leq r\right\}
        \end{displaymath}
        which is a compact set. So by Weiestrass' Theorem, there exists some $\mathbfit{x}^* \in B(\mathbfit{0}, r)$ such that $f(\mathbfit{x}^*) \leq f(\mathbfit{x}) \leq f(\mathbfit{0})$ for all $\mathbfit{x} \in B(\mathbfit{0}, r)$. Now, this means that for all $\mathbfit{x} \in \mathbf{R}^n$, we have $(\mathbfit{x}^*) \leq f(\mathbfit{x})$, which means that $\mathbfit{x}^*$ is a global minimiser for $f$.
    \end{proof}
\end{thmbox}

\section{Stationary Points}
It should be natural to think of a \textit{stationary point} when we try to determine the local extrema of a function.
\begin{dfnbox}{Stationary (Critical) Point}{stationaryPt}
    Let $X \subseteq \mathbf{R}^n$ be an open set and let $f \colon X \to \mathbf{R}$ be a function. An interior point $\mathbfit{x}^*$ is called a {\color{red} \textbf{stationary point}} of $f$ if $\nabla f(\mathbfit{x}^*) = \mathbfit{0}$.
\end{dfnbox}
Note that it is not sufficient to ascertain the existence of local extrema from stationary points, but the converse always holds.
\begin{thmbox}{Necessary Condition for Local Extrema}{ifLocalExtremaThen}
    Let $\mathcal{X} \subseteq \mathbf{R}^n$ be an open set and let $f \colon \mathcal{X} \to \mathbf{R}$ have continuous first order partial derivatives on $\mathcal{X}$. If $\mathbfit{x^*} \in \mathcal{X}$ is a local minimiser (maximiser) of $f$, then it is a stationary point, and $H_f(\mathbfit{x^*})$ is positive (negative) semidefinite if $f$ has continuous second order partial derivatives.
\end{thmbox}
We can use the above theorem to rule out the cases where a stationary point is not a point of local extrema. In this case, the stationary point is known as a \textit{saddle} point.
\begin{dfnbox}{Saddle Point}{Saddle}
    A stationary point $\mathbfit{x}^*$ of a function $f$ which is neither a local minimisr nor a local maximiser is called a {\color{red} \textbf{saddle point}}.
\end{dfnbox}
\begin{corbox}{}{saddleCor}
    Let $\mathbfit{x}^*$ be a stationary point of $f$. If $H_f(\mathbfit{x}^*)$ is indefinite, then $\mathbfit{x}^*$ is a saddle point.
\end{corbox}
Note that we can impose a stronger requirement on top of Theorem \ref{thm:ifLocalExtremaThen} to derive a sufficient condition for local minimisers and maximisers.
\begin{thmbox}{Sufficient Condition for Local Extrema}{ifThenLocalExtrema}
    Let $\mathcal{X} \subseteq \mathbf{R}^n$ be an open set and let $f \colon \mathcal{X} \to \mathbf{R}$ have continuous second order partial derivatives on $\mathcal{X}$. If $\mathbfit{x^*} \in \mathcal{X}$ is a stationary point and $H_f(\mathbfit{x^*})$ is positive (negative) definite, then $\mathbfit{x^*}$ is a strict local minimiser (maximiser) of $f$.
\end{thmbox}
Note that the above is simply the generalised version of the \textbf{second order derivative test} in one-dimensional calculus.

\section{Unconstrained Convex NLP}
A convex function defined over a convex set has many nice properties. For example, the local minimiser of such a function would also be the global minimiser, which is formulated into the following:
\begin{thmbox}{Global Minimisers of Convex Functions}{convexFuncGlobalMin}
    Let $f \colon D \to \mathbf{R}$ be a convex function where $D$ is a non-empty open convex set. Suppose that $\mathbfit{x^*} \in D$ is a local minimiser of $f$, then it is a global minimiser of $f$. In particular, if $f$ is strictly convex, then $\mathbfit{x^*}$ is the unique global minimiser.
\end{thmbox}
Linking back to Proposition \ref{pro:convexTan} and Definition \ref{dfn:stationaryPt}, we arrive at the following corollary:
\begin{corbox}{Stationary Points and Global Minimisers of Convex Functions}{stationaryGlovalMinConvexFunc}
    If $f$ is a convex (respectively concave) function with continuous first partial derivatives on some open convex set $D$, then any stationary point of $f$ is a global minimiser (respectively maximiser) of $f$.
\end{corbox}

\subsection{Unconstrained Convex Quadratic Programming}
Quadratic functions are a basic type of functions in which we are interested in many NLPs.
\begin{dfnbox}{Quadratic Function}{quadFunc}
    A {\color{red} \textbf{quadratic function}} $q \colon \mathbf{R}^n \to \mathbf{R}$ is defined by
    \begin{equation*}
        q(\mathbfit{x}) = \frac{1}{2}\mathbfit{x}^{\mathrm{T}}\mathbfit{Qx} + \mathbfit{c}^{\mathrm{T}}\mathbfit{x}
    \end{equation*}
    where $\mathbfit{Q}$ is an $n \times n$ symmetric matrix and $\mathbfit{c} \in \mathbf{R}^n$.
\end{dfnbox}
Note that $\mathbfit{Q}$ and $\mathbfit{c}$ are essentially the coefficients for the quadratic terms and linear terms respectively. In particular, we can determine the convexity of a quadratic function with the following theorem:
\begin{thmbox}{Convexity of Quadratic Functions}{convexQuad}
    Consider the quadratic function $q \colon \mathbf{R}^n \to \mathbf{R}$ defined by
    \begin{equation*}
        q(\mathbfit{x}) = \frac{1}{2}\mathbfit{x}^{\mathrm{T}}\mathbfit{Qx} + \mathbfit{c}^{\mathrm{T}}\mathbfit{x}
    \end{equation*}
    where $\mathbfit{Q}$ is an $n \times n$ symmetric matrix and $\mathbfit{c} \in \mathbf{R}^n$. $q$ is convex if and only if $\mathbfit{Q}$ is positive semidefinite.
\end{thmbox}
Analogously to the one-dimensional case, we have the following result:
\begin{thmbox}{Global Minimiser of Quadratic Functions}{QuadGlobalMin}
    Let $\mathbfit{c} \in \mathbf{R}^n$ and let $\mathbfit{Q}$ be a positive semidefinite $n \times n$ symmetric matrix, then the quadratic function
    \begin{equation*}
        q(\mathbfit{x}) = \frac{1}{2}\mathbfit{x}^{\mathrm{T}}\mathbfit{Qx} + \mathbfit{c}^{\mathrm{T}}\mathbfit{x}
    \end{equation*}
    over an open convex set has a global minimiser $\mathbfit{x^*}$ if and only if $\mathbfit{Qx^*} = -\mathbfit{c}$.
\end{thmbox}
Clearly, if $\mathbfit{Q}$ is invertible, we have $\mathbfit{x^*} = -\mathbfit{Q}^{-1}\mathbfit{c}$.

\chapter{Numerical Methods}
\section{Bisection}
Recall the \textit{Intermediate Value Theorem}:
\begin{thmbox}{Intermediate Value Theorem}{interVal}
    Let $f$ be a continuous function on $[a, b]$ with $f(a)f(b) < 0$, then there exists some $r \in (a, b)$ such that $f(r) = 0$.
\end{thmbox}
\begin{codebox}{Sample Code in Python}{}
    \begin{amzcode}{python3}
        def bisection(f: Callable[[float], float],\
            a: float, b: float, t: float, x: float) -> float:
            while (Math.abs(a - b) > t):
                if (f(a) * f(x) < 0):
                    b = x;
                    x = (b - a) / 2;
            return x;
    \end{amzcode}
\end{codebox}

\section{Newton's Method}
\subsection{Multivariable Case}
Let $f$ be a multivariable function with continuous second order partial derivatives. Given $\mathbfit{x_0} \in \mathbf{R}^n$, by Taylor expansion there is some quadratic function $q$ such that
\begin{equation*}
    f(\mathbfit{x}) \approx q(\mathbfit{x}) = f(\mathbfit{x}_0) + \nabla f(\mathbfit{x}_0)^{\mathrm{T}}(\mathbfit{x} - \mathbfit{x}_0) + \frac{1}{2}(\mathbfit{x} - \mathbfit{x}_0)^{\mathrm{T}}H_f(\mathbfit{x}_0)(\mathbfit{x} - \mathbfit{x}_0).
\end{equation*}
Assumptions:
\begin{itemize}
    \item The Hessian $H_f(\mathbfit{x}^*)$ at the stationary point $\mathbfit{x}^*$ is non-singular.
    \item $H_f(\mathbfit{x})$ is Lipschitz continuous in a neighbourhood of $\mathbfit{x}^8$.
\end{itemize}
\begin{probox}{Convergence of the Newton Method}{newtonConverge}
    If $\mathbfit{x}_0$ is sufficiently close to $\mathbfit{x}^*$, then the sequence $\{\mathbfit{x}_k\}$ generated by the Newton Method converges to $\mathbfit{x}^*$ quadratically, i.e., there exists some $M \in \R$ such that
    \begin{equation*}
        \norm{\mathbfit{x}_{k + 1} - \mathbfit{x}^*} \leq M\norm{\mathbfit{x}_k - \mathbfit{x}^*}^2.
    \end{equation*}
\end{probox}
\subsection{Armijo Line Search}

\section{Goldern Section Method}
\begin{dfnbox}{Unimodal Function}{unimodal}
    A function $f$ is said to be {\color{red} \textbf{unimodal}} on $[a, b]$ if it has exactly one global minimiser (maximiser) $x^*$ in $[a, b]$ and is strictly decreasing (increasing) on $[a, x^*]$ abd strictly increasing (decreasing) on $[x^*, b]$.
\end{dfnbox}
\begin{tecbox}{Golden Section Method}{goldeSect}
    Let $f$ be a unimodal function. Consider $[a_0, b_0]$ as the initial interval. For each iteration, we consider
    \begin{align*}
        \lambda_n & = b_n - \Phi(b_n - a_n) \\
        \mu_n & = a_n + \Phi(b_n - a_n)
    \end{align*}
    where $\Phi = \frac{\sqrt{5} - 1}{2}$ is the Golden Ratio constant. Then we update the interval by:
    \begin{equation*}
        [a_{n + 1}, b_{n + 1}] = \begin{cases}
            [\lambda_n, b_n], \quad \textrm{if } f(\lambda_n) > f(\mu_n)\\
            [a_n, \mu_n], \quad \textrm{if } f(\lambda_n) < f(\mu_n)\\
            [\lambda_n, \mu_n], \quad \textrm{if } f(\lambda_n) = f(\mu_n)
        \end{cases}.
    \end{equation*}
    The reason why we use $\Phi$ to generate the intervals recursively is because that it reduces the number of computations. Consider that $[a_k, b_k] = [\lambda_{k - 1}, b_{k - 1}]$, then
    \begin{align*}
        \lambda_k & = b_k - \Phi(b_k - a_k) \\
        & = b_{k - 1} - \Phi(b_{k - 1} - \lambda_{k - 1}) \\
        & = b_{k - 1} - \Phi(b_{k - 1} - b_{k - 1} + \Phi(b_{k - 1} - a_{k - 1})) \\
        & = b_{k - 1} - \Phi^2(b_{k - 1} - a_{k - 1}) \\
        & = b_{k - 1} - (1 - \Phi)(b_{k - 1} - a_{k - 1}) \\
        & = a_{k - 1} + \Phi(b_{k - 1} - a_{k - 1}) \\
        & = \mu_{k - 1}.
    \end{align*}
    However, note that we have already computed $\mu_{k - 1}$ in the previous iteration! Therefore, by applying ideas of \textit{dynamic programming}, this eliminates the need to re-compute its value. Similarly, we can show that $\mu_k = \lambda_{k - 1}$ if $f(\lambda_k) < f(\mu_k)$.
\end{tecbox}

\section{General Framework of Optimisation Algorithms}
\begin{genbox}{Pseudo-code}{}
    \begin{verbatim}
Initialise x with some guess;
while the interval > tolerance, do:
    x -> f(x);
return x;
    \end{verbatim}
\end{genbox}
\end{document}