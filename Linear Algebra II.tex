\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\I}{\mathbfit{I}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Linear Algebra II
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Vector Spaces}
\section{Fields, Scalars and Vectors}
In elementary mathematics, we often refer to a vector as an ordered tuple of numbers with a direction and a magnitude. However, there is a much more abstract aspect to the notion of vectors. In fact, let us first generalise the notion of \textit{scalars}, which are taken as complex constants in an elementary level. 

In general, we have the following algebraic structure:
\begin{dfnbox}{Field}{field}
    A {\color{red} \textbf{field}} is a set $\mathcal{F}$ with two binary operations $\mathcal{F}^2 \to \mathcal{F}$, namely addition and multiplication, such that
    \begin{enumerate}
        \item $u + v = v + u$ for all $u, v \in \mathcal{F}$;
        \item $(u + v) + w = u + (v + w)$ for all $u, v, w \in \mathcal{F}$;
        \item $uv = vu$ for all $u, v \in \mathcal{F}$;
        \item $(uv)w = u(vw)$ for all $u, v, w \in \mathcal{F}$;
        \item $u(v + w) = uv + uw$ for all $u, v, w \in \mathcal{F}$;
        \item there exists $0 \in \mathcal{F}$ such that $u + 0 = u$ for all $u \in \mathcal{F}$;
        \item there exists $1 \in \mathcal{F}$ such that $1u = u$ for all $u \in \mathcal{F}$;
        \item for every $u \in \mathcal{F}$, there exists some $v \in \mathcal{F}$ such that $u + v = 0$;
        \item for every $u \in \mathcal{F}$, there exists some $v \in \mathcal{F}$ such that $uv = 1$.
    \end{enumerate}
\end{dfnbox}
One may check that both $\R$ and $\C$ are fields. It turns out that we can also generalise the concept of vectors as any objects which possess properties similar to that of Euclidean vectors, i.e., we can view a vector as a mathematical quantity which can be added up and multiplied by another quantity called a scalar with some axioms which they follow. Rigorously, we define the notion of a \textit{vector space}.
\begin{dfnbox}{Vector Space}{vecSpace}
    A {\color{red} \textbf{vector space}} is a set $V$ over a field $\mathcal{F}$ with two binary operations, namely 
    \begin{itemize}
        \item addition $+ \colon V^2 \to V$, and
        \item scalar multiplication $(\quad)(\quad) \colon \mathcal{F} \times V \to V$,
    \end{itemize}
    such that
    \begin{enumerate}
        \item $\mathbfit{u + v = v + u}$ for all $\mathbfit{u}, \mathbfit{v} \in V$;
        \item $\mathbfit{(u + v) + w = u + (v + w)}$ for all $\mathbfit{u, v, w} \in V$;
        \item $ab\mathbfit{v} = a(b\mathbfit{v})$ for all $a, b \in \mathcal{F}$ and $\mathbfit{v} \in V$;
        \item there exists an {\color{red} \textbf{additive identity}} or {\color{red} \textbf{zero vector}} $\zero \in V$ such that $\mathbfit{v} + \zero = \mathbfit{v}$ for all $\mathbfit{v} \in V$;
        \item every $\mathbfit{v} \in V$ has an {\color{red} \textbf{additive inverse}} $\mathbfit{w} \in V$ with $\mathbfit{v + w} = 0$;
        \item there exists a {\color{red} \textbf{multiplicative identity}} $1 \in \mathcal{F}$ such that $1\mathbfit{v} = \mathbfit{v}$ for all $\mathbfit{v} \in V$;
        \item $a\mathbfit{(u + v)} = a\mathbfit{u} + a\mathbfit{v}$ and $(a + b)\mathbfit{u} = a\mathbfit{u} + b\mathbfit{u}$ for all $a, b \in \mathcal{F}$ and $\mathbfit{u, v} \in V$.
    \end{enumerate}
\end{dfnbox}
Notice that here, the definitions of addition in scalar multiplication in a vector space imply that any vector space must be \textbf{closed} under these two operations. Notice also that the operations ``addition'' and ``scalar multiplication'' are not necessary the addition and scalar multiplication which we are used to in $\R^n$, but abstract mappings which satisfy the given axioms.

We shall prove a few basic properties regarding vector spaces.
\begin{thmbox}{Uniqueness of Additive Identity}{unique0}
    Let $V$ be a vector space with $\zero \in V$ as an additive identity, then $\zero$ is unique.
    \tcblower
    \begin{proof}
        Suppose on contrary that there exists $\mathbfit{u} \in V$ such that $\mathbfit{v + u = v}$ for all $\mathbfit{v} \in V$. Since $\zero \in V$, we have
        \begin{equation*}
            \zero + \mathbfit{u} = \zero.
        \end{equation*}
        However, $\zero$ is the additive identity, so 
        \begin{equation*}
            \mathbfit{u} = \mathbfit{u} + \zero = \zero + \mathbfit{u} = \zero,
        \end{equation*}
        i.e. $\zero$ is unique.
    \end{proof}
\end{thmbox}
Similarly, we can also prove the uniqueness of additive inverse.
\begin{thmbox}{Uniqueness of Additive Inverse}{unique-1}
    Let $V$ be a vector space, then every $\mathbfit{v} \in V$ has a unique additive inverse.
    \tcblower
    \begin{proof}
        Suppose on contrary that there exist $\mathbfit{u, w} \in V$ both being additive inverse of~$\mathbfit{v}$, then $\mathbfit{u + v} = \zero$ and $\mathbfit{w + v} = \zero$. Therefore,
        \begin{equation*}
            \mathbfit{u} = \mathbfit{(u + v) + u} = \mathbfit{(w + v) + u} = \mathbfit{w + (u + v)} = \mathbfit{w},
        \end{equation*}
        i.e., $\mathbfit{v}$ has a unique additive inverse.
    \end{proof}
\end{thmbox}
Theorem \ref{thm:unique-1} justifies the notation $-\mathbfit{u}$ to denote the additive inverse of $\mathbfit{u}$. However, so far we have not ascertained the fact that $-\mathbfit{u} = (-1)\mathbfit{u}$ (note that the former means the inverse of $\mathbfit{u}$ while the latter means $\mathbfit{u}$ multiplied by the scalar $-1$)! While seemingly innocent, this result is not as easily proven as it looks.

First, we shall justify that $0\mathbfit{u} = \zero$ for all $\mathbfit{u} \in V$. Notice that
\begin{equation*}
    0\mathbfit{u} = (0 + 0)\mathbfit{u} = 0\mathbfit{u} + 0\mathbfit{u}.
\end{equation*}
Adding $-(0\mathbfit{u})$ to both sides of the equation yields $0\mathbfit{u} = \zero$ as desired. From this result we see that
\begin{equation*}
    (-1)\mathbfit{u} + \mathbfit{u} = (-1 + 1)\mathbfit{u} = 0\mathbfit{u} = \zero.
\end{equation*}
By uniqueness of additive inverse, we must have $(-1)\mathbfit{u} = -\mathbfit{u}$.

Note that by using a similar technique we can prove that $a\zero = \zero$ for all $a \in \mathcal{F}$, and so~$\zero = -\zero$ as a consequence.

Additionally, note that subtraction is defined as $\mathbf{u - v} = \mathbfit{u} + (-1)\mathbfit{v}$, so the above result allows us to write $\mathbfit{u - v} = \mathbfit{u} + (-\mathbfit{v})$.

\subsection{Subspaces}
Note that a vector space is extended based on a set of vectors, so we can define \textit{subspaces} similarly to the notion of subsets.
\begin{dfnbox}{Subspace}{subspace}
    Let $V$ be a vector space. $U \subseteq V$ is called a {\color{red} \textbf{subspace}} if $U$ is a vector space under addition and scalar multiplication in $V$.
\end{dfnbox}
It is easy to see that the intersection of any number of subspaces of a vector space $V$ is still a subspace of $V$, but the union might not be so. In particular, we would like to consider a special construct known as \textit{direct sum}.
\begin{dfnbox}{Direct Sum}{directSum}
    Let $V$ be a vector space and $U_1, U_2 \subseteq V$ such that $U_1 \cap U_2 = \{\zero\}$, then their {\color{red} \textbf{direct sum}} is defined as
    \begin{equation*}
        U_1 \oplus U_2 \coloneqq \left\{\mathbfit{u}_1 + \mathbfit{u}_2 \colon \mathbfit{u}_1 \in U_1, \mathbfit{u}_2 \in U_2\right\}.
    \end{equation*}
\end{dfnbox}
More generally, we can let $U_1$ and $U_2$ be any subsets of $V$ and define $U_1 + U_2$ in the same manner, which is known as the \textit{sum} of $U_1$ and $U_2$.

It can be easily proven that for any vector space $V$, the direct sum of any two subspaces of $V$ is still a subspace of $V$. A nice property of direct sum can be proven as follows:
\begin{probox}{Unique Decomposition with Direct Sums}{uniqueDecomp}
    Let $V = U_1 \oplus U_2$, then every $\mathbfit{v} \in V$ can be uniquely expressed as $\mathbfit{u + w}$ for some $\mathbfit{u} \in U_1$ and $\mathbfit{w} \in U_2$.
    \tcblower
    \begin{proof}
        The existence of $\mathbfit{u}$ and $\mathbfit{w}$ is trivial by Definition \ref{dfn:directSum}. Suppose there exist $\mathbfit{u}' \in U_1$ and $\mathbfit{w}' \in U_2$ such that $\mathbfit{u + w} = \mathbfit{u}' + \mathbfit{w}'$, then we have $\mathbfit{u - u}' = \mathbfit{w}' - \mathbfit{w}$. Note that $\mathbfit{u - u}' \in U_1$ and $\mathbfit{w}' - \mathbfit{w} \in U_2$, so we have $\mathbfit{u - u}', \mathbfit{w}' - \mathbfit{w} \in U_1 \cap U_2 = \{\zero\}$, i.e.,
        \begin{equation*}
            \mathbfit{u - u}' = \mathbfit{w}' - \mathbfit{w} = \zero.
        \end{equation*} 
        Therefore, $\mathbfit{u} = \mathbfit{u}'$ and $\mathbfit{w} = \mathbfit{w}'$, i.e., $\mathbfit{u}$ and $\mathbfit{w}$ are unique.
    \end{proof}
\end{probox}
In some sense, a direct sum of $V$ can be viewed as a ``partition'' of $V$ into two subsets with a minimal overlap. Note that unlike partition in its real definition, the subspaces $U_1$ and $U_2$ here cannot be disjoint sets as both of them have to contain the zero vector in $V$. More generally, for any subspace $U \subseteq V$, we have $\zero_U = \zero_V$, the proof of which should be trivial enough as an exercise to the reader.

In particular, we would like to consider $\mathcal{F}^n$ for a general field $\mathcal{F}$. We can define the dot product operation over $\mathcal{F}^n$ in the same way as $\R^n$. Take any subspace $U \subseteq \mathcal{F}^n$ and define the set
\begin{equation*}
    U_{\perp} \coloneqq \left\{\mathbfit{u} \in \mathcal{F}^n \colon \mathbfit{u \cdot v} = 0 \quad\textrm{for all } \mathbfit{v} \in U\right\},
\end{equation*}
then $\mathcal{F}^n = U \oplus U_{\perp}$.

To justify this, we first take any $\mathbfit{v} \in \mathcal{F}^n$. Using some calculus, we can show that there exists 
\begin{equation*}
    \mathbfit{u}_0 = \argmin_{\mathbfit{u} \in U}\abs{\mathbfit{u \cdot v}}.
\end{equation*}
Let $\mathbfit{w = v - u}_0$, then clearly $\mathbfit{v = w + u}_0$ where $\mathbfit{u}_0 \in U$ and $\mathbfit{w} \in U_{\perp}$. This implies that~$V = U + U_{\perp}$. Note that $\zero$ is the only vector in $\mathcal{F}^n$ which is orthogonal to itself, so we have $U \cap U_{\perp} = \{\zero\}$. It follows that $V = U \oplus U_{\perp}$.

\section{Isomorphism}
Since the underlying structure of a vector space is still a set, the notion of a mapping between two vector spaces is well-defined. However, note that a vector space possesses unique algebraic structures and properties, namely that the linear combinations of any members of the space are still in the space, so we would like to focus on mapping which preserves such properties.
\begin{dfnbox}{Homomorphism}{homomorphic}
    Let $U$ and $V$ be vector spaces, a {\color{red} \textbf{homomorphism}} is a mapping $\phi \colon U \to V$ such that
    \begin{equation*}
        \phi(\mathbfit{u + v}) = \phi(\mathbfit{u}) + \phi(\mathbfit{v})
    \end{equation*}
    for any $\mathbfit{u}, \mathbfit{v} \in U$.
\end{dfnbox}
Note that it suffices to only require $\phi(\mathbfit{u + v}) = \phi(\mathbfit{u}) + \phi(\mathbfit{v})$ but not $\phi(c\mathbfit{u}) = c\phi(\mathbfit{u})$. Here, note that a vector space is a connected set, so we can easily prove the above claim with some knowledge in mathematical analysis.

Naturally, if a homomorphism is bijective, then it means that the elements in two vector spaces have a one-to-one correspondence. In practice, this means we can treat them as equivalent spaces in some sense.
\begin{dfnbox}{Isomorphism}{isomorphic}
    An {\color{red} \textbf{isomorphism}} between vector spaces $U$ and $V$ is a homomorphism between them which is bijective.
\end{dfnbox}
An interesting fact here is that an isomorphism between any vector spaces is not unique. To see this, let us first consider an arbitrary vector space $V$. Now, we can always find the trivial isomorphism $\mathrm{id}_V \colon V \to V$. In fact, any mapping $\phi \colon \mathbfit{v} \mapsto c\mathbfit{v}$ where $c$ is a scalar is clearly an isomorphism from $V$ to $V$. This means that there are infinitely many isomorphisms from~$V$ to itself.

Let $U$ be an arbitrary vector space such that there exists some isomorphism $\psi \colon V \to U$. We consider the following theorem:
\begin{thmbox}{Composition Preserves Isomorphism}{compoPreserveIsomorphic}
    Let $U, V, W$ be vector spaces. If $\phi \colon U \to V$ and $\psi \colon V \to W$ are isomorphisms, then the composite mapping $\phi \circ \psi \colon U \to W$ is an isomorphism.
    \tcblower
    \begin{proof}
        Since both $\phi$ and $\psi$ are bijective, it is clear that $\psi \circ \phi$ is bijective. Take any $\mathbfit{u}, \mathbfit{v} \in U$, then
        \begin{align*}
            \psi\bigl(\phi(\mathbfit{u + v})\bigr) = \psi\bigl(\phi(\mathbfit{u}) + \phi(\mathbfit{v})\bigr) = \psi\bigl(\phi(\mathbfit{u})\bigr) + \psi\bigl(\phi(\mathbfit{v})\bigr)
        \end{align*}
        since $\phi(\mathbfit{u}), \phi(\mathbfit{v}) \in V$. Therefore, $\psi \circ \phi$ is an isomorphism.
    \end{proof}
\end{thmbox}
Using Theorem \ref{thm:compoPreserveIsomorphic}, we can immediately see that if $\phi \colon V \to V$ is any isomorphism and $\psi \colon V \to U$ is an isomorphism, then $\psi \circ \phi$ is an isomorphism between $V$ and $U$. Therefore, there are infinitely many isomorphisms between $V$ and $U$.
\begin{notebox}
    \begin{remark}
        If $V$ is isomorphic to $U$, we write $V \cong U$. Clearly, $\cong$ is an equivalence relation.
    \end{remark}
\end{notebox}

Now, observe that for any field $\mathcal{F}$, the set $\mathcal{F}^n$ for any $n \in \N$ is a vector space over $\mathcal{F}$.
\begin{dfnbox}{Finite-Dimensional Vector Space}{finiteDim}
    A vector space $V$ is said to be {\color{red} \textbf{finite-dimensional}} over a field $\mathcal{F}$ if it it is isomorphic to $\mathcal{F}^n$ for some $n \in \N$. $n$ is called the {\color{red} \textbf{dimension}} of $V$.
\end{dfnbox}
Obviously, a vector space which is not finite-dimensional is called \textit{infinite-dimensional}. For example, the set of all polynomials is a vector space of infinite dimension.

\section{Basis}
Recall that a \textit{linear combination} of vectors is in the form of
\begin{equation*}
    \sum a_i\mathbfit{v}_i = a_1\mathbfit{v}_1 + a_2\mathbfit{v}_2 + \cdots.
\end{equation*}
In case where no confusion is caused, this can be abbreviated as $a_i\mathbfit{v}_i$. Recall also that a \textit{span} of a set of vectors is defined as
\begin{equation*}
    \mathrm{span}(V) \coloneqq \left\{a_i\mathbfit{v}_i \colon \mathbfit{v}_i \in V\right\},
\end{equation*}
where $a_i$'s are scalars. A span of a subset of a vector space $V$ is clearly a subspace of $V$. We also know that a set of vectors $S$ is said to be \textit{linearly independent} if and only if  $a_i\mathbfit{v}_i = \zero$ implies that $a_i = 0$ for all $i = 1, 2, \cdots$. A \textit{basis} of a vector space $V$ is a linearly independent set $S$ such that $\mathrm{span}(S) = V$. One may check that if $S$ is a basis for $V$, then any $\mathbfit{v} \in V$ can be \textbf{uniquely} expressed as a linear combination of the members of $S$, but $S$ itself is not unique. In particular, the coefficients in this linear combination is known as the \textit{components} of $\mathbfit{v}$ relative to $S$.

We will see that the basis is closely related to the dimension of vector spaces. First, let us consider the trivial basis for $\mathcal{F}^n$.
\begin{dfnbox}{Canonical Basis}{canonicalBasis}
    The {\color{red} \textbf{canonical basis}} for $\mathcal{F}^n$ is defined as $\left\{\mathbfit{e}_i \colon i = 1, 2, \cdots, n\right\}$, where each $\mathbfit{e}_i$ is a column vector with $1_{\mathcal{F}}$ in its $i$-th row and $0_{\mathcal{F}}$ in the other rows.
\end{dfnbox}
It is easy to see that the number of vectors in any basis of a finite-dimensional vector space~$V$ is uniquely equal to its dimension.
\begin{probox}{Dimension as Cardinality of Basis}{dimIsCardBasis}
    Let $V$ be a finite-dimensional vector space with dimension $n$ and basis $S$, then $n = \abs{S}$.
    \tcblower
    \begin{proof}
        Note that $V \cong \mathcal{F}^n$. Let $\mathbfit{v} \in V$ be an arbitrary vector, then there is some~$\mathbfit{u} \in \mathcal{F}^n$ and a bijection $\phi \colon \mathcal{F}^n \to V$ such that 
        \begin{align*}
            \mathbfit{v} & = \phi(\mathbfit{u}) \\
            & = \phi\left(\sum_{i = 1}^{n}a_i\mathbfit{e}_i\right) \\
            & = \sum_{i = 1}^{n}a_i\phi(\mathbfit{e}_i),
        \end{align*}
        where $a_i \in \mathcal{F}$ for $i = 1, 2, \cdots, n$. This means that $V$ is spanned by at most $n$ vectors and so its basis is finite. Suppose on contrary that $\abs{S} = m < n$, then for any $\mathbfit{w} \in \mathcal{F}^n$, there is some $\mathbfit{r} \in V$ such that
        \begin{align*}
            \mathbfit{w} & = \phi^{-1}(\mathbfit{r}) \\
            & = \phi^{-1}\left(\sum_{i = 1}^{m}b_i\mathbfit{s}_i\right) \\
            & = \sum_{i = 1}^{m}b_i\phi^{-1}(\mathbfit{s}_i),
        \end{align*}
        where $b_i \in \mathcal{F}$ for $i = 1, 2, \cdots, m$. This means that $\mathcal{F}^n$ is spanned by $m$ vectors, which is a contradiction. Therefore, $\abs{S} = n$.
    \end{proof}
\end{probox}
An immediate corollary from Proposition \ref{pro:dimIsCardBasis} is that a finite-dimensional vector space always has a unique dimension, as otherwise it will have two bases with different cardinalities.

Note that since every vector in a vector space $V$ can be uniquely expressed as a linear combination of a basis $S$ for $V$, this really means that we can view the notion of a basis equivalently as a bijection between $\mathcal{F}^n$ and $V$, i.e., for any $(a_1, a_2, \cdots, a_n) \in \mathcal{F}^n$, we can map the tuple to a vector in $V$ whose components are exactly $a_1, a_2, \cdots, a_n$.

Now, let us denote a basis for $V$ by the mapping $z$. Notice that for any $\mathbfit{c}_1, \mathbfit{c}_2 \in \mathcal{F}^n$, we have 
\begin{equation*}
    z(\mathbfit{c}_1 + \mathbfit{c}_2) = z(\mathbfit{c}_1) + z(\mathbfit{c}_2),
\end{equation*}
so a basis is nothing more but an isomorphism!
\end{document}