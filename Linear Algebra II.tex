\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\I}{\mathbfit{I}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Linear Algebra II
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Vector Spaces}
\section{Fields, Scalars and Vectors}
In elementary mathematics, we often refer to a vector as an ordered tuple of numbers with a direction and a magnitude. However, there is a much more abstract aspect to the notion of vectors. In fact, let us first generalise the notion of \textit{scalars}, which are taken as complex constants in an elementary level. 

In general, we have the following algebraic structure:
\begin{dfnbox}{Field}{field}
    A {\color{red} \textbf{field}} is a set $\mathcal{F}$ with two binary operations $\mathcal{F}^2 \to \mathcal{F}$, namely addition and multiplication, such that
    \begin{enumerate}
        \item $u + v = v + u$ for all $u, v \in \mathcal{F}$;
        \item $(u + v) + w = u + (v + w)$ for all $u, v, w \in \mathcal{F}$;
        \item $uv = vu$ for all $u, v \in \mathcal{F}$;
        \item $(uv)w = u(vw)$ for all $u, v, w \in \mathcal{F}$;
        \item $u(v + w) = uv + uw$ for all $u, v, w \in \mathcal{F}$;
        \item there exists $0 \in \mathcal{F}$ such that $u + 0 = u$ for all $u \in \mathcal{F}$;
        \item there exists $1 \in \mathcal{F}$ such that $1u = u$ for all $u \in \mathcal{F}$;
        \item for every $u \in \mathcal{F}$, there exists some $v \in \mathcal{F}$ such that $u + v = 0$;
        \item for every $u \in \mathcal{F}$, there exists some $v \in \mathcal{F}$ such that $uv = 1$.
    \end{enumerate}
\end{dfnbox}
One may check that both $\R$ and $\C$ are fields. It turns out that we can also generalise the concept of vectors as any objects which possess properties similar to that of Euclidean vectors, i.e., we can view a vector as a mathematical quantity which can be added up and multiplied by another quantity called a scalar with some axioms which they follow. Rigorously, we define the notion of a \textit{vector space}.
\begin{dfnbox}{Vector Space}{vecSpace}
    A {\color{red} \textbf{vector space}} is a set $V$ over a field $\mathcal{F}$ with two binary operations, namely 
    \begin{itemize}
        \item addition $+ \colon V^2 \to V$, and
        \item scalar multiplication $(\quad)(\quad) \colon \mathcal{F} \times V \to V$,
    \end{itemize}
    such that
    \begin{enumerate}
        \item $\mathbfit{u + v = v + u}$ for all $\mathbfit{u}, \mathbfit{v} \in V$;
        \item $\mathbfit{(u + v) + w = u + (v + w)}$ for all $\mathbfit{u, v, w} \in V$;
        \item $ab\mathbfit{v} = a(b\mathbfit{v})$ for all $a, b \in \mathcal{F}$ and $\mathbfit{v} \in V$;
        \item there exists an {\color{red} \textbf{additive identity}} or {\color{red} \textbf{zero vector}} $\zero \in V$ such that $\mathbfit{v} + \zero = \mathbfit{v}$ for all $\mathbfit{v} \in V$;
        \item every $\mathbfit{v} \in V$ has an {\color{red} \textbf{additive inverse}} $\mathbfit{w} \in V$ with $\mathbfit{v + w} = 0$;
        \item there exists a {\color{red} \textbf{multiplicative identity}} $1 \in \mathcal{F}$ such that $1\mathbfit{v} = \mathbfit{v}$ for all $\mathbfit{v} \in V$;
        \item $a\mathbfit{(u + v)} = a\mathbfit{u} + a\mathbfit{v}$ and $(a + b)\mathbfit{u} = a\mathbfit{u} + b\mathbfit{u}$ for all $a, b \in \mathcal{F}$ and $\mathbfit{u, v} \in V$.
    \end{enumerate}
\end{dfnbox}
Notice that here, the definitions of addition in scalar multiplication in a vector space imply that any vector space must be \textbf{closed} under these two operations. Notice also that the operations ``addition'' and ``scalar multiplication'' are not necessary the addition and scalar multiplication which we are used to in $\R^n$, but abstract mappings which satisfy the given axioms.

We shall prove a few basic properties regarding vector spaces.
\begin{thmbox}{Uniqueness of Additive Identity}{unique0}
    Let $V$ be a vector space with $\zero \in V$ as an additive identity, then $\zero$ is unique.
    \tcblower
    \begin{proof}
        Suppose on contrary that there exists $\mathbfit{u} \in V$ such that $\mathbfit{v + u = v}$ for all $\mathbfit{v} \in V$. Since $\zero \in V$, we have
        \begin{equation*}
            \zero + \mathbfit{u} = \zero.
        \end{equation*}
        However, $\zero$ is the additive identity, so 
        \begin{equation*}
            \mathbfit{u} = \mathbfit{u} + \zero = \zero + \mathbfit{u} = \zero,
        \end{equation*}
        i.e. $\zero$ is unique.
    \end{proof}
\end{thmbox}
Similarly, we can also prove the uniqueness of additive inverse.
\begin{thmbox}{Uniqueness of Additive Inverse}{unique-1}
    Let $V$ be a vector space, then every $\mathbfit{v} \in V$ has a unique additive inverse.
    \tcblower
    \begin{proof}
        Suppose on contrary that there exist $\mathbfit{u, w} \in V$ both being additive inverse of~$\mathbfit{v}$, then $\mathbfit{u + v} = \zero$ and $\mathbfit{w + v} = \zero$. Therefore,
        \begin{equation*}
            \mathbfit{u} = \mathbfit{(u + v) + u} = \mathbfit{(w + v) + u} = \mathbfit{w + (u + v)} = \mathbfit{w},
        \end{equation*}
        i.e., $\mathbfit{v}$ has a unique additive inverse.
    \end{proof}
\end{thmbox}
Theorem \ref{thm:unique-1} justifies the notation $-\mathbfit{u}$ to denote the additive inverse of $\mathbfit{u}$. However, so far we have not ascertained the fact that $-\mathbfit{u} = (-1)\mathbfit{u}$ (note that the former means the inverse of $\mathbfit{u}$ while the latter means $\mathbfit{u}$ multiplied by the scalar $-1$)! While seemingly innocent, this result is not as easily proven as it looks.

First, we shall justify that $0\mathbfit{u} = \zero$ for all $\mathbfit{u} \in V$. Notice that
\begin{equation*}
    0\mathbfit{u} = (0 + 0)\mathbfit{u} = 0\mathbfit{u} + 0\mathbfit{u}.
\end{equation*}
Adding $-(0\mathbfit{u})$ to both sides of the equation yields $0\mathbfit{u} = \zero$ as desired. From this result we see that
\begin{equation*}
    (-1)\mathbfit{u} + \mathbfit{u} = (-1 + 1)\mathbfit{u} = 0\mathbfit{u} = \zero.
\end{equation*}
By uniqueness of additive inverse, we must have $(-1)\mathbfit{u} = -\mathbfit{u}$.

Note that by using a similar technique we can prove that $a\zero = \zero$ for all $a \in \mathcal{F}$, and so~$\zero = -\zero$ as a consequence.

Additionally, note that subtraction is defined as $\mathbf{u - v} = \mathbfit{u} + (-1)\mathbfit{v}$, so the above result allows us to write $\mathbfit{u - v} = \mathbfit{u} + (-\mathbfit{v})$.

\subsection{Subspaces}
Note that a vector space is extended based on a set of vectors, so we can define \textit{subspaces} similarly to the notion of subsets.
\begin{dfnbox}{Subspace}{subspace}
    Let $V$ be a vector space. $U \subseteq V$ is called a {\color{red} \textbf{subspace}} if $U$ is a vector space under addition and scalar multiplication in $V$.
\end{dfnbox}
It is easy to see that the intersection of any number of subspaces of a vector space $V$ is still a subspace of $V$, but the union might not be so. In particular, we would like to consider a special construct known as \textit{direct sum}.
\begin{dfnbox}{Direct Sum}{directSum}
    Let $V$ be a vector space and $U_1, U_2 \subseteq V$ such that $U_1 \cap U_2 = \{\zero\}$, then their {\color{red} \textbf{direct sum}} is defined as
    \begin{equation*}
        U_1 \oplus U_2 \coloneqq \left\{\mathbfit{u}_1 + \mathbfit{u}_2 \colon \mathbfit{u}_1 \in U_1, \mathbfit{u}_2 \in U_2\right\}.
    \end{equation*}
\end{dfnbox}
More generally, we can let $U_1$ and $U_2$ be any subsets of $V$ and define $U_1 + U_2$ in the same manner, which is known as the \textit{sum} of $U_1$ and $U_2$.

It can be easily proven that for any vector space $V$, the direct sum of any two subspaces of $V$ is still a subspace of $V$. A nice property of direct sum can be proven as follows:
\begin{probox}{Unique Decomposition with Direct Sums}{uniqueDecomp}
    Let $V = U_1 \oplus U_2$, then every $\mathbfit{v} \in V$ can be uniquely expressed as $\mathbfit{u + w}$ for some $\mathbfit{u} \in U_1$ and $\mathbfit{w} \in U_2$.
    \tcblower
    \begin{proof}
        The existence of $\mathbfit{u}$ and $\mathbfit{w}$ is trivial by Definition \ref{dfn:directSum}. Suppose there exist $\mathbfit{u}' \in U_1$ and $\mathbfit{w}' \in U_2$ such that $\mathbfit{u + w} = \mathbfit{u}' + \mathbfit{w}'$, then we have $\mathbfit{u - u}' = \mathbfit{w}' - \mathbfit{w}$. Note that $\mathbfit{u - u}' \in U_1$ and $\mathbfit{w}' - \mathbfit{w} \in U_2$, so we have $\mathbfit{u - u}', \mathbfit{w}' - \mathbfit{w} \in U_1 \cap U_2 = \{\zero\}$, i.e.,
        \begin{equation*}
            \mathbfit{u - u}' = \mathbfit{w}' - \mathbfit{w} = \zero.
        \end{equation*} 
        Therefore, $\mathbfit{u} = \mathbfit{u}'$ and $\mathbfit{w} = \mathbfit{w}'$, i.e., $\mathbfit{u}$ and $\mathbfit{w}$ are unique.
    \end{proof}
\end{probox}
In some sense, a direct sum of $V$ can be viewed as a ``partition'' of $V$ into two subsets with a minimal overlap. Note that unlike partition in its real definition, the subspaces $U_1$ and $U_2$ here cannot be disjoint sets as both of them have to contain the zero vector in $V$. More generally, for any subspace $U \subseteq V$, we have $\zero_U = \zero_V$, the proof of which should be trivial enough as an exercise to the reader.

In particular, we would like to consider $\mathcal{F}^n$ for a general field $\mathcal{F}$. We can define the dot product operation over $\mathcal{F}^n$ in the same way as $\R^n$. Take any subspace $U \subseteq \mathcal{F}^n$ and define the set
\begin{equation*}
    U_{\perp} \coloneqq \left\{\mathbfit{u} \in \mathcal{F}^n \colon \mathbfit{u \cdot v} = 0 \quad\textrm{for all } \mathbfit{v} \in U\right\},
\end{equation*}
then $\mathcal{F}^n = U \oplus U_{\perp}$.

To justify this, we first take any $\mathbfit{v} \in \mathcal{F}^n$. Using some calculus, we can show that there exists 
\begin{equation*}
    \mathbfit{u}_0 = \argmin_{\mathbfit{u} \in U}\abs{\mathbfit{u \cdot v}}.
\end{equation*}
Let $\mathbfit{w = v - u}_0$, then clearly $\mathbfit{v = w + u}_0$ where $\mathbfit{u}_0 \in U$ and $\mathbfit{w} \in U_{\perp}$. This implies that~$V = U + U_{\perp}$. Note that $\zero$ is the only vector in $\mathcal{F}^n$ which is orthogonal to itself, so we have $U \cap U_{\perp} = \{\zero\}$. It follows that $V = U \oplus U_{\perp}$.

\section{Isomorphism}
Since the underlying structure of a vector space is still a set, the notion of a mapping between two vector spaces is well-defined. However, note that a vector space possesses unique algebraic structures and properties, namely that the linear combinations of any members of the space are still in the space, so we would like to focus on mapping which preserves such properties.
\begin{dfnbox}{Homomorphism}{homomorphic}
    Let $U$ and $V$ be vector spaces, a {\color{red} \textbf{homomorphism}} is a mapping $\phi \colon U \to V$ such that
    \begin{equation*}
        \phi(\mathbfit{u + v}) = \phi(\mathbfit{u}) + \phi(\mathbfit{v})
    \end{equation*}
    for any $\mathbfit{u}, \mathbfit{v} \in U$.
\end{dfnbox}
Note that it suffices to only require $\phi(\mathbfit{u + v}) = \phi(\mathbfit{u}) + \phi(\mathbfit{v})$ but not $\phi(c\mathbfit{u}) = c\phi(\mathbfit{u})$. Here, note that a vector space is a connected set, so we can easily prove the above claim with some knowledge in mathematical analysis.

Naturally, if a homomorphism is bijective, then it means that the elements in two vector spaces have a one-to-one correspondence. In practice, this means we can treat them as equivalent spaces in some sense.
\begin{dfnbox}{Isomorphism}{isomorphic}
    An {\color{red} \textbf{isomorphism}} between vector spaces $U$ and $V$ is a homomorphism between them which is bijective.
\end{dfnbox}
An interesting fact here is that an isomorphism between any vector spaces is not unique. To see this, let us first consider an arbitrary vector space $V$. Now, we can always find the trivial isomorphism $\mathrm{id}_V \colon V \to V$. In fact, any mapping $\phi \colon \mathbfit{v} \mapsto c\mathbfit{v}$ where $c$ is a scalar is clearly an isomorphism from $V$ to $V$. This means that there are infinitely many isomorphisms from~$V$ to itself.

Let $U$ be an arbitrary vector space such that there exists some isomorphism $\psi \colon V \to U$. We consider the following theorem:
\begin{thmbox}{Composition Preserves Isomorphism}{compoPreserveIsomorphic}
    Let $U, V, W$ be vector spaces. If $\phi \colon U \to V$ and $\psi \colon V \to W$ are isomorphisms, then the composite mapping $\phi \circ \psi \colon U \to W$ is an isomorphism.
    \tcblower
    \begin{proof}
        Since both $\phi$ and $\psi$ are bijective, it is clear that $\psi \circ \phi$ is bijective. Take any $\mathbfit{u}, \mathbfit{v} \in U$, then
        \begin{align*}
            \psi\bigl(\phi(\mathbfit{u + v})\bigr) = \psi\bigl(\phi(\mathbfit{u}) + \phi(\mathbfit{v})\bigr) = \psi\bigl(\phi(\mathbfit{u})\bigr) + \psi\bigl(\phi(\mathbfit{v})\bigr)
        \end{align*}
        since $\phi(\mathbfit{u}), \phi(\mathbfit{v}) \in V$. Therefore, $\psi \circ \phi$ is an isomorphism.
    \end{proof}
\end{thmbox}
Using Theorem \ref{thm:compoPreserveIsomorphic}, we can immediately see that if $\phi \colon V \to V$ is any isomorphism and $\psi \colon V \to U$ is an isomorphism, then $\psi \circ \phi$ is an isomorphism between $V$ and $U$. Therefore, there are infinitely many isomorphisms between $V$ and $U$.
\begin{notebox}
    \begin{remark}
        If $V$ is isomorphic to $U$, we write $V \cong U$. Clearly, $\cong$ is an equivalence relation.
    \end{remark}
\end{notebox}

Now, observe that for any field $\mathcal{F}$, the set $\mathcal{F}^n$ for any $n \in \N$ is a vector space over $\mathcal{F}$.
\begin{dfnbox}{Finite-Dimensional Vector Space}{finiteDim}
    A vector space $V$ is said to be {\color{red} \textbf{finite-dimensional}} over a field $\mathcal{F}$ if it it is isomorphic to $\mathcal{F}^n$ for some $n \in \N$. $n$ is called the {\color{red} \textbf{dimension}} of $V$.
\end{dfnbox}
Obviously, a vector space which is not finite-dimensional is called \textit{infinite-dimensional}. For example, the set of all polynomials is a vector space of infinite dimension.

\section{Basis}
Recall that a \textit{linear combination} of vectors is in the form of
\begin{equation*}
    \sum a_i\mathbfit{v}_i = a_1\mathbfit{v}_1 + a_2\mathbfit{v}_2 + \cdots.
\end{equation*}
In case where no confusion is caused, this can be abbreviated as $a_i\mathbfit{v}_i$. Recall also that a \textit{span} of a set of vectors is defined as
\begin{equation*}
    \mathrm{span}(V) \coloneqq \left\{a_i\mathbfit{v}_i \colon \mathbfit{v}_i \in V\right\},
\end{equation*}
where $a_i$'s are scalars. A span of a subset of a vector space $V$ is clearly a subspace of $V$. We also know that a set of vectors $S$ is said to be \textit{linearly independent} if and only if  $a_i\mathbfit{v}_i = \zero$ implies that $a_i = 0$ for all $i = 1, 2, \cdots$. A \textit{basis} of a vector space $V$ is a linearly independent set $S$ such that $\mathrm{span}(S) = V$. One may check that if $S$ is a basis for $V$, then any $\mathbfit{v} \in V$ can be \textbf{uniquely} expressed as a linear combination of the members of $S$, but $S$ itself is not unique. In particular, the coefficients in this linear combination is known as the \textit{components} of $\mathbfit{v}$ relative to $S$.

We will see that the basis is closely related to the dimension of vector spaces. First, let us consider the trivial basis for $\mathcal{F}^n$.
\begin{dfnbox}{Canonical Basis}{canonicalBasis}
    The {\color{red} \textbf{canonical basis}} for $\mathcal{F}^n$ is defined as $\left\{\mathbfit{e}_i \colon i = 1, 2, \cdots, n\right\}$, where each $\mathbfit{e}_i$ is a column vector with $1_{\mathcal{F}}$ in its $i$-th row and $0_{\mathcal{F}}$ in the other rows.
\end{dfnbox}
It is easy to see that the number of vectors in any basis of a finite-dimensional vector space~$V$ is uniquely equal to its dimension.
\begin{probox}{Dimension as Cardinality of Basis}{dimIsCardBasis}
    Let $V$ be a finite-dimensional vector space with dimension $n$ and basis $S$, then $n = \abs{S}$.
    \tcblower
    \begin{proof}
        Note that $V \cong \mathcal{F}^n$. Let $\mathbfit{v} \in V$ be an arbitrary vector, then there is some~$\mathbfit{u} \in \mathcal{F}^n$ and a bijection $\phi \colon \mathcal{F}^n \to V$ such that 
        \begin{align*}
            \mathbfit{v} & = \phi(\mathbfit{u}) \\
            & = \phi\left(\sum_{i = 1}^{n}a_i\mathbfit{e}_i\right) \\
            & = \sum_{i = 1}^{n}a_i\phi(\mathbfit{e}_i),
        \end{align*}
        where $a_i \in \mathcal{F}$ for $i = 1, 2, \cdots, n$. This means that $V$ is spanned by at most $n$ vectors and so its basis is finite. Suppose on contrary that $\abs{S} = m < n$, then for any $\mathbfit{w} \in \mathcal{F}^n$, there is some $\mathbfit{r} \in V$ such that
        \begin{align*}
            \mathbfit{w} & = \phi^{-1}(\mathbfit{r}) \\
            & = \phi^{-1}\left(\sum_{i = 1}^{m}b_i\mathbfit{s}_i\right) \\
            & = \sum_{i = 1}^{m}b_i\phi^{-1}(\mathbfit{s}_i),
        \end{align*}
        where $b_i \in \mathcal{F}$ for $i = 1, 2, \cdots, m$. This means that $\mathcal{F}^n$ is spanned by $m$ vectors, which is a contradiction. Therefore, $\abs{S} = n$.
    \end{proof}
\end{probox}
An immediate corollary from Proposition \ref{pro:dimIsCardBasis} is that a finite-dimensional vector space always has a unique dimension, as otherwise it will have two bases with different cardinalities.

Note that since every vector in a vector space $V$ can be uniquely expressed as a linear combination of a basis $S$ for $V$, this really means that we can view the notion of a basis equivalently as a bijection between $\mathcal{F}^n$ and $V$, i.e., for any $(a_1, a_2, \cdots, a_n) \in \mathcal{F}^n$, we can map the tuple to a vector in $V$ whose components are exactly $a_1, a_2, \cdots, a_n$.

Now, let us denote a basis for $V$ by the mapping $z$. Notice that for any $\mathbfit{c}_1, \mathbfit{c}_2 \in \mathcal{F}^n$, we have 
\begin{equation*}
    z(\mathbfit{c}_1 + \mathbfit{c}_2) = z(\mathbfit{c}_1) + z(\mathbfit{c}_2),
\end{equation*}
so a basis is nothing more but an isomorphism!

\chapter{Linear Transformations}
\section{Linear Transformations}
\begin{dfnbox}{Linear Transformation}{linearTrans}
    A {\color{red} \textbf{linear transformation}} is a mapping $T \colon V \to W$, where $V$ and $W$ are vector spaces over $\mathcal{F}$, such that
    \begin{equation*}
        T(\mathbfit{v + u}) + T(\mathbfit{v}) + T(\mathbfit{u}), \qquad T(c\mathbfit{v}) = cT(\mathbfit{v})
    \end{equation*}
    for all $\mathbfit{v}, \mathbfit{u} \in V$ and all $c \in \mathcal{F}$.
\end{dfnbox}
In essence, a linear transformation is a function between vector spaces which preserves the vector structure of its domain. We will see that many notions discussed so far can actually be abstracted into a linear transformation.

Let $V \cong \mathcal{F}^n$ be a finite-dimensional vector space. Recall that a basis for $V$ is essentially a bijective mapping $z \colon \mathcal{F}^n \to V$, so a basis is a linear transformation. In particular, if $y$ is another basis for $V$, then clearly $y \circ z^{-1}$ is a mapping from $V$ to itself. Let $Q = y \circ z^{-1}$, then we have $y = Q \circ z$. This is known as a change of basis.

Now, consider a vector space $V$ over $\mathcal{F}$. Fix some $\mathbfit{v} \in V$ and define a mapping $\Theta_{\mathbfit{v}} \colon \mathcal{F} \to V$ by $\Theta_{\mathbfit{v}}(a) = a\mathbfit{v}$. One may check that $\Theta_{\mathbfit{v}}$ is a linear transformation, but $\Theta_{\mathbfit{v}}$ is essentially $\mathbfit{v}$, so vectors are linear transformations as well.
\begin{dfnbox}{Range}{range}
    Let $T \colon V \to W$ be a linear transformation. The {\color{red} \textbf{range}} of $T$ is defined as
    \begin{equation*}
        \mathrm{range}(T) \coloneqq \left\{T(\mathbfit{v}) \colon \mathbfit{v} \in V\right\}.
    \end{equation*}
    The dimension of $\mathrm{range}(T)$ is called the {\color{red} \textbf{rank}} of $T$.
\end{dfnbox}
It is immediate from the definition that $T$ is surjective if and only if $\mathrm{range}(T) = W$. Consider $T(\mathbfit{v}), T(\mathbfit{u}) \in \mathrm{range}(T)$ for some $\mathbfit{v} \neq \mathbfit{u}$ in $V$, then we have
\begin{equation*}
    \alpha T(\mathbfit{v}) + T(\mathbfit{u}) = T(\alpha\mathbfit{v} + \mathbfit{u}).
\end{equation*}
Clearly, $\alpha\mathbfit{v} + \mathbfit{u} \in V$, so $\alpha T(\mathbfit{v}) + T(\mathbfit{u}) \in \mathrm{range}(T)$ and $\mathrm{range}(T) \subseteq W$ is a vector space.
\begin{dfnbox}{Kernel}{ker}
    Let $T \colon V \to W$ be a linear transformation. The {\color{red} \textbf{kernel}} of $T$ is defined as 
    \begin{equation*}
        \mathrm{ker}(T) \coloneqq \left\{\mathbfit{v} \in V \colon T(\mathbfit{v}) = \zero_{W}\right\}.
    \end{equation*}
    The dimension of $\ker(T)$ is called the {\color{red} \textbf{nullity}} of $T$.
\end{dfnbox}
Obviously, $\ker(T)$ is a vector space. We claim that $\mathrm{null}(T)$ is related to the injectivity of $T$ by the following result:
\begin{probox}{Injectivity Test}{injectTest}
    A linear transformation $T$ is injective if and only if $\mathrm{null}(T) = 0$.
    \tcblower
    \begin{proof}
        Suppose that $T$ is injective. We shall prove that $\mathrm{null}(T) = 0$ by considering the contrapositive. Suppose that $\mathrm{null}(T) \neq 0$, then there is some non-zero $\mathbfit{v} \in \ker(T)$ with $T(\mathbfit{v}) = \zero$, so $T$ is not injective.
        \\\\
        Suppose conversely that $\mathrm{null}(T) = 0$, then $\ker(T) = \{\zero\}$. Let $T(\mathbfit{v}) = T(\mathbfit{u})$, then
        \begin{equation*}
            \zero = T(\mathbfit{v}) - T(\mathbfit{u}) = T(\mathbfit{v - u}),
        \end{equation*}
        so $\mathbfit{v - u} \in \ker(T)$. This means that $\mathbfit{v - u} = \zero$ so $\mathbfit{v = u}$. Therefore, $T$ is injective.
    \end{proof}
\end{probox}
For any linear transformation $T \colon V \to W$, we have 
\begin{equation*}
    T(\zero_V) = T(\zero_V + \zero_V) = 2T(\zero_V).
\end{equation*}
Cancelling $T(\zero_V)$ on both sides we have $\zero_W = T(\zero_V)$. Therefore, $\ker(T) - \{\zero_V\}$ consists of all non-zero vectors which are mapped to zero under $T$, i.e., for all $\mathbfit{u} \in U \coloneqq V - (\ker(T) - \{\zero_V\})$, $T(\mathbfit{u}) \neq \zero_W$. 
\begin{thmbox}{Fundamental Theorem of Linear Transformations}{rankNull}
    Let $T \colon V \to W$ be a linear transformation, then $\dim(V) = \mathrm{rank}(T) + \mathrm{null}(T)$.
    \tcblower
    \begin{proof}
        Define $U \coloneqq V - (\ker(T) - \{\zero_V\})$, then $V = U \oplus \ker(T)$. Let $S \colon U \to \mathrm{range}(T)$ be defined by $S(\mathbfit{u}) = T(\mathbfit{u})$. Note that for all $\mathbfit{v} \in \mathrm{range}(T)$, there exists some $\mathbfit{u} \in U$ such that $S(\mathbfit{u}) = \mathbfit{v}$, so $S$ is surjective. Suppose there exist vectors $\mathbfit{u}_1, \mathbfit{u}_2 \in U$ such that $S(\mathbfit{u}_1) = S(\mathbfit{u}_2)$, then
        \begin{equation*}
            \zero_W = S(\mathbfit{u}_1) - S(\mathbfit{u}_2) = S(\mathbfit{u}_1 - \mathbfit{u}_2).
        \end{equation*}
        Therefore, $\mathbfit{u}_1 - \mathbfit{u}_2 = \zero_U$, and so $\mathbfit{u}_1 = \mathbfit{u}_2$, which implies that $S$ is injective. Therefore,~$S$ is a bijection and so $U \cong \mathrm{range}(T)$, which means $\dim(U) = \mathrm{rank}(T)$. Therefore,
        \begin{equation*}
            \dim(V) = \dim(U) + \mathrm{null}(T) = \mathrm{rank}(T) + \mathrm{null}(T).
        \end{equation*}
    \end{proof}
\end{thmbox}
An interesting application of Theorem \ref{thm:rankNull} gives the following corollary:
\begin{corbox}{Bijectivity of Reflexive Mappings}{bijectiveReflexiveMap}
    Let $T \colon V \to V$ be a linear transformation, then $T$ is injective if and only if it is surjective.
    \tcblower
    \begin{proof}
        Suppose that $T$ is injective, then $V \cong \mathrm{range}(T)$ and so $\mathrm{null}(T) = 0$, which implies that $\dim(V) = \mathrm{rank}(T)$. However, $\mathrm{range}(T) \subseteq V$, so $\mathrm{range}(T) = V$. This means that $T$ is surjective.
        \\\\
        Suppose that $T$ is surjective, then $\dim(V) = \mathrm{rank}(T)$ and so $\mathrm{null}(T) = 0$. Therefore, $T$ is injective.
    \end{proof}
\end{corbox}
\section{Duality}
Let $V$ and $W$ be vector spaces and define $\mathcal{L}(V, W)$ to be the set of all linear transformations from $V$ to $W$. One may check that $\mathcal{L}(V, W)$ is a vector space.
\begin{dfnbox}{Dual Space}{dualSpace}
    Let $V$ be a vector space over $\mathcal{F}$. The {\color{red} \textbf{dual space}} of $V$ is defined as $\hat{V} \coloneqq \mathcal{L}(V, \mathcal{F})$. The elements of $\hat{V}$ are called {\color{red} \textbf{dual vectors}}.
\end{dfnbox}
An intuitive example for an element of $\hat{V}$ is as follows: let $\left\{\mathbfit{z}_1, \mathbfit{z}_2, \cdots, \mathbfit{z}_n\right\}$ be a basis for $V$. For any $\mathbfit{v} \in V$, we can write
\begin{equation*}
    \mathbfit{v} = \sum_{i = 1}^{n}a_i\mathbfit{z}_i
\end{equation*}
for $a_1, a_2, \cdots, a_n \in \mathcal{F}$. Now, define a mapping $\zeta^i \colon V \to \mathcal{F}$ as $\zeta^i(\mathbfit{v}) = a_i$, then clearly $\zeta^i \in \hat{V}$ for $i = 1, 2, \cdots, n$.

In particular, we see that
\begin{equation*}
    \zeta^i(\mathbfit{z}_j) = \begin{cases}
        1 & \textrm{if } i = j \\
        0 & \textrm{otherwise}
    \end{cases}.
\end{equation*}
Intuitively, this means that for each $\mathbfit{v} \in V$, we have $z^{-1}(\mathbfit{v}) = \sum_{i = 1}^{n}a_i\zeta^i(\mathbfit{z}_i)$.
\begin{dfnbox}{Dual Basis}{dualBasis}
    Let $V$ be a vector space with a basis $\left\{\mathbfit{z}_1, \mathbfit{z}_2, \cdots, \mathbfit{z}_n\right\}$. The {\color{red} \textbf{dual basis}} of $\left\{\mathbfit{z}_1, \mathbfit{z}_2, \cdots, \mathbfit{z}_n\right\}$ is defined as the set of all $\zeta^i \in \hat{V}$ such that 
    \begin{equation*}
        \zeta^i(\mathbfit{z}_j) = \begin{cases}
            1 & \textrm{if } i = j \\
            0 & \textrm{otherwise}
        \end{cases}.
    \end{equation*}
\end{dfnbox}
Let $\zeta$ be a dual basis and $\alpha \in \hat{V}$ be a linear mapping over a finite-dimensional vector space $V$ with $\dim(V) = n$. For any $\mathbfit{v} \in V$ with respect to basis $z$, define a mapping $\beta \colon V \to \mathcal{F}$ by 
\begin{equation*}
    \beta(\mathbfit{v}) = \left(\sum_{i = 1}^{n}\alpha(\mathbfit{z}_i)\zeta^i\right)(\mathbfit{v}).
\end{equation*}
Clearly, $\beta$ is a linear combination of the elements of $\zeta$. For any $\mathbfit{z}_j \in z$, we have
\begin{align*}
    \beta(\mathbfit{z}_j) & = \left(\sum_{i = 1}^{n}\alpha(\mathbfit{z}_i)\zeta^i\right)(\mathbfit{z}_j) \\
    & = \sum_{i = 1}^{n}\alpha(\mathbfit{z}_i)\zeta^i(\mathbfit{z}_j) \\
    & = \alpha(\mathbfit{z}_j).
\end{align*}
Since $z$ is a basis for $V$, this implies that for any $\mathbfit{v} \in V$, we have $\beta(\mathbfit{v}) = \alpha(\mathbfit{v})$. Therefore, $\alpha \in \mathrm{span}(\zeta)$. 

Consider
\begin{equation*}
    \sum_{i = 1}^{n}p_i\zeta^i = 0_{\hat{V}},
\end{equation*}
which is the zero mapping from $V$ to $\{0\}$, this means that for any $\mathbfit{v} \in V$, we have
\begin{align*}
    0 & = \left(\sum_{i = 1}^{n}p_i\zeta^i\right)(\mathbfit{v}) \\
    & = \left(\sum_{i = 1}^{n}p_i\zeta^i\right)\left(\sum_{j = 1}^{n}a_j\mathbfit{z}_j\right) \\
    & = \sum_{i = 1}^{n}\left(p_i\sum_{j = 1}^{n}a_j\zeta^i(\mathbfit{z}_j)\right) \\
    & = \sum_{i = 1}^{n}p_ia_i.
\end{align*}
However, since the $a_i$'s are arbitrary, we have $p_i = 0$ for all $i = 1, 2, \cdots, n$. Therefore, $\zeta$ is linearly independent. This means that $\zeta$ is indeed a basis. Clearly, this also implies that any finite-dimensional vector space has the same dimension as its dual space. 
\begin{probox}{$\dim(V) = \dim\left(\hat{V}\right)$}{dualHasEqualDim}
    Let $V$ be any finite-dimensional vector space, then $\dim(V) = \dim\left(\hat{V}\right)$.
\end{probox}
Furthermore, note that for any dual vector $\phi = \sum_{i = 1}^{n}p_i\zeta^i$, we have
\begin{equation*}
    \phi(\mathbfit{z}_j) = \sum_{i = 1}^{n}p_i\zeta^i(\mathbfit{z}_j) = p_j.
\end{equation*}
Therefore, any dual vector with respect to basis $z$ can be written as
\begin{equation*}
    \phi = \sum_{i = 1}^{n}\phi(\mathbfit{z}_i)\zeta^i.
\end{equation*}
Recall that in Definition \ref{dfn:canonicalBasis}, we defined the canonical basis of $\mathcal{F}^n$ to be the set of unit vectors with a single non-zero entry. Similarly, we can define the canonical basis for the dual space $\hat{\mathcal{F}^n} \coloneqq \mathcal{L}(\mathcal{F}^n, \mathcal{F})$.
\begin{dfnbox}{Canonical Dual Basis}{canonicalDualBasis}
    Write each $\mathbfit{v} \in \mathcal{F}^n$ as
    \begin{equation*}
        \mathbfit{v} = \begin{bmatrix}
            v_1 \\
            v_2 \\
            \vdots \\
            v_n
        \end{bmatrix}.
    \end{equation*}
    The {\color{red} \textbf{canonical dual basis}} of $\mathcal{F}^n$ is defined as $\left\{\epsilon^i \colon i = 1, 2, \cdots, n\right\}$ such that $\epsilon^i(\mathbfit{v}) = v_i$ for all $\mathbfit{v} \in \mathcal{F}^n$.
\end{dfnbox}
Consider any $p \in \hat{\mathcal{F}^n}$, then we can write
\begin{equation*}
    p = \sum_{i = 1}^{n}q_i\epsilon^i
\end{equation*} 
where $q_i \in \mathcal{F}$ for $i = 1, 2, \cdots, n$ and $\epsilon^i$'s are the dual canonical basis. Recall that a basis of an $n$-dimensional vector space $V$ is actually a mapping from $\mathcal{F}^n$ to $V$, so it follows that the dual basis is in fact a mapping $\zeta \colon \hat{\mathcal{F}^n} \to \hat{V}$. We claim that this $\zeta$ is in fact just a mapping that satisfies 
\begin{equation*}
    \zeta(p)\bigl(z(\mathbfit{a})\bigr) = p(\mathbfit{a})
\end{equation*}
for any $p \in \hat{\mathcal{F}^n}$ and $\mathbfit{a} \in \mathcal{F}^n$. 

To prove that our new definition is consistent with Definition \ref{dfn:dualBasis}, it suffices to show that $\zeta(\epsilon^i) = \zeta^i$. Notice that
\begin{align*}
    \zeta(\epsilon^i)\bigl(z(\mathbfit{e}_j)\bigr) & = \epsilon^i(\mathbfit{e}_j) \\
    & = \begin{cases}
        1 & \textrm{if } i = j \\
        0 & \textrm{otherwise}
    \end{cases}.
\end{align*}
However, $z(\mathbfit{e}_j) = \mathbfit{z}_j$, so by Definition \ref{dfn:dualBasis} we have $\zeta(\epsilon^i)(\mathbfit{z}_j) = \zeta^i(\mathbfit{z}_j)$. Let $\mathbfit{v} \in V$ be any vector with
\begin{equation*}
    \mathbfit{v} = \sum_{i = 1}^{n}a_i\mathbfit{z}_i,
\end{equation*}
then 
\begin{align*}
    \zeta(\epsilon^i)(\mathbfit{v}) & = \zeta(\epsilon^i)\left(\sum_{j = 1}^{n}a_j\mathbfit{z}_j\right) \\
    & = \sum_{j = 1}^{n}a_j\zeta(\epsilon^i)(\mathbfit{z}_j) \\
    & = \sum_{j = 1}^{n}a_j\zeta^i(\mathbfit{z}_j) \\
    & = a_i \\
    & = \zeta^i(\mathbfit{v}).
\end{align*}
Therefore, $\zeta(\epsilon^i) = \zeta^i$. Let $\phi \in \hat{V}$ be a mapping, then
\begin{align*}
    \phi & = \sum_{i = 1}^{n}a_i\zeta^i \\
    & = \sum_{i = 1}^{n}a_i\zeta(\epsilon^i) \\
    & = \zeta\left(\sum_{i = 1}^{n}a_i\epsilon^i\right) \\
    & = \zeta(p)
\end{align*}
for some $p \in \hat{\mathcal{F}^n}$. Therefore, $\zeta$ is surjective. One may check that $\zeta$ is injective. Moreover, note that for $p, q \in \hat{\mathcal{F}^n}$,
\begin{align*}
    \bigl(m\zeta(p) + n\zeta(q)\bigr)\bigl(z(\mathbfit{a})\bigr) & = m\zeta(p)\bigl(z(\mathbfit{a})\bigr) + n\zeta(q)\bigl(z(\mathbfit{a})\bigr) \\
    & = mp(\mathbfit{a}) + nq(\mathbfit{a}) \\
    & = (mp + nq)(\mathbfit{a}) \\
    & = \zeta(mp + nq)\bigl(z(\mathbfit{a})\bigr).
\end{align*}
Therefore, $\zeta$ is a linear mapping and so it is an isomorphism. Indeed, this implies that $\zeta$ is a basis for $\hat{V}$.

Naturally, it feels justified to define a ``dual space of the dual space'' of $V$, namely 
\begin{equation*}
    \hat{\hat{V}} \coloneqq \mathcal{L}\left(\hat{V}, \mathcal{F}\right) = \mathcal{L}\bigl(\mathcal{L}(V, \mathcal{F}), \mathcal{F}\bigr).
\end{equation*}
We can in fact prove that this space is isomorphic to $V$ itself. Therefore, this means that it is perfectly legal to view each $\mathbfit{v} \in V$ as a mapping from $\hat{V}$ to $\mathcal{F}$. We can verify that $\mathbfit{v}$ is linear, because
\begin{align*}
    \mathbfit{v}(m\alpha + n\beta) & = m\alpha(\mathbfit{v}) + n\beta(\mathbfit{v}) \\
    & = m\mathbfit{v}(\alpha) + n\mathbfit{v}(\beta).
\end{align*}
Therefore, we can in some sense view a vector space and its dual space the \textit{dual} of each other. Consider a mapping $T \colon V \to V$ for some vector space $V$, the duality between $V$ and~$\hat{V}$ tempts us to think that there exists some mapping $S \colon \hat{V} \colon \hat{V}$ with a certain correspondence to $T$.
\begin{dfnbox}{Transpose}{tranpose}
    Let $T \colon V \to V$ be a linear transformation. The {\color{red} \textbf{transpose}} of $T$ is defined as the mapping~$\hat{T} \colon \hat{V} \to \hat{V}$ such that 
    \begin{equation*}
        \hat{T}(\alpha)(\mathbfit{v}) = \alpha\bigl(T(\mathbfit{v})\bigr)
    \end{equation*}
    for any $\alpha \in \hat{V}$. 
\end{dfnbox}
\section{Tensor Product}
\begin{dfnbox}{Tensor Product}{tensorProd}
    Let $V$ be a vector space with the dual space $\hat{V}$. For any $\mathbfit{v} \in V$ and $\alpha \in \hat{V}$, their {\color{red} \textbf{tensor product}} is defined as a mapping $\mathbfit{v} \otimes \alpha \in \mathcal{L}(V, V)$ such that
    \begin{equation*}
        (\mathbfit{v} \otimes \alpha)(\mathbfit{u}) = \alpha(\mathbfit{u})\mathbfit{v}.
    \end{equation*}
\end{dfnbox}
We can see that $\mathbfit{v} \otimes \alpha$ is linear with respect to both $\mathbfit{v}$ and $\alpha$, because
\begin{align*}
    \bigl((\mathbfit{u + v}) \otimes \alpha\bigr)(\mathbfit{w}) & = \alpha(\mathbfit{w})(\mathbfit{u + v}) = \alpha(\mathbfit{w})\mathbfit{u} + \alpha(\mathbfit{w})\mathbfit{v} = (\mathbfit{u} \otimes \alpha + \mathbfit{v} \otimes \alpha)(\mathbfit{v}), \\
    \bigl(\mathbfit{v} \otimes (\alpha + \beta)\bigr)(\mathbfit{w}) & = (\alpha + \beta)(\mathbfit{w})\mathbfit{v} = \alpha(\mathbfit{w})\mathbfit{v} + \beta(\mathbfit{w})\mathbfit{v} = (\mathbfit{v} \otimes \alpha + \mathbfit{v} \otimes \beta)(\mathbfit{w}).
\end{align*}
In particular, let $\beta \in \hat{V}$, then clearly $\beta \circ (\mathbfit{v} \otimes \alpha) \in \hat{V}$. Define a mapping $T$ over $\hat{V}$ by
\begin{equation*}
    T(\beta) = \beta \circ (\mathbfit{v} \otimes \alpha),
\end{equation*}
then we can see that $T \in \mathcal{L}\left(\hat{V}, \hat{V}\right)$. In fact, for any $\mathbfit{u} \in V$, consider
\begin{align*}
    T(\beta)(\mathbfit{u}) & = \bigl(\beta \circ (\mathbfit{v} \otimes \alpha)\bigr)(\mathbfit{u}) \\
    & = \beta\bigl((\mathbfit{v} \otimes \alpha)(\mathbfit{u})\bigr).
\end{align*}
By Definition \ref{dfn:tranpose}, $T$ is the transpose of $\mathbfit{v} \otimes \alpha$ for any $\mathbfit{v} \in V$ and $\alpha \in \hat{V}$. Furthermore, note that $\alpha(\mathbfit{u}) \in \mathcal{F}$, we see that
\begin{align*}
    T(\beta)(\mathbfit{u}) & = \beta\bigl((\mathbfit{v} \otimes \alpha)(\mathbfit{u})\bigr) \\
    & = \beta\bigl(\alpha(\mathbfit{u})\mathbfit{v}\bigr) \\
    & = \alpha(\mathbfit{u})\beta(\mathbfit{v}) \\
    & = \bigl(\beta(\mathbfit{v})\alpha\bigr)(\mathbfit{u}).
\end{align*}
Therefore, actually $\beta \circ (\mathbfit{v} \otimes \alpha) = \beta(\mathbfit{v})\alpha$.
\begin{probox}{Tensor Products Form A Basis For $\mathcal{L}(V, V)$}{tenserProdBasis}
    Let $\zeta$ be the dual basis for $V$ with respect to some basis $z$, then 
    \begin{equation*}
        \left\{\mathbfit{z}_i \otimes \zeta^j \colon i, j = 1, 2, \cdots, n\right\}
    \end{equation*}
    is a basis for $\mathcal{L}(V, V)$.
    \tcblower
    \begin{proof}
        Let $T \in \mathcal{L}(V, V)$. For each $i = 1, 2, \cdots, n$, we have
        \begin{equation*}
            T(\mathbfit{z}_i) = \sum_{j = 1}^{n}a_{ij}\mathbfit{z}_j.
        \end{equation*}
        Define a mapping
        \begin{equation*}
            S \coloneqq \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}(\mathbfit{z}_j \otimes \zeta^i).
        \end{equation*}
        Note that $S \in \mathcal{L}(V, V)$. For any $\mathbfit{z}_k$, we have
        \begin{align*}
            S(\mathbfit{z}_k) & = \left(\sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}(\mathbfit{z}_j \otimes \zeta^i)\right)(\mathbfit{z}_k) \\
            & = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}\zeta^i(\mathbfit{z}_k)\mathbfit{z}_j \\
            & = \sum_{j = 1}^{n}a_{kj}\mathbfit{z}_j \\
            & = T(\mathbfit{z}_k).
        \end{align*}
        Therefore, 
        \begin{equation*}
            T = S = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}(\mathbfit{z}_j \otimes \zeta^i),
        \end{equation*} 
        so $\mathcal{L}(V, V) = \mathrm{span}\left\{\mathbfit{z}_i \otimes \zeta^j \colon i, j = 1, 2, \cdots, n\right\}$. Suppose $T = 0_{\mathcal{L}}$ is the zero mapping, then for each $\mathbfit{z}_k$ with $k = 1, 2, \cdots, n$, we have $S(\mathbfit{z}_k) = T(\mathbfit{z}_k) = \zero$. This implies that 
        \begin{equation*}
            \sum_{j = 1}^{n}a_{kj}\mathbfit{z}_k = \zero
        \end{equation*}
        for each $k = 1, 2, \cdots, n$. Since $z$ is a basis, we have $a_{kj} = 0$ for all $k, j = 1, 2, \cdots, n$. Therefore, the set $\left\{\mathbfit{z}_i \otimes \zeta^j \colon i, j = 1, 2, \cdots, n\right\}$
        is linearly independent and so is a basis for $\mathcal{L}(V, V)$.
    \end{proof}
\end{probox}
Therefore, we see that for every vector space $V$, if we fix any basis $\left\{\mathbfit{z}_i \colon i = 1, 2, \cdots, n\right\}$, then any $T \in \mathcal{L}(V, V)$ can be expressed as a linear combination of $\mathbfit{z}_i \otimes \zeta^j$ where $\zeta$ is the dual basis with respect to $z$. Proposition \ref{pro:tenserProdBasis} also implies that for any finite-dimensional vector space $V$, we have $\dim\bigl(\mathcal{L}(V, V)\bigr) = \dim(V)^2$.

Let $T \in \mathcal{L}(V, V)$. Fix a basic vector $\mathbfit{z}_j$ of $V$ with dual vector $\zeta^i$, we can ``extract'' the $(i, j)$ component of $T$ by
\begin{equation*}
    \zeta^i\bigl(T(\mathbfit{z}_j)\bigr) = \zeta^i\left(\sum_{k = 1}^{n}a_{kj}\mathbfit{z}_k\right) = \sum_{k = 1}^{n}a_{kj}\zeta^i(\mathbfit{z}_k) = a_{ij}.
\end{equation*}
\section{Matrix}
\begin{dfnbox}{Matrix}{matrix}
    An $n \times m$ {\color{red} \textbf{matrix}} is defined as an $m$-tuple of column vectors in $\mathcal{F}^n$.
\end{dfnbox}
Clearly, the set of all $n \times m$ matrices for any $m, n \in \N$ is a vector space. We denote this vector space as $\mathcal{M}_{n \times m}$. However, as compared to other vectors, we can also define the notion of multiplication between some matrices.
\begin{dfnbox}{Matrix Multiplication}{matMult}
    Let $\mathbfit{M}$ be an $n \times m$ matrix and $\mathbfit{N}$ be an $m \times p$ matrix, then their product $\mathbfit{P = MN}$ is a matrix whose entries are given by
    \begin{equation*}
        P^i_j = \sum_{k = 1}^{m}M^i_kN^k_i.
    \end{equation*}
\end{dfnbox}
Consider the vector space $\mathcal{M}_{n \times n}$ of square matrices. It is easy to see that this space is \textbf{closed under multiplication.} 
\begin{dfnbox}{Algebra}{algebra}
    An {\color{red} \textbf{algebra}} is a vector space $V$ with a binary mapping $\times \colon V^2 \to V$ known as multiplication.
\end{dfnbox}
It is not difficult to see that from Proposition \ref{pro:tenserProdBasis}, the components of a mapping $T \in \mathcal{L}(V, V)$ can be essentially seen as the entries of a matrix.
\begin{dfnbox}{Matrix of A Linear Transformation}{linTransMat}
    Let $T \in \mathcal{L}(V, V)$. If for some basis $z$ of $V$, we have
    \begin{equation*}
        T = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}\mathbfit{z}_i \otimes \zeta^j,
    \end{equation*}
    then the matrix $\mathbfit{T}$ with $T^i_j = a_{ij}$ is called the matrix of $T$ relative to $z$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $T^i_j = \zeta^i\bigl(T(\mathbfit{z}_j)\bigr)$.
    \end{remark}
\end{notebox}
Let $\hat{T}$ be the transpose of $T$, then by Definition \ref{dfn:tranpose} we have
\begin{equation*}
    \zeta^i\bigl(T(\mathbfit{z}_j)\bigr) = \hat{T}(\zeta^i)(\mathbfit{z}_j) = z_j\bigl(\hat{T}(\zeta^i)\bigr),
\end{equation*}
which we can use to verify that the matrix of $\hat{T}$ is indeed $\mathbfit{T}^{\mathrm{T}}$.

Let $V$ be a finite-dimensional vector space with $\dim(V) = n$ and let $\mathcal{M}_{n \times n}$ be the vector space of all $n \times n$ matrices. Define $M_z \colon \mathcal{L}(V, V) \to \mathcal{M}_{n \times n}$ that maps each $T \in \mathcal{L}(V, V)$ to its matrix representation $\mathbfit{T}$. Intuitively, it is an isomorphism.

\begin{notebox}
    \begin{remark}
        Note that actually $\mathcal{M}_{n \times n} \cong \mathcal{F}^{n^2}$ and that a basis for $\mathcal{L}(V, V)$ is just an isomorphism $z \colon \mathcal{F}^{n^2} \to \mathcal{L}(V, V)$, so in some sense $M_z$ is just $z^{-1}$.
    \end{remark}
\end{notebox}

Consider $\mathcal{L}(\mathcal{F}^n, \mathcal{F}^n)$, it is clear that every $T \in \mathcal{L}(\mathcal{F}^n, \mathcal{F}^n)$ can be expressed as
\begin{equation*}
    T = \sum_{i = 1}^{n}\sum_{j = 1}^{n}M_z(T)_i^j\left(\mathbfit{e}_i \otimes \epsilon^i\right).
\end{equation*}
\begin{probox}{Matrices of Composite Mappings}{matComp}
    Let $S, T \in \mathcal{L}(V, V)$, then for any basis $z$ of $V$, 
    \begin{equation*}
        M_z(ST) = M_z(S)M_z(T).
    \end{equation*}
\end{probox}
Suppose $\mathbfit{u} = T(\mathbfit{v})$ and $z(\mathbfit{a}) = \mathbfit{v}$, $z(\mathbfit{b}) = \mathbfit{u}$, then 
\begin{equation*}
    \mathbfit{b} = \left(z^{-1} \circ T \circ z\right)(\mathbfit{a}).
\end{equation*}
Suppose $\alpha \in \hat{V}$, we define some $\alpha^*_z \in \hat{\mathcal{F}^n}$ by $\alpha^*_z \coloneqq \alpha \circ z$.

Let $z, y$ be bases for some finite-dimensional vector space $V$. Define a mapping
\begin{equation*}
    P \coloneqq z^{-1} \circ y \in \mathcal{L}\left(\mathcal{F}^n, \mathcal{F}^n\right),
\end{equation*}
then if $\mathbfit{v} \in V$ is such that $\mathbfit{v} = z(\mathbfit{a}) = y(\mathbfit{b})$, clearly $\mathbfit{a} = P(\mathbfit{b})$. Note that $y = z \circ P$, so
\begin{align*}
    y_i & = \epsilon^i(y) \\
    & = \epsilon^i(z \circ P) \\
    & = \sum_{k = 1}^{n}\mathbfit{P}^k_iz_k.
\end{align*} 
\end{document}