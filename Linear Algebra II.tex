\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\I}{\mathbfit{I}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Linear Algebra II
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Vector Spaces}
\section{Fields, Scalars and Vectors}
In elementary mathematics, we often refer to a vector as an ordered tuple of numbers with a direction and a magnitude. However, there is a much more abstract aspect to the notion of vectors. In fact, let us first generalise the notion of \textit{scalars}, which are taken as complex constants in an elementary level. 

In general, we have the following algebraic structure:
\begin{dfnbox}{Field}{field}
    A {\color{red} \textbf{field}} is a set $\mathcal{F}$ with two binary operations $\mathcal{F}^2 \to \mathcal{F}$, namely addition and multiplication, such that
    \begin{enumerate}
        \item $u + v = v + u$ for all $u, v \in \mathcal{F}$;
        \item $(u + v) + w = u + (v + w)$ for all $u, v, w \in \mathcal{F}$;
        \item $uv = vu$ for all $u, v \in \mathcal{F}$;
        \item $(uv)w = u(vw)$ for all $u, v, w \in \mathcal{F}$;
        \item $u(v + w) = uv + uw$ for all $u, v, w \in \mathcal{F}$;
        \item there exists $0 \in \mathcal{F}$ such that $u + 0 = u$ for all $u \in \mathcal{F}$;
        \item there exists $1 \in \mathcal{F}$ such that $1u = u$ for all $u \in \mathcal{F}$;
        \item for every $u \in \mathcal{F}$, there exists some $v \in \mathcal{F}$ such that $u + v = 0$;
        \item for every $u \in \mathcal{F}$, there exists some $v \in \mathcal{F}$ such that $uv = 1$.
    \end{enumerate}
\end{dfnbox}
One may check that both $\R$ and $\C$ are fields. It turns out that we can also generalise the concept of vectors as any objects which possess properties similar to that of Euclidean vectors, i.e., we can view a vector as a mathematical quantity which can be added up and multiplied by another quantity called a scalar with some axioms which they follow. Rigorously, we define the notion of a \textit{vector space}.
\begin{dfnbox}{Vector Space}{vecSpace}
    A {\color{red} \textbf{vector space}} is a set $V$ over a field $\mathcal{F}$ with two binary operations, namely 
    \begin{itemize}
        \item addition $+ \colon V^2 \to V$, and
        \item scalar multiplication $(\quad)(\quad) \colon \mathcal{F} \times V \to V$,
    \end{itemize}
    such that
    \begin{enumerate}
        \item $\mathbfit{u + v = v + u}$ for all $\mathbfit{u}, \mathbfit{v} \in V$;
        \item $\mathbfit{(u + v) + w = u + (v + w)}$ for all $\mathbfit{u, v, w} \in V$;
        \item $ab\mathbfit{v} = a(b\mathbfit{v})$ for all $a, b \in \mathcal{F}$ and $\mathbfit{v} \in V$;
        \item there exists an {\color{red} \textbf{additive identity}} or {\color{red} \textbf{zero vector}} $\zero \in V$ such that $\mathbfit{v} + \zero = \mathbfit{v}$ for all $\mathbfit{v} \in V$;
        \item every $\mathbfit{v} \in V$ has an {\color{red} \textbf{additive inverse}} $\mathbfit{w} \in V$ with $\mathbfit{v + w} = 0$;
        \item there exists a {\color{red} \textbf{multiplicative identity}} $1 \in \mathcal{F}$ such that $1\mathbfit{v} = \mathbfit{v}$ for all $\mathbfit{v} \in V$;
        \item $a\mathbfit{(u + v)} = a\mathbfit{u} + a\mathbfit{v}$ and $(a + b)\mathbfit{u} = a\mathbfit{u} + b\mathbfit{u}$ for all $a, b \in \mathcal{F}$ and $\mathbfit{u, v} \in V$.
    \end{enumerate}
\end{dfnbox}
Notice that here, the definitions of addition in scalar multiplication in a vector space imply that any vector space must be \textbf{closed} under these two operations. Notice also that the operations ``addition'' and ``scalar multiplication'' are not necessary the addition and scalar multiplication which we are used to in $\R^n$, but abstract mappings which satisfy the given axioms.

We shall prove a few basic properties regarding vector spaces.
\begin{thmbox}{Uniqueness of Additive Identity}{unique0}
    Let $V$ be a vector space with $\zero \in V$ as an additive identity, then $\zero$ is unique.
    \tcblower
    \begin{proof}
        Suppose on contrary that there exists $\mathbfit{u} \in V$ such that $\mathbfit{v + u = v}$ for all $\mathbfit{v} \in V$. Since $\zero \in V$, we have
        \begin{equation*}
            \zero + \mathbfit{u} = \zero.
        \end{equation*}
        However, $\zero$ is the additive identity, so 
        \begin{equation*}
            \mathbfit{u} = \mathbfit{u} + \zero = \zero + \mathbfit{u} = \zero,
        \end{equation*}
        i.e. $\zero$ is unique.
    \end{proof}
\end{thmbox}
Similarly, we can also prove the uniqueness of additive inverse.
\begin{thmbox}{Uniqueness of Additive Inverse}{unique-1}
    Let $V$ be a vector space, then every $\mathbfit{v} \in V$ has a unique additive inverse.
    \tcblower
    \begin{proof}
        Suppose on contrary that there exist $\mathbfit{u, w} \in V$ both being additive inverse of~$\mathbfit{v}$, then $\mathbfit{u + v} = \zero$ and $\mathbfit{w + v} = \zero$. Therefore,
        \begin{equation*}
            \mathbfit{u} = \mathbfit{(u + v) + u} = \mathbfit{(w + v) + u} = \mathbfit{w + (u + v)} = \mathbfit{w},
        \end{equation*}
        i.e., $\mathbfit{v}$ has a unique additive inverse.
    \end{proof}
\end{thmbox}
Theorem \ref{thm:unique-1} justifies the notation $-\mathbfit{u}$ to denote the additive inverse of $\mathbfit{u}$. However, so far we have not ascertained the fact that $-\mathbfit{u} = (-1)\mathbfit{u}$ (note that the former means the inverse of $\mathbfit{u}$ while the latter means $\mathbfit{u}$ multiplied by the scalar $-1$)! While seemingly innocent, this result is not as easily proven as it looks.

First, we shall justify that $0\mathbfit{u} = \zero$ for all $\mathbfit{u} \in V$. Notice that
\begin{equation*}
    0\mathbfit{u} = (0 + 0)\mathbfit{u} = 0\mathbfit{u} + 0\mathbfit{u}.
\end{equation*}
Adding $-(0\mathbfit{u})$ to both sides of the equation yields $0\mathbfit{u} = \zero$ as desired. From this result we see that
\begin{equation*}
    (-1)\mathbfit{u} + \mathbfit{u} = (-1 + 1)\mathbfit{u} = 0\mathbfit{u} = \zero.
\end{equation*}
By uniqueness of additive inverse, we must have $(-1)\mathbfit{u} = -\mathbfit{u}$.

Note that by using a similar technique we can prove that $a\zero = \zero$ for all $a \in \mathcal{F}$, and so~$\zero = -\zero$ as a consequence.

Additionally, note that subtraction is defined as $\mathbf{u - v} = \mathbfit{u} + (-1)\mathbfit{v}$, so the above result allows us to write $\mathbfit{u - v} = \mathbfit{u} + (-\mathbfit{v})$.

\subsection{Subspaces}
Note that a vector space is extended based on a set of vectors, so we can define \textit{subspaces} similarly to the notion of subsets.
\begin{dfnbox}{Subspace}{subspace}
    Let $V$ be a vector space. $U \subseteq V$ is called a {\color{red} \textbf{subspace}} if $U$ is a vector space under addition and scalar multiplication in $V$.
\end{dfnbox}
It is easy to see that the intersection of any number of subspaces of a vector space $V$ is still a subspace of $V$, but the union might not be so. In particular, we would like to consider a special construct known as \textit{direct sum}.
\begin{dfnbox}{Direct Sum}{directSum}
    Let $V$ be a vector space and $U_1, U_2 \subseteq V$ such that $U_1 \cap U_2 = \{\zero\}$, then their {\color{red} \textbf{direct sum}} is defined as
    \begin{equation*}
        U_1 \oplus U_2 \coloneqq \left\{\mathbfit{u}_1 + \mathbfit{u}_2 \colon \mathbfit{u}_1 \in U_1, \mathbfit{u}_2 \in U_2\right\}.
    \end{equation*}
\end{dfnbox}
More generally, we can let $U_1$ and $U_2$ be any subsets of $V$ and define $U_1 + U_2$ in the same manner, which is known as the \textit{sum} of $U_1$ and $U_2$.

It can be easily proven that for any vector space $V$, the direct sum of any two subspaces of $V$ is still a subspace of $V$. A nice property of direct sum can be proven as follows:
\begin{probox}{Unique Decomposition with Direct Sums}{uniqueDecomp}
    Let $V = U_1 \oplus U_2$, then every $\mathbfit{v} \in V$ can be uniquely expressed as $\mathbfit{u + w}$ for some $\mathbfit{u} \in U_1$ and $\mathbfit{w} \in U_2$.
    \tcblower
    \begin{proof}
        The existence of $\mathbfit{u}$ and $\mathbfit{w}$ is trivial by Definition \ref{dfn:directSum}. Suppose there exist $\mathbfit{u}' \in U_1$ and $\mathbfit{w}' \in U_2$ such that $\mathbfit{u + w} = \mathbfit{u}' + \mathbfit{w}'$, then we have $\mathbfit{u - u}' = \mathbfit{w}' - \mathbfit{w}$. Note that $\mathbfit{u - u}' \in U_1$ and $\mathbfit{w}' - \mathbfit{w} \in U_2$, so we have $\mathbfit{u - u}', \mathbfit{w}' - \mathbfit{w} \in U_1 \cap U_2 = \{\zero\}$, i.e.,
        \begin{equation*}
            \mathbfit{u - u}' = \mathbfit{w}' - \mathbfit{w} = \zero.
        \end{equation*} 
        Therefore, $\mathbfit{u} = \mathbfit{u}'$ and $\mathbfit{w} = \mathbfit{w}'$, i.e., $\mathbfit{u}$ and $\mathbfit{w}$ are unique.
    \end{proof}
\end{probox}
In some sense, a direct sum of $V$ can be viewed as a ``partition'' of $V$ into two subsets with a minimal overlap. Note that unlike partition in its real definition, the subspaces $U_1$ and $U_2$ here cannot be disjoint sets as both of them have to contain the zero vector in $V$. More generally, for any subspace $U \subseteq V$, we have $\zero_U = \zero_V$, the proof of which should be trivial enough as an exercise to the reader.

In particular, we would like to consider $\mathcal{F}^n$ for a general field $\mathcal{F}$. We can define the dot product operation over $\mathcal{F}^n$ in the same way as $\R^n$. Take any subspace $U \subseteq \mathcal{F}^n$ and define the set
\begin{equation*}
    U_{\perp} \coloneqq \left\{\mathbfit{u} \in \mathcal{F}^n \colon \mathbfit{u \cdot v} = 0 \quad\textrm{for all } \mathbfit{v} \in U\right\},
\end{equation*}
then $\mathcal{F}^n = U \oplus U_{\perp}$.

To justify this, we first take any $\mathbfit{v} \in \mathcal{F}^n$. Using some calculus, we can show that there exists 
\begin{equation*}
    \mathbfit{u}_0 = \argmin_{\mathbfit{u} \in U}\abs{\mathbfit{u \cdot v}}.
\end{equation*}
Let $\mathbfit{w = v - u}_0$, then clearly $\mathbfit{v = w + u}_0$ where $\mathbfit{u}_0 \in U$ and $\mathbfit{w} \in U_{\perp}$. This implies that~$V = U + U_{\perp}$. Note that $\zero$ is the only vector in $\mathcal{F}^n$ which is orthogonal to itself, so we have $U \cap U_{\perp} = \{\zero\}$. It follows that $V = U \oplus U_{\perp}$.

\section{Isomorphism}
Since the underlying structure of a vector space is still a set, the notion of a mapping between two vector spaces is well-defined. However, note that a vector space possesses unique algebraic structures and properties, namely that the linear combinations of any members of the space are still in the space, so we would like to focus on mapping which preserves such properties.
\begin{dfnbox}{Homomorphism}{homomorphic}
    Let $U$ and $V$ be vector spaces, a {\color{red} \textbf{homomorphism}} is a mapping $\phi \colon U \to V$ such that
    \begin{equation*}
        \phi(\mathbfit{u + v}) = \phi(\mathbfit{u}) + \phi(\mathbfit{v}), \qquad \phi(c\mathbfit{u}) = c\phi(\mathbfit{u})
    \end{equation*}
    for any $\mathbfit{u}, \mathbfit{v} \in U$ and $c \in \mathcal{F}$.
\end{dfnbox}
Naturally, if a homomorphism is bijective, then it means that the elements in two vector spaces have a one-to-one correspondence. In practice, this means we can treat them as equivalent spaces in some sense.
\begin{dfnbox}{Isomorphism}{isomorphic}
    An {\color{red} \textbf{isomorphism}} between vector spaces $U$ and $V$ is a homomorphism between them which is bijective.
\end{dfnbox}
An interesting fact here is that an isomorphism between any vector spaces is not unique. To see this, let us first consider an arbitrary vector space $V$. Now, we can always find the trivial isomorphism $\mathrm{id}_V \colon V \to V$. In fact, any mapping $\phi \colon \mathbfit{v} \mapsto c\mathbfit{v}$ where $c$ is a scalar is clearly an isomorphism from $V$ to $V$. This means that there are infinitely many isomorphisms from~$V$ to itself.

Let $U$ be an arbitrary vector space such that there exists some isomorphism $\psi \colon V \to U$. We consider the following theorem:
\begin{thmbox}{Composition Preserves Isomorphism}{compoPreserveIsomorphic}
    Let $U, V, W$ be vector spaces. If $\phi \colon U \to V$ and $\psi \colon V \to W$ are isomorphisms, then the composite mapping $\phi \circ \psi \colon U \to W$ is an isomorphism.
    \tcblower
    \begin{proof}
        Since both $\phi$ and $\psi$ are bijective, it is clear that $\psi \circ \phi$ is bijective. Take any $\mathbfit{u}, \mathbfit{v} \in U$, then
        \begin{align*}
            \psi\bigl(\phi(\mathbfit{u + v})\bigr) = \psi\bigl(\phi(\mathbfit{u}) + \phi(\mathbfit{v})\bigr) = \psi\bigl(\phi(\mathbfit{u})\bigr) + \psi\bigl(\phi(\mathbfit{v})\bigr)
        \end{align*}
        since $\phi(\mathbfit{u}), \phi(\mathbfit{v}) \in V$. Therefore, $\psi \circ \phi$ is an isomorphism.
    \end{proof}
\end{thmbox}
Using Theorem \ref{thm:compoPreserveIsomorphic}, we can immediately see that if $\phi \colon V \to V$ is any isomorphism and $\psi \colon V \to U$ is an isomorphism, then $\psi \circ \phi$ is an isomorphism between $V$ and $U$. Therefore, there are infinitely many isomorphisms between $V$ and $U$.
\begin{notebox}
    \begin{remark}
        If $V$ is isomorphic to $U$, we write $V \cong U$. Clearly, $\cong$ is an equivalence relation.
    \end{remark}
\end{notebox}

Now, observe that for any field $\mathcal{F}$, the set $\mathcal{F}^n$ for any $n \in \N$ is a vector space over $\mathcal{F}$.
\begin{dfnbox}{Finite-Dimensional Vector Space}{finiteDim}
    A vector space $V$ is said to be {\color{red} \textbf{finite-dimensional}} over a field $\mathcal{F}$ if it it is isomorphic to $\mathcal{F}^n$ for some $n \in \N$. $n$ is called the {\color{red} \textbf{dimension}} of $V$.
\end{dfnbox}
Obviously, a vector space which is not finite-dimensional is called \textit{infinite-dimensional}. For example, the set of all polynomials is a vector space of infinite dimension.

\section{Basis}
Recall that a \textit{linear combination} of vectors is in the form of
\begin{equation*}
    \sum a_i\mathbfit{v}_i = a_1\mathbfit{v}_1 + a_2\mathbfit{v}_2 + \cdots.
\end{equation*}
In case where no confusion is caused, this can be abbreviated as $a_i\mathbfit{v}_i$. Recall also that a \textit{span} of a set of vectors is defined as
\begin{equation*}
    \mathrm{span}(V) \coloneqq \left\{a_i\mathbfit{v}_i \colon \mathbfit{v}_i \in V\right\},
\end{equation*}
where $a_i$'s are scalars. A span of a subset of a vector space $V$ is clearly a subspace of $V$. We also know that a set of vectors $S$ is said to be \textit{linearly independent} if and only if  $a_i\mathbfit{v}_i = \zero$ implies that $a_i = 0$ for all $i = 1, 2, \cdots$. A \textit{basis} of a vector space $V$ is a linearly independent set $S$ such that $\mathrm{span}(S) = V$. One may check that if $S$ is a basis for $V$, then any $\mathbfit{v} \in V$ can be \textbf{uniquely} expressed as a linear combination of the members of $S$, but $S$ itself is not unique. In particular, the coefficients in this linear combination is known as the \textit{components} of $\mathbfit{v}$ relative to $S$.

We will see that the basis is closely related to the dimension of vector spaces. First, let us consider the trivial basis for $\mathcal{F}^n$.
\begin{dfnbox}{Canonical Basis}{canonicalBasis}
    The {\color{red} \textbf{canonical basis}} for $\mathcal{F}^n$ is defined as $\left\{\mathbfit{e}_i \colon i = 1, 2, \cdots, n\right\}$, where each $\mathbfit{e}_i$ is a column vector with $1_{\mathcal{F}}$ in its $i$-th row and $0_{\mathcal{F}}$ in the other rows.
\end{dfnbox}
It is easy to see that the number of vectors in any basis of a finite-dimensional vector space~$V$ is uniquely equal to its dimension.
\begin{probox}{Dimension as Cardinality of Basis}{dimIsCardBasis}
    Let $V$ be a finite-dimensional vector space with dimension $n$ and basis $S$, then $n = \abs{S}$.
    \tcblower
    \begin{proof}
        Note that $V \cong \mathcal{F}^n$. Let $\mathbfit{v} \in V$ be an arbitrary vector, then there is some~$\mathbfit{u} \in \mathcal{F}^n$ and a bijection $\phi \colon \mathcal{F}^n \to V$ such that 
        \begin{align*}
            \mathbfit{v} & = \phi(\mathbfit{u}) \\
            & = \phi\left(\sum_{i = 1}^{n}a_i\mathbfit{e}_i\right) \\
            & = \sum_{i = 1}^{n}a_i\phi(\mathbfit{e}_i),
        \end{align*}
        where $a_i \in \mathcal{F}$ for $i = 1, 2, \cdots, n$. This means that $V$ is spanned by at most $n$ vectors and so its basis is finite. Suppose on contrary that $\abs{S} = m < n$, then for any $\mathbfit{w} \in \mathcal{F}^n$, there is some $\mathbfit{r} \in V$ such that
        \begin{align*}
            \mathbfit{w} & = \phi^{-1}(\mathbfit{r}) \\
            & = \phi^{-1}\left(\sum_{i = 1}^{m}b_i\mathbfit{s}_i\right) \\
            & = \sum_{i = 1}^{m}b_i\phi^{-1}(\mathbfit{s}_i),
        \end{align*}
        where $b_i \in \mathcal{F}$ for $i = 1, 2, \cdots, m$. This means that $\mathcal{F}^n$ is spanned by $m$ vectors, which is a contradiction. Therefore, $\abs{S} = n$.
    \end{proof}
\end{probox}
An immediate corollary from Proposition \ref{pro:dimIsCardBasis} is that a finite-dimensional vector space always has a unique dimension, as otherwise it will have two bases with different cardinalities.

Note that since every vector in a vector space $V$ can be uniquely expressed as a linear combination of a basis $S$ for $V$, this really means that we can view the notion of a basis equivalently as a bijection between $\mathcal{F}^n$ and $V$, i.e., for any $(a_1, a_2, \cdots, a_n) \in \mathcal{F}^n$, we can map the tuple to a vector in $V$ whose components are exactly $a_1, a_2, \cdots, a_n$.

Now, let us denote a basis for $V$ by the mapping $z$. Notice that for any $\mathbfit{c}_1, \mathbfit{c}_2 \in \mathcal{F}^n$, we have 
\begin{equation*}
    z(\mathbfit{c}_1 + \mathbfit{c}_2) = z(\mathbfit{c}_1) + z(\mathbfit{c}_2),
\end{equation*}
so a basis is nothing more but an isomorphism!

\chapter{Linear Transformations}
\section{Linear Transformations}
\begin{dfnbox}{Linear Transformation}{linearTrans}
    A {\color{red} \textbf{linear transformation}} is a mapping $T \colon V \to W$, where $V$ and $W$ are vector spaces over $\mathcal{F}$, such that
    \begin{equation*}
        T(\mathbfit{v + u}) + T(\mathbfit{v}) + T(\mathbfit{u}), \qquad T(c\mathbfit{v}) = cT(\mathbfit{v})
    \end{equation*}
    for all $\mathbfit{v}, \mathbfit{u} \in V$ and all $c \in \mathcal{F}$.
\end{dfnbox}
In essence, a linear transformation is a function between vector spaces which preserves the vector structure of its domain. We will see that many notions discussed so far can actually be abstracted into a linear transformation.

Let $V \cong \mathcal{F}^n$ be a finite-dimensional vector space. Recall that a basis for $V$ is essentially a bijective mapping $z \colon \mathcal{F}^n \to V$, so a basis is a linear transformation. In particular, if $y$ is another basis for $V$, then clearly $y \circ z^{-1}$ is a mapping from $V$ to itself. Let $Q = y \circ z^{-1}$, then we have $y = Q \circ z$. This is known as a change of basis.

Now, consider a vector space $V$ over $\mathcal{F}$. Fix some $\mathbfit{v} \in V$ and define a mapping $\Theta_{\mathbfit{v}} \colon \mathcal{F} \to V$ by $\Theta_{\mathbfit{v}}(a) = a\mathbfit{v}$. One may check that $\Theta_{\mathbfit{v}}$ is a linear transformation, but $\Theta_{\mathbfit{v}}$ is essentially $\mathbfit{v}$, so vectors are linear transformations as well.
\begin{dfnbox}{Range}{range}
    Let $T \colon V \to W$ be a linear transformation. The {\color{red} \textbf{range}} of $T$ is defined as
    \begin{equation*}
        \mathrm{range}(T) \coloneqq \left\{T(\mathbfit{v}) \colon \mathbfit{v} \in V\right\}.
    \end{equation*}
    The dimension of $\mathrm{range}(T)$ is called the {\color{red} \textbf{rank}} of $T$.
\end{dfnbox}
It is immediate from the definition that $T$ is surjective if and only if $\mathrm{range}(T) = W$. Consider $T(\mathbfit{v}), T(\mathbfit{u}) \in \mathrm{range}(T)$ for some $\mathbfit{v} \neq \mathbfit{u}$ in $V$, then we have
\begin{equation*}
    \alpha T(\mathbfit{v}) + T(\mathbfit{u}) = T(\alpha\mathbfit{v} + \mathbfit{u}).
\end{equation*}
Clearly, $\alpha\mathbfit{v} + \mathbfit{u} \in V$, so $\alpha T(\mathbfit{v}) + T(\mathbfit{u}) \in \mathrm{range}(T)$ and $\mathrm{range}(T) \subseteq W$ is a vector space.
\begin{dfnbox}{Kernel}{ker}
    Let $T \colon V \to W$ be a linear transformation. The {\color{red} \textbf{kernel}} of $T$ is defined as 
    \begin{equation*}
        \mathrm{ker}(T) \coloneqq \left\{\mathbfit{v} \in V \colon T(\mathbfit{v}) = \zero_{W}\right\}.
    \end{equation*}
    The dimension of $\ker(T)$ is called the {\color{red} \textbf{nullity}} of $T$.
\end{dfnbox}
Obviously, $\ker(T)$ is a vector space. We claim that $\mathrm{null}(T)$ is related to the injectivity of $T$ by the following result:
\begin{probox}{Injectivity Test}{injectTest}
    A linear transformation $T$ is injective if and only if $\mathrm{null}(T) = 0$.
    \tcblower
    \begin{proof}
        Suppose that $T$ is injective. We shall prove that $\mathrm{null}(T) = 0$ by considering the contrapositive. Suppose that $\mathrm{null}(T) \neq 0$, then there is some non-zero $\mathbfit{v} \in \ker(T)$ with $T(\mathbfit{v}) = \zero$, so $T$ is not injective.
        \\\\
        Suppose conversely that $\mathrm{null}(T) = 0$, then $\ker(T) = \{\zero\}$. Let $T(\mathbfit{v}) = T(\mathbfit{u})$, then
        \begin{equation*}
            \zero = T(\mathbfit{v}) - T(\mathbfit{u}) = T(\mathbfit{v - u}),
        \end{equation*}
        so $\mathbfit{v - u} \in \ker(T)$. This means that $\mathbfit{v - u} = \zero$ so $\mathbfit{v = u}$. Therefore, $T$ is injective.
    \end{proof}
\end{probox}
For any linear transformation $T \colon V \to W$, we have 
\begin{equation*}
    T(\zero_V) = T(\zero_V + \zero_V) = 2T(\zero_V).
\end{equation*}
Cancelling $T(\zero_V)$ on both sides we have $\zero_W = T(\zero_V)$. Therefore, $\ker(T) - \{\zero_V\}$ consists of all non-zero vectors which are mapped to zero under $T$, i.e., for all $\mathbfit{u} \in U \coloneqq V - (\ker(T) - \{\zero_V\})$, $T(\mathbfit{u}) \neq \zero_W$. 
\begin{thmbox}{Fundamental Theorem of Linear Transformations}{rankNull}
    Let $T \colon V \to W$ be a linear transformation, then $\dim(V) = \mathrm{rank}(T) + \mathrm{null}(T)$.
    \tcblower
    \begin{proof}
        Define $U \coloneqq V - (\ker(T) - \{\zero_V\})$, then $V = U \oplus \ker(T)$. Let $S \colon U \to \mathrm{range}(T)$ be defined by $S(\mathbfit{u}) = T(\mathbfit{u})$. Note that for all $\mathbfit{v} \in \mathrm{range}(T)$, there exists some $\mathbfit{u} \in U$ such that $S(\mathbfit{u}) = \mathbfit{v}$, so $S$ is surjective. Suppose there exist vectors $\mathbfit{u}_1, \mathbfit{u}_2 \in U$ such that $S(\mathbfit{u}_1) = S(\mathbfit{u}_2)$, then
        \begin{equation*}
            \zero_W = S(\mathbfit{u}_1) - S(\mathbfit{u}_2) = S(\mathbfit{u}_1 - \mathbfit{u}_2).
        \end{equation*}
        Therefore, $\mathbfit{u}_1 - \mathbfit{u}_2 = \zero_U$, and so $\mathbfit{u}_1 = \mathbfit{u}_2$, which implies that $S$ is injective. Therefore,~$S$ is a bijection and so $U \cong \mathrm{range}(T)$, which means $\dim(U) = \mathrm{rank}(T)$. Therefore,
        \begin{equation*}
            \dim(V) = \dim(U) + \mathrm{null}(T) = \mathrm{rank}(T) + \mathrm{null}(T).
        \end{equation*}
    \end{proof}
\end{thmbox}
An interesting application of Theorem \ref{thm:rankNull} gives the following corollary:
\begin{corbox}{Bijectivity of Reflexive Mappings}{bijectiveReflexiveMap}
    Let $T \colon V \to V$ be a linear transformation, then $T$ is injective if and only if it is surjective.
    \tcblower
    \begin{proof}
        Suppose that $T$ is injective, then $V \cong \mathrm{range}(T)$ and so $\mathrm{null}(T) = 0$, which implies that $\dim(V) = \mathrm{rank}(T)$. However, $\mathrm{range}(T) \subseteq V$, so $\mathrm{range}(T) = V$. This means that $T$ is surjective.
        \\\\
        Suppose that $T$ is surjective, then $\dim(V) = \mathrm{rank}(T)$ and so $\mathrm{null}(T) = 0$. Therefore, $T$ is injective.
    \end{proof}
\end{corbox}
\section{Duality}
Let $V$ and $W$ be vector spaces and define $\mathcal{L}(V, W)$ to be the set of all linear transformations from $V$ to $W$. One may check that $\mathcal{L}(V, W)$ is a vector space.
\begin{dfnbox}{Dual Space}{dualSpace}
    Let $V$ be a vector space over $\mathcal{F}$. The {\color{red} \textbf{dual space}} of $V$ is defined as $\hat{V} \coloneqq \mathcal{L}(V, \mathcal{F})$. The elements of $\hat{V}$ are called {\color{red} \textbf{dual vectors}}.
\end{dfnbox}
An intuitive example for an element of $\hat{V}$ is as follows: let $\left\{\mathbfit{z}_1, \mathbfit{z}_2, \cdots, \mathbfit{z}_n\right\}$ be a basis for $V$. For any $\mathbfit{v} \in V$, we can write
\begin{equation*}
    \mathbfit{v} = \sum_{i = 1}^{n}a_i\mathbfit{z}_i
\end{equation*}
for $a_1, a_2, \cdots, a_n \in \mathcal{F}$. Now, define a mapping $\zeta^i \colon V \to \mathcal{F}$ as $\zeta^i(\mathbfit{v}) = a_i$, then clearly $\zeta^i \in \hat{V}$ for $i = 1, 2, \cdots, n$.

In particular, we see that
\begin{equation*}
    \zeta^i(\mathbfit{z}_j) = \begin{cases}
        1 & \textrm{if } i = j \\
        0 & \textrm{otherwise}
    \end{cases}.
\end{equation*}
Intuitively, this means that for each $\mathbfit{v} \in V$, we have $z^{-1}(\mathbfit{v}) = \sum_{i = 1}^{n}a_i\zeta^i(\mathbfit{z}_i)$.
\begin{dfnbox}{Dual Basis}{dualBasis}
    Let $V$ be a vector space with a basis $\left\{\mathbfit{z}_1, \mathbfit{z}_2, \cdots, \mathbfit{z}_n\right\}$. The {\color{red} \textbf{dual basis}} of $\left\{\mathbfit{z}_1, \mathbfit{z}_2, \cdots, \mathbfit{z}_n\right\}$ is defined as the set of all $\zeta^i \in \hat{V}$ such that 
    \begin{equation*}
        \zeta^i(\mathbfit{z}_j) = \begin{cases}
            1 & \textrm{if } i = j \\
            0 & \textrm{otherwise}
        \end{cases}.
    \end{equation*}
\end{dfnbox}
Let $\zeta$ be a dual basis and $\alpha \in \hat{V}$ be a linear mapping over a finite-dimensional vector space $V$ with $\dim(V) = n$. For any $\mathbfit{v} \in V$ with respect to basis $z$, define a mapping $\beta \colon V \to \mathcal{F}$ by 
\begin{equation*}
    \beta(\mathbfit{v}) = \left(\sum_{i = 1}^{n}\alpha(\mathbfit{z}_i)\zeta^i\right)(\mathbfit{v}).
\end{equation*}
Clearly, $\beta$ is a linear combination of the elements of $\zeta$. For any $\mathbfit{z}_j \in z$, we have
\begin{align*}
    \beta(\mathbfit{z}_j) & = \left(\sum_{i = 1}^{n}\alpha(\mathbfit{z}_i)\zeta^i\right)(\mathbfit{z}_j) \\
    & = \sum_{i = 1}^{n}\alpha(\mathbfit{z}_i)\zeta^i(\mathbfit{z}_j) \\
    & = \alpha(\mathbfit{z}_j).
\end{align*}
Since $z$ is a basis for $V$, this implies that for any $\mathbfit{v} \in V$, we have $\beta(\mathbfit{v}) = \alpha(\mathbfit{v})$. Therefore, $\alpha \in \mathrm{span}(\zeta)$. 

Consider
\begin{equation*}
    \sum_{i = 1}^{n}p_i\zeta^i = 0_{\hat{V}},
\end{equation*}
which is the zero mapping from $V$ to $\{0\}$, this means that for any $\mathbfit{v} \in V$, we have
\begin{align*}
    0 & = \left(\sum_{i = 1}^{n}p_i\zeta^i\right)(\mathbfit{v}) \\
    & = \left(\sum_{i = 1}^{n}p_i\zeta^i\right)\left(\sum_{j = 1}^{n}a_j\mathbfit{z}_j\right) \\
    & = \sum_{i = 1}^{n}\left(p_i\sum_{j = 1}^{n}a_j\zeta^i(\mathbfit{z}_j)\right) \\
    & = \sum_{i = 1}^{n}p_ia_i.
\end{align*}
However, since the $a_i$'s are arbitrary, we have $p_i = 0$ for all $i = 1, 2, \cdots, n$. Therefore, $\zeta$ is linearly independent. This means that $\zeta$ is indeed a basis. Clearly, this also implies that any finite-dimensional vector space has the same dimension as its dual space. 
\begin{probox}{$\dim(V) = \dim\left(\hat{V}\right)$}{dualHasEqualDim}
    Let $V$ be any finite-dimensional vector space, then $\dim(V) = \dim\left(\hat{V}\right)$.
\end{probox}
Furthermore, note that for any dual vector $\phi = \sum_{i = 1}^{n}p_i\zeta^i$, we have
\begin{equation*}
    \phi(\mathbfit{z}_j) = \sum_{i = 1}^{n}p_i\zeta^i(\mathbfit{z}_j) = p_j.
\end{equation*}
Therefore, any dual vector with respect to basis $z$ can be written as
\begin{equation*}
    \phi = \sum_{i = 1}^{n}\phi(\mathbfit{z}_i)\zeta^i.
\end{equation*}
Recall that in Definition \ref{dfn:canonicalBasis}, we defined the canonical basis of $\mathcal{F}^n$ to be the set of unit vectors with a single non-zero entry. Similarly, we can define the canonical basis for the dual space $\hat{\mathcal{F}^n} \coloneqq \mathcal{L}(\mathcal{F}^n, \mathcal{F})$.
\begin{dfnbox}{Canonical Dual Basis}{canonicalDualBasis}
    Write each $\mathbfit{v} \in \mathcal{F}^n$ as
    \begin{equation*}
        \mathbfit{v} = \begin{bmatrix}
            v_1 \\
            v_2 \\
            \vdots \\
            v_n
        \end{bmatrix}.
    \end{equation*}
    The {\color{red} \textbf{canonical dual basis}} of $\mathcal{F}^n$ is defined as $\left\{\epsilon^i \colon i = 1, 2, \cdots, n\right\}$ such that $\epsilon^i(\mathbfit{v}) = v_i$ for all $\mathbfit{v} \in \mathcal{F}^n$.
\end{dfnbox}
Consider any $p \in \hat{\mathcal{F}^n}$, then we can write
\begin{equation*}
    p = \sum_{i = 1}^{n}q_i\epsilon^i
\end{equation*} 
where $q_i \in \mathcal{F}$ for $i = 1, 2, \cdots, n$ and $\epsilon^i$'s are the dual canonical basis. Recall that a basis of an $n$-dimensional vector space $V$ is actually a mapping from $\mathcal{F}^n$ to $V$, so it follows that the dual basis is in fact a mapping $\zeta \colon \hat{\mathcal{F}^n} \to \hat{V}$. We claim that this $\zeta$ is in fact just a mapping that satisfies 
\begin{equation*}
    \zeta(p)\bigl(z(\mathbfit{a})\bigr) = p(\mathbfit{a})
\end{equation*}
for any $p \in \hat{\mathcal{F}^n}$ and $\mathbfit{a} \in \mathcal{F}^n$. 

To prove that our new definition is consistent with Definition \ref{dfn:dualBasis}, it suffices to show that $\zeta(\epsilon^i) = \zeta^i$. Notice that
\begin{align*}
    \zeta(\epsilon^i)\bigl(z(\mathbfit{e}_j)\bigr) & = \epsilon^i(\mathbfit{e}_j) \\
    & = \begin{cases}
        1 & \textrm{if } i = j \\
        0 & \textrm{otherwise}
    \end{cases}.
\end{align*}
However, $z(\mathbfit{e}_j) = \mathbfit{z}_j$, so by Definition \ref{dfn:dualBasis} we have $\zeta(\epsilon^i)(\mathbfit{z}_j) = \zeta^i(\mathbfit{z}_j)$. Let $\mathbfit{v} \in V$ be any vector with
\begin{equation*}
    \mathbfit{v} = \sum_{i = 1}^{n}a_i\mathbfit{z}_i,
\end{equation*}
then 
\begin{align*}
    \zeta(\epsilon^i)(\mathbfit{v}) & = \zeta(\epsilon^i)\left(\sum_{j = 1}^{n}a_j\mathbfit{z}_j\right) \\
    & = \sum_{j = 1}^{n}a_j\zeta(\epsilon^i)(\mathbfit{z}_j) \\
    & = \sum_{j = 1}^{n}a_j\zeta^i(\mathbfit{z}_j) \\
    & = a_i \\
    & = \zeta^i(\mathbfit{v}).
\end{align*}
Therefore, $\zeta(\epsilon^i) = \zeta^i$. Let $\phi \in \hat{V}$ be a mapping, then
\begin{align*}
    \phi & = \sum_{i = 1}^{n}a_i\zeta^i \\
    & = \sum_{i = 1}^{n}a_i\zeta(\epsilon^i) \\
    & = \zeta\left(\sum_{i = 1}^{n}a_i\epsilon^i\right) \\
    & = \zeta(p)
\end{align*}
for some $p \in \hat{\mathcal{F}^n}$. Therefore, $\zeta$ is surjective. One may check that $\zeta$ is injective. Moreover, note that for $p, q \in \hat{\mathcal{F}^n}$,
\begin{align*}
    \bigl(m\zeta(p) + n\zeta(q)\bigr)\bigl(z(\mathbfit{a})\bigr) & = m\zeta(p)\bigl(z(\mathbfit{a})\bigr) + n\zeta(q)\bigl(z(\mathbfit{a})\bigr) \\
    & = mp(\mathbfit{a}) + nq(\mathbfit{a}) \\
    & = (mp + nq)(\mathbfit{a}) \\
    & = \zeta(mp + nq)\bigl(z(\mathbfit{a})\bigr).
\end{align*}
Therefore, $\zeta$ is a linear mapping and so it is an isomorphism. Indeed, this implies that $\zeta$ is a basis for $\hat{V}$.

Naturally, it feels justified to define a ``dual space of the dual space'' of $V$, namely 
\begin{equation*}
    \hat{\hat{V}} \coloneqq \mathcal{L}\left(\hat{V}, \mathcal{F}\right) = \mathcal{L}\bigl(\mathcal{L}(V, \mathcal{F}), \mathcal{F}\bigr).
\end{equation*}
We can in fact prove that this space is isomorphic to $V$ itself. Therefore, this means that it is perfectly legal to view each $\mathbfit{v} \in V$ as a mapping from $\hat{V}$ to $\mathcal{F}$. We can verify that $\mathbfit{v}$ is linear, because
\begin{align*}
    \mathbfit{v}(m\alpha + n\beta) & = m\alpha(\mathbfit{v}) + n\beta(\mathbfit{v}) \\
    & = m\mathbfit{v}(\alpha) + n\mathbfit{v}(\beta).
\end{align*}
Therefore, we can in some sense view a vector space and its dual space the \textit{dual} of each other. Consider a mapping $T \colon V \to V$ for some vector space $V$, the duality between $V$ and~$\hat{V}$ tempts us to think that there exists some mapping $S \colon \hat{V} \colon \hat{V}$ with a certain correspondence to $T$.
\begin{dfnbox}{Transpose}{tranpose}
    Let $T \colon V \to V$ be a linear transformation. The {\color{red} \textbf{transpose}} of $T$ is defined as the mapping~$\hat{T} \colon \hat{V} \to \hat{V}$ such that 
    \begin{equation*}
        \hat{T}(\alpha)(\mathbfit{v}) = \alpha\bigl(T(\mathbfit{v})\bigr)
    \end{equation*}
    for any $\alpha \in \hat{V}$. 
\end{dfnbox}
\section{Tensor Product}
\begin{dfnbox}{Tensor Product}{tensorProd}
    Let $V$ be a vector space with the dual space $\hat{V}$. For any $\mathbfit{v} \in V$ and $\alpha \in \hat{V}$, their {\color{red} \textbf{tensor product}} is defined as a mapping $\mathbfit{v} \otimes \alpha \in \mathcal{L}(V, V)$ such that
    \begin{equation*}
        (\mathbfit{v} \otimes \alpha)(\mathbfit{u}) = \alpha(\mathbfit{u})\mathbfit{v}.
    \end{equation*}
\end{dfnbox}
We can see that $\mathbfit{v} \otimes \alpha$ is linear with respect to both $\mathbfit{v}$ and $\alpha$, because
\begin{align*}
    \bigl((\mathbfit{u + v}) \otimes \alpha\bigr)(\mathbfit{w}) & = \alpha(\mathbfit{w})(\mathbfit{u + v}) = \alpha(\mathbfit{w})\mathbfit{u} + \alpha(\mathbfit{w})\mathbfit{v} = (\mathbfit{u} \otimes \alpha + \mathbfit{v} \otimes \alpha)(\mathbfit{v}), \\
    \bigl(\mathbfit{v} \otimes (\alpha + \beta)\bigr)(\mathbfit{w}) & = (\alpha + \beta)(\mathbfit{w})\mathbfit{v} = \alpha(\mathbfit{w})\mathbfit{v} + \beta(\mathbfit{w})\mathbfit{v} = (\mathbfit{v} \otimes \alpha + \mathbfit{v} \otimes \beta)(\mathbfit{w}).
\end{align*}
In particular, let $\beta \in \hat{V}$, then clearly $\beta \circ (\mathbfit{v} \otimes \alpha) \in \hat{V}$. Define a mapping $T$ over $\hat{V}$ by
\begin{equation*}
    T(\beta) = \beta \circ (\mathbfit{v} \otimes \alpha),
\end{equation*}
then we can see that $T \in \mathcal{L}\left(\hat{V}, \hat{V}\right)$. In fact, for any $\mathbfit{u} \in V$, consider
\begin{align*}
    T(\beta)(\mathbfit{u}) & = \bigl(\beta \circ (\mathbfit{v} \otimes \alpha)\bigr)(\mathbfit{u}) \\
    & = \beta\bigl((\mathbfit{v} \otimes \alpha)(\mathbfit{u})\bigr).
\end{align*}
By Definition \ref{dfn:tranpose}, $T$ is the transpose of $\mathbfit{v} \otimes \alpha$ for any $\mathbfit{v} \in V$ and $\alpha \in \hat{V}$. Furthermore, note that $\alpha(\mathbfit{u}) \in \mathcal{F}$, we see that
\begin{align*}
    T(\beta)(\mathbfit{u}) & = \beta\bigl((\mathbfit{v} \otimes \alpha)(\mathbfit{u})\bigr) \\
    & = \beta\bigl(\alpha(\mathbfit{u})\mathbfit{v}\bigr) \\
    & = \alpha(\mathbfit{u})\beta(\mathbfit{v}) \\
    & = \bigl(\beta(\mathbfit{v})\alpha\bigr)(\mathbfit{u}).
\end{align*}
Therefore, actually $\beta \circ (\mathbfit{v} \otimes \alpha) = \beta(\mathbfit{v})\alpha$.
\begin{probox}{Tensor Products Form A Basis For $\mathcal{L}(V, V)$}{tenserProdBasis}
    Let $\zeta$ be the dual basis for $V$ with respect to some basis $z$, then 
    \begin{equation*}
        \left\{\mathbfit{z}_i \otimes \zeta^j \colon i, j = 1, 2, \cdots, n\right\}
    \end{equation*}
    is a basis for $\mathcal{L}(V, V)$.
    \tcblower
    \begin{proof}
        Let $T \in \mathcal{L}(V, V)$. For each $i = 1, 2, \cdots, n$, we have
        \begin{equation*}
            T(\mathbfit{z}_i) = \sum_{j = 1}^{n}a_{ij}\mathbfit{z}_j.
        \end{equation*}
        Define a mapping
        \begin{equation*}
            S \coloneqq \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}(\mathbfit{z}_j \otimes \zeta^i).
        \end{equation*}
        Note that $S \in \mathcal{L}(V, V)$. For any $\mathbfit{z}_k$, we have
        \begin{align*}
            S(\mathbfit{z}_k) & = \left(\sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}(\mathbfit{z}_j \otimes \zeta^i)\right)(\mathbfit{z}_k) \\
            & = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}\zeta^i(\mathbfit{z}_k)\mathbfit{z}_j \\
            & = \sum_{j = 1}^{n}a_{kj}\mathbfit{z}_j \\
            & = T(\mathbfit{z}_k).
        \end{align*}
        Therefore, 
        \begin{equation*}
            T = S = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}(\mathbfit{z}_j \otimes \zeta^i),
        \end{equation*} 
        so $\mathcal{L}(V, V) = \mathrm{span}\left\{\mathbfit{z}_i \otimes \zeta^j \colon i, j = 1, 2, \cdots, n\right\}$. Suppose $T = 0_{\mathcal{L}}$ is the zero mapping, then for each $\mathbfit{z}_k$ with $k = 1, 2, \cdots, n$, we have $S(\mathbfit{z}_k) = T(\mathbfit{z}_k) = \zero$. This implies that 
        \begin{equation*}
            \sum_{j = 1}^{n}a_{kj}\mathbfit{z}_k = \zero
        \end{equation*}
        for each $k = 1, 2, \cdots, n$. Since $z$ is a basis, we have $a_{kj} = 0$ for all $k, j = 1, 2, \cdots, n$. Therefore, the set $\left\{\mathbfit{z}_i \otimes \zeta^j \colon i, j = 1, 2, \cdots, n\right\}$
        is linearly independent and so is a basis for $\mathcal{L}(V, V)$.
    \end{proof}
\end{probox}
Therefore, we see that for every vector space $V$, if we fix any basis $\left\{\mathbfit{z}_i \colon i = 1, 2, \cdots, n\right\}$, then any $T \in \mathcal{L}(V, V)$ can be expressed as a linear combination of $\mathbfit{z}_i \otimes \zeta^j$ where $\zeta$ is the dual basis with respect to $z$. Proposition \ref{pro:tenserProdBasis} also implies that for any finite-dimensional vector space $V$, we have $\dim\bigl(\mathcal{L}(V, V)\bigr) = \dim(V)^2$.

Let $T \in \mathcal{L}(V, V)$. Fix a basic vector $\mathbfit{z}_j$ of $V$ with dual vector $\zeta^i$, we can ``extract'' the $(i, j)$ component of $T$ by
\begin{equation*}
    \zeta^i\bigl(T(\mathbfit{z}_j)\bigr) = \zeta^i\left(\sum_{k = 1}^{n}a_{kj}\mathbfit{z}_k\right) = \sum_{k = 1}^{n}a_{kj}\zeta^i(\mathbfit{z}_k) = a_{ij}.
\end{equation*}
\section{Matrix}
\begin{dfnbox}{Matrix}{matrix}
    An $n \times m$ {\color{red} \textbf{matrix}} is defined as an $m$-tuple of column vectors in $\mathcal{F}^n$.
\end{dfnbox}
Clearly, the set of all $n \times m$ matrices for any $m, n \in \N$ is a vector space. We denote this vector space as $\mathcal{M}_{n \times m}$. However, as compared to other vectors, we can also define the notion of multiplication between some matrices.
\begin{dfnbox}{Matrix Multiplication}{matMult}
    Let $\mathbfit{M}$ be an $n \times m$ matrix and $\mathbfit{N}$ be an $m \times p$ matrix, then their product $\mathbfit{P = MN}$ is a matrix whose entries are given by
    \begin{equation*}
        P^i_j = \sum_{k = 1}^{m}M^i_kN^k_i.
    \end{equation*}
\end{dfnbox}
Consider the vector space $\mathcal{M}_{n \times n}$ of square matrices. It is easy to see that this space is \textbf{closed under multiplication.} 
\begin{dfnbox}{Algebra}{algebra}
    An {\color{red} \textbf{algebra}} is a vector space $V$ with a binary mapping $\times \colon V^2 \to V$ known as multiplication.
\end{dfnbox}
It is not difficult to see that from Proposition \ref{pro:tenserProdBasis}, the components of a mapping $T \in \mathcal{L}(V, V)$ can be essentially seen as the entries of a matrix.
\begin{dfnbox}{Matrix of A Linear Transformation}{linTransMat}
    Let $T \in \mathcal{L}(V, V)$. If for some basis $z$ of $V$, we have
    \begin{equation*}
        T = \sum_{i = 1}^{n}\sum_{j = 1}^{n}a_{ij}\mathbfit{z}_i \otimes \zeta^j,
    \end{equation*}
    then the matrix $\mathbfit{T}$ with $T^i_j = a_{ij}$ is called the matrix of $T$ relative to $z$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $T^i_j = \zeta^i\bigl(T(\mathbfit{z}_j)\bigr)$.
    \end{remark}
\end{notebox}
Let $\hat{T}$ be the transpose of $T$, then by Definition \ref{dfn:tranpose} we have
\begin{equation*}
    \zeta^i\bigl(T(\mathbfit{z}_j)\bigr) = \hat{T}(\zeta^i)(\mathbfit{z}_j) = z_j\bigl(\hat{T}(\zeta^i)\bigr),
\end{equation*}
which we can use to verify that the matrix of $\hat{T}$ is indeed $\mathbfit{T}^{\mathrm{T}}$.

Let $V$ be a finite-dimensional vector space with $\dim(V) = n$ and let $\mathcal{M}_{n \times n}$ be the vector space of all $n \times n$ matrices. Define $M_z \colon \mathcal{L}(V, V) \to \mathcal{M}_{n \times n}$ that maps each $T \in \mathcal{L}(V, V)$ to its matrix representation $\mathbfit{T}$. Intuitively, it is an isomorphism.

\begin{notebox}
    \begin{remark}
        Note that actually $\mathcal{M}_{n \times n} \cong \mathcal{F}^{n^2}$ and that a basis for $\mathcal{L}(V, V)$ is just an isomorphism $z \colon \mathcal{F}^{n^2} \to \mathcal{L}(V, V)$, so in some sense $M_z$ is just $z^{-1}$.
    \end{remark}
\end{notebox}

Consider $\mathcal{L}(\mathcal{F}^n, \mathcal{F}^n)$, it is clear that every $T \in \mathcal{L}(\mathcal{F}^n, \mathcal{F}^n)$ can be expressed as
\begin{equation*}
    T = \sum_{i = 1}^{n}\sum_{j = 1}^{n}M_e(T)_i^j\left(\mathbfit{e}_i \otimes \epsilon^i\right).
\end{equation*}
When the context is clear, we can be a bit sloppy and regard $M_z$ just as $\mathbfit{T}$.

We can augment $\mathcal{L}(V, V)$ with the $\circ$ operation. For any $S, T \in \mathcal{L}(V, V)$, clearly $S \circ T \in \mathcal{L}(V, V)$, so $\mathcal{L}(V, V)$ is an algebra. This means that it is justified to write $S \circ T$ as a product $ST$.

Let $\mathbfit{u}, \mathbfit{v} \in V$ and $\alpha, \beta \in \hat{V}$, for any $\mathbfit{w} \in V$, consider
\begin{align*}
    \bigl((\mathbfit{u} \otimes \alpha) \circ (\mathbfit{v} \otimes \beta)\bigr)(\mathbfit{w}) & = (\mathbfit{u} \otimes \alpha)\bigl(\beta(\mathbfit{w})\mathbfit{v}\bigr) \\
    & = \alpha\bigl(\beta(\mathbfit{w})\mathbfit{v}\bigr)\mathbfit{u} \\
    & = \beta(\mathbfit{w})\alpha(\mathbfit{v})\mathbfit{u} \\
    & = \bigl(\alpha(\mathbfit{v})\mathbfit{u} \otimes \beta\bigr)(\mathbfit{w}).
\end{align*}
Therefore, $(\mathbfit{u} \otimes \alpha) \circ (\mathbfit{v} \otimes \beta) = \alpha(\mathbfit{v})\mathbfit{u} \otimes \beta$.

Intuitively, the matrix of a composite map can be obtained via matrix multiplication.
\begin{probox}{Matrices of Composite Mappings}{matComp}
    Let $S, T \in \mathcal{L}(V, V)$, then for any basis $z$ of $V$, 
    \begin{equation*}
        M_z(ST) = M_z(S)M_z(T).
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $\zeta^i$ be the dual vector of $\mathbfit{z}_h$. By Definition \ref{dfn:linTransMat}, 
        \begin{equation*}
            S = \sum_{i = 1}^{n}\sum_{h = 1}^{n}S^h_i(\mathbfit{z}_h \otimes \zeta^i), \qquad T = \sum_{k = 1}^{n}\sum_{j = 1}^{n}T^j_k(\mathbfit{z}_j \otimes \zeta^k).
        \end{equation*}
        Therefore,
        \begin{align*}
            ST & = \left(\sum_{i = 1}^{n}\sum_{h = 1}^{n}S^h_i(\mathbfit{z}_h \otimes \zeta^i)\right)\left(\sum_{k = 1}^{n}\sum_{j = 1}^{n}T^j_k(\mathbfit{z}_j \otimes \zeta^k)\right) \\
            & = \sum_{i = 1}^{n}\sum_{h = 1}^{n}\sum_{k = 1}^{n}\sum_{j = 1}^{n}S^h_iT^j_k(\mathbfit{z}_h \otimes \zeta^i)(\mathbfit{z}_j \otimes \zeta^k) \\
            & = \sum_{i = 1}^{n}\sum_{h = 1}^{n}\sum_{k = 1}^{n}\sum_{j = 1}^{n}S^h_iT^j_k\zeta^i(\mathbfit{z}_j)\mathbfit{z}_h \otimes \zeta^k \\
            & = \sum_{i = 1}^{n}\sum_{h = 1}^{n}\sum_{k = 1}^{n}S^h_iT^i_k\mathbfit{z}_h \otimes \zeta^k \\
            & = \sum_{h = 1}^{n}\sum_{k = 1}^{n}\bigl(M_z(S)M_z(T)\bigr)^h_k\mathbfit{z}_h \otimes \zeta^k.
        \end{align*}
        Therefore, $M_z(ST) = M_z(S)M_z(T)$.
    \end{proof}
\end{probox}
\section{Change of Basis}\label{sectionChangeBasis}
Let $z$ be a basis for a vector space $V$. For any $\mathbfit{v} \in V$, define $\mathbfit{v}^*_z \coloneqq z^{-1}(\mathbfit{v})$. Clearly $\mathbfit{v}^*_z \in \mathcal{F}^n$. We can view $\mathbfit{v}^*_z$ as the ``coordinates'' of $\mathbfit{v}$ with respect to $z$.

Suppose $\alpha \in \hat{V}$, we define some $\alpha^*_z \in \hat{\mathcal{F}^n}$ by $\alpha^*_z \coloneqq \alpha \circ z$. In particular, suppose the $i$-th component of $\alpha$ is $\alpha_i$, we notice that
\begin{equation*}
    \alpha^*_z(\mathbfit{e}_i) = \alpha\bigl(z(\mathbfit{e}_i)\bigr) = \alpha(\mathbfit{z}_i) = \alpha_i.
\end{equation*}
So $\alpha^*_z$ and $\alpha$ can be represented with the same row vector. Similarly, consider $T \in \mathcal{L}(V, V)$. Fix any basis $z$ of $V$. Define $T^*_z \coloneqq z^{-1} \circ T \circ z$. We claim that $M_z(T^*_z) = M_z(T)$, because
\begin{align*}
    M_z(T^*_z)^i_j & = \epsilon^i\bigl(T^*_z(\mathbfit{e}_j)\bigr) \\
    & = \epsilon^i\bigl((z^{-1} \circ T \circ z)(\mathbfit{e}_j)\bigr) \\
    & = \epsilon^i\Bigl(z^{-1}\bigl(T(\mathbfit{z}_j)\bigr)\Bigr) \\
    & = \epsilon^i \circ z^{-1}\left(\sum_{k = 1}^{n}T^k_j\mathbfit{z}_k\right) \\
    & = \epsilon^i\left(\sum_{k = 1}^{n}T^k_j\bigl(z^{-1}(\mathbfit{z}_k)\bigr)\right) \\
    & = \left(\sum_{k = 1}^{n}T^k_j\epsilon^i(\mathbfit{e}_k)\right) \\
    & = T^i_j.
\end{align*}
Note that if $\mathbfit{u} = T(\mathbfit{v})$ and $z(\mathbfit{a}) = \mathbfit{v}$, $z(\mathbfit{b}) = \mathbfit{u}$, then 
\begin{equation*}
    \mathbfit{b} = \left(z^{-1} \circ T \circ z\right)(\mathbfit{a}).
\end{equation*}

Let $z, y$ be bases for some finite-dimensional vector space $V$. Define a mapping
\begin{equation*}
    P \coloneqq z^{-1} \circ y \in \mathcal{L}\left(\mathcal{F}^n, \mathcal{F}^n\right),
\end{equation*}
then if $\mathbfit{v} \in V$ is such that $\mathbfit{v} = z(\mathbfit{a}) = y(\mathbfit{b})$, clearly $\mathbfit{a} = P(\mathbfit{b})$. Note that $y = z \circ P$, so
\begin{align*}
    \mathbfit{y}_i & = y(\mathbfit{e}_i) \\
    & = z\bigl(P(\mathbfit{e}_i)\bigr) \\
    & = \sum_{k = 1}^{n}M_z(P)^k_i\mathbfit{z}_k.
\end{align*} 
This means that we can obtain the $i$-th column of $M_z(P)$ by expressing $\mathbfit{y}_i$ in terms of the $\mathbfit{z}_k$'s and fill the coefficients into the column.

Let $\mathbfit{v} \in V$ be a vector with coordinate vector $\mathbfit{v}_y^* \in \mathcal{F}^n$ such that $y(\mathbfit{v}_y^*) = \mathbfit{v}$, then 
\begin{equation*}
    \mathbfit{v}^*_y = y^{-1}(\mathbfit{v}) = \left(y^{-1} \circ z\right)\left(z^{-1}(\mathbfit{v})\right) = P^{-1}\left(\mathbfit{v}^*_z\right).
\end{equation*}
Combining everything, we have 
\begin{dfnbox}{Change of Basis Matrix}{changeBasis}
    Let $\mathbfit{T}_z^*$ be the matrix of a linear transformation $T \colon V \to V$ with respect to the basis $z$ and let $\mathbfit{P}$ be the matrix of the mapping $z^{-1} \circ y$ for some basis $y$ for $V$, then the matrix of $T$ with respect to $y$ is 
    \begin{equation*}
        \mathbfit{T}^*_y \coloneqq \mathbfit{P}^{-1}\mathbfit{T}_z^*\mathbfit{P}.
    \end{equation*}
    $\mathbfit{P}$ is called a {\color{red} \textbf{change of basis matrix}}.
\end{dfnbox}
\section{Classifying Linear Transformations}
Since there are infinitely many linear transformations between two vectors spaces, it is not of much help to study each one of them individually. Instead, it is useful to consider categorising all linear transformations into different families according to some well-defined properties.
\begin{dfnbox}{Operator}{operator}
    A linear transformation $T \colon V \to V$ is called an {\color{red} \textbf{operator}} on $V$.
\end{dfnbox}
Let us recall the notion of eigenvectors. In the old-fashioned way, we define a non-zero vector $\mathbfit{v}$ to be an eigenvector if there is some scalar $\lambda$ such that $\mathbfit{Av} = \lambda\mathbfit{v}$. We can see that the left-multiplication by $\mathbfit{A}$ actually corresponds to a linear transformation from a vector space to itself, so we can re-write this definition in terms of linear transformations.
\begin{dfnbox}{Eigenvector and Eigenvalue}{eigenvec}
    Let $T \colon V \to V$ be an operator on a finite-dimensional vector space $V$ over $\mathcal{F}$. A vector~$\mathbfit{v} \in V$ is called an {\color{red} \textbf{eigenvector}} of $T$ if $T(\mathbfit{v}) = \lambda\mathbfit{v}$ for some $\lambda \in \mathcal{F}$, which is known as the {\color{red} \textbf{eigenvalue}} associated with $\mathbfit{v}$.
\end{dfnbox}
Note that the operator $T$ is a well-defined function, so for every eigenvector, its associated eigenvalue is unique. The uniqueness of eigenvalues guarantees the following contrapositive statement:
\begin{probox}{Uniqueness of Eigenvalue}{uniqueEigenval}
    If $\mathbfit{u}, \mathbfit{v}$ are eigenvectors of an operator $T \colon V \to V$ associated to eigenvalues~$\lambda \neq \mu$ respectively, then $\mathbfit{u}$ and $\mathbfit{v}$ are linearly independent.
    \tcblower
    \begin{proof}
        We shall prove the contrapositive statement. Suppose conversely that $\mathbfit{u}$ and $\mathbfit{v}$ are linearly dependent, then there exists some scalar $\alpha$ such that $\mathbfit{v} = \alpha\mathbfit{u}$. Note that 
        \begin{equation*}
            \mu\mathbfit{v} = T(\mathbfit{v}) = T(\alpha\mathbfit{u}) = \alpha\lambda\mathbfit{u} = \lambda\mathbfit{v},
        \end{equation*}
        so $\mu = \lambda$.
    \end{proof}
\end{probox}
However, the converse is not true, because if $T(\mathbfit{v}) = \lambda\mathbfit{v}$, then $T(\mu\mathbfit{v}) = \lambda(\mu\mathbfit{v})$ for all $\mu \in \mathcal{F}$. In fact, two linearly independent vectors can have the same eigenvalue. A silly example: for any finite-dimensional space $V$, consider the identity operator $\mathrm{id}_V$. Clearly, every vector in $V$ is an eigenvector with the associated eigenvalue of~$1$.
\begin{dfnbox}{Eigenspace}{eigenspace}
    Let $T$ be an operator on a finite-dimensional vector space $V$ over $\mathcal{F}$. For every eigenvalue $\lambda \in \mathcal{F}$ associated to some vector in $V$, the {\color{red} \textbf{eigenspace}} associated with $\lambda$ is 
    \begin{equation*}
        E_\lambda \coloneqq \left\{\mathbfit{v} \in V \colon T(\mathbfit{v}) = \lambda\mathbfit{v}\right\}.
    \end{equation*}
\end{dfnbox}
One can easily verify that every eigenspace is a subspace. Intuitively, eigenspaces associated to different eigenvalues should be different. To see this more clearly, consider the following proposition, which is a generalisation of Proposition \ref{pro:uniqueEigenval}:
\begin{probox}{Linear Independence between Eigenspaces}{liEigenspace}
    Let $T \colon V \to V$ be an operator with distinct eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_k$ associated with eigenvectors $\mathbfit{v}_1, \mathbfit{v}_2, \cdots, \mathbfit{v}_k$ respectively, then $\mathbfit{v}_1, \mathbfit{v}_2, \cdots, \mathbfit{v}_k$ are linearly independent.
    \tcblower
    \begin{proof}
        The case where $k = 2$ is exactly Proposition \ref{pro:uniqueEigenval}. Suppose that there is some $m \in \Z^+$ with $m \geq 2$ such that eigenvectors $\mathbfit{v}_1, \mathbfit{v}_2, \cdots, \mathbfit{v}_m$ associated with distinct eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_m$ respectively are linearly independent. Let $\lambda_{m + 1}$ be an eigenvalue associated to $\mathbfit{v}_{m + 1}$, we shall prove that $\mathbfit{v}_1, \mathbfit{v}_2, \cdots, \mathbfit{v}_{m + 1}$ are linearly independent by considering the contrapositive statement. Suppose conversely that $\mathbfit{v}_1, \mathbfit{v}_2, \cdots, \mathbfit{v}_{m + 1}$ are linearly dependent vectors. Since $\mathbfit{v}_{m + 1} \neq \zero$, there exist scalars $\alpha_1, \alpha_2, \cdots, \alpha_m$ not all zero such that 
        \begin{equation*}
            \mathbfit{v}_1 = \sum_{i = 1}^{m}\alpha_i\mathbfit{v}_i.
        \end{equation*}
        Therefore, 
        \begin{align*}
            T(\mathbfit{v}_{m + 1}) & = T\left(\sum_{i = 1}^{m}\alpha_i\mathbfit{v}_i\right) \\
            & = \sum_{i = 1}^{m}\alpha_iT(\mathbfit{v}_i) \\
            & = \sum_{i = 1}^{m}\alpha_i\lambda_i\mathbfit{v}_i,
        \end{align*}
        but $T(\mathbfit{v}_{m + 1}) = \lambda_{m + 1}\mathbfit{v}_{m + 1} = \lambda_{m + 1}\sum_{i = 1}^{m}\alpha_i\mathbfit{v}_i$, so
        \begin{equation*}
            \sum_{i = 1}^m\alpha_i(\lambda_i - \lambda_{m + 1})\mathbfit{v}_i = \zero.
        \end{equation*}
        Since $\mathbfit{v}_1, \mathbfit{v}_2, \cdots, \mathbfit{v}_{m}$ are linearly independent, this implies that $\alpha_i(\lambda_i - \lambda_{m + 1}) = 0$ for all $i = 1, 2, \cdots, m$. Note that there exists some integer $j \in [1, m]$ such that $\alpha_j \neq 0$, so $\lambda_j = \lambda_{m + 1}$, which means that $\lambda_1, \lambda_2, \cdots, \lambda_{m + 1}$ are not distinct eigenvalues.
    \end{proof}
\end{probox}
Proposition \ref{pro:liEigenspace} reveals an important fact that any finite-dimensional vector space has finitely many eigenvalues. In fact, for any $V \cong \mathcal{F}^n$, any linearly independent set contains at most $n$ distinct vectors, so it means that $V$ can have at most $\dim(V)$ different eigenvalues.

The next natural question is: given any operator on a finite-dimensional vector space, does the operator always has an eigenvector? By instinct, one may answer ``no'' because it seems that the rotation transformation in $\R^2$, namely
\begin{equation*}
    (x, y) \mapsto (x\cos\theta - y\sin\theta, x\sin\theta + y\cos\theta),
\end{equation*}
does not have any eigenvector because there is no vector in $\R^2$ being mapped to a parallel image. However, we notice something interesting when we extend the domain of the rotation transformation to $\C^2$, because
\begin{equation*}
    \begin{bmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
    \end{bmatrix}\begin{bmatrix}
        1 \\
        -i
    \end{bmatrix} = \begin{bmatrix}
        \cos\theta + i\sin\theta \\
        -i(\cos\theta + i\sin\theta)
    \end{bmatrix} = \begin{bmatrix}
        \mathrm{e}^{i\theta} \\
        -i\mathrm{e}^{i\theta}
    \end{bmatrix} = \mathrm{e}^{i\theta}\begin{bmatrix}
        1 \\
        -i
    \end{bmatrix}.
\end{equation*}
Therefore, it is important to note that the eigenvalues and eigenvectors of a real vector space might not be real! The following proposition states this result rigorously:
\begin{probox}{Existence of Eigenvalue}{eigenvalExists}
    Every operator on a finite-dimensional vector space over $\C$ has at least one eigenvalue.
    \tcblower
    \begin{proof}
        Let $V$ be an $n$-dimensional vector space over $\C$ and let $T \colon V \to V$ be an operator. Take any non-zero vector $\mathbfit{v} \in V$, then $\mathbfit{v}, T(\mathbfit{v}), T^2(\mathbfit{v}), \cdots, T^n(\mathbfit{v})$ are $(n + 1)$ vectors in $V$ which must be linearly dependent. Therefore, there exists $c_0, c_1, \cdots, c_n \in \C$ not all zero such that 
        \begin{equation*}
            \sum_{i = 0}^{n}c_iT^i(\mathbfit{v}) = \zero.
        \end{equation*}
        This implies that $\sum_{i = 0}^{n}c_iT^i$ is the zero transformation, i.e., $\sum_{i = 0}^{n}c_iT^i = 0$. Note that this is a polynomial equation with complex coefficients, so it has exactly $(n + 1)$ roots. Therefore, 
        \begin{equation*}
            \sum_{i = 0}^{n}c_iT^i(\mathbfit{v}) = a_0\left(\prod_{i = 1}^{n}(T - a_i\mathrm{id}_V)\right)(\mathbfit{v}) = \zero
        \end{equation*}
        for some $a_0, a_1, \cdots, a_n \in \C$. Note that not all of the $(T - a_i\mathrm{id}_V)$'s are injective, because otherwise $\mathbfit{v} = \zero$ by Proposition \ref{pro:injectTest}. Therefore, there is some $T - a_j\mathrm{id}_V$ which is not injective. This means that there exists a non-zero vector $\mathbfit{w} \in V$ such that $(T - a_j\mathrm{id}_V)(\mathbfit{w}) = \zero$, but this implies that $T(\mathbfit{w}) = a_j\mathbfit{w}$, so $a_j$ is an eigenvalue of $T$ associated with $\mathbfit{w}$.
    \end{proof}
\end{probox}
\section{Triangularisation}
To better classify operators, we consider a special type of matrices known as \textit{triangular matrices}.
\begin{dfnbox}{Upper Triangular Matrix}{upTri}
    A matrix $\mathbfit{M}$ is called {\color{red} \textbf{upper triangular}} if $M^i_j = 0$ whenever $i > j$.
\end{dfnbox}
We consider the following nice operator on a finite-dimensional vector space $V$: the operator $T \colon V \to V$ is such that there exists a basis $t$ of $V$ such that $T(\mathbfit{t}_i)$ can be expressed as a linear combination of the $\mathbfit{t}_j$'s for all $j \leq i$. Intuitively, if such an operator exists, then its matrix representation with respect to $t$ must be upper-triangular. We will introduce several useful preliminary results.
\begin{dfnbox}{Similar Matrices}{similar}
    Matrices $\mathbfit{A}$ and $\mathbfit{B}$ are {\color{red} \textbf{similar}} if there exists a matrix $\mathbfit{P}$ such that $\mathbfit{A} = \mathbfit{P}^{-1}\mathbfit{B}\mathbfit{P}$.
\end{dfnbox}
Let $\lambda$ be an eigenvalue of $\mathbfit{A}$, then there exists some non-zero vector $\mathbfit{x}$ such that $\mathbfit{Ax} = \lambda\mathbfit{x}$. If $\mathbfit{A} = \mathbfit{P}^{-1}\mathbfit{B}\mathbfit{P}$, this means that 
\begin{equation*}
    \mathbfit{P}^{-1}\mathbfit{B}\mathbfit{P}\mathbfit{x} = \lambda\mathbfit{x},
\end{equation*}
and so 
\begin{equation*}
    \mathbfit{B}(\mathbfit{Px}) = \lambda\mathbfit{Px}.
\end{equation*}
Therefore, $\lambda$ is also an eigenvalue of $\mathbfit{B}$ with associated eigenvector $\mathbfit{Px} \neq \zero$ (because $\mathbfit{P} \neq \zero$). This implies that similar matrices always have the same eigenvalues.

Now, let $T$ be any operator on $V$ and $z \colon \mathcal{F}^n \to V$ be a basis, then we know that the operator $T^*_z \coloneqq z^{-1} \circ T \circ z$ on $\mathcal{F}^n$ has the same eigenvalues as $T$. Notice also that if $T$ satisfies the property that $T(\mathbfit{t}_i)$ can be expressed as a linear combination of the $\mathbfit{t}_j$'s for all $j \leq i$, then so does $T^*_t$. However, as we have discussed before, the matrix representation of $T^*_t$ is upper-triangular. Therefore, we might want to conclude that the matrix representation of any operator on $V$ is similar to some upper-triangular matrix.
\begin{thmbox}{Triangularisability}{triable}
    For every operator $T \colon V \to V$ where $V$ is a finite-dimensional vector space over $\C$, there exists a basis $t$ of $V$ such that $T(\mathbfit{t}_i)$ can be expressed as a linear combination of the $\mathbfit{t}_j$'s for all $j \leq i$.
    \tcblower
    \begin{proof}
        Let $\dim(V) = n$, then it suffices to prove that every complex-valued $n \times n$ matrix is similar to some upper-triangular matrix. 
        \\\\
        The case where $n = 1$ is trivial because every $1 \times 1$ matrix is upper-triangular. Suppose that there is some $k \in \Z^+$ such that every complex-valued $k \times k$ matrix is similar to some upper-triangular matrix. Let $\mathbfit{M}$ be any $(k + 1) \times (k + 1)$ complex-valued matrix. By Proposition \ref{pro:eigenvalExists}, there is some $\lambda \in \C$ which is an eigenvalue of $\mathbfit{M}$ associated to some non-zero $\mathbfit{v} \in \C^{k + 1}$. Note that we can find $\mathbfit{u}_1, \cdots, \mathbfit{u}_k \in \C^{k + 1}$ such that the columns of 
        \begin{equation*}
            \mathbfit{N} = \begin{bmatrix}
                \mathbfit{v} & \mathbfit{u}_1 & \cdots & \mathbfit{u}_k
            \end{bmatrix}
        \end{equation*}
        is linearly independent. Note that $\mathbfit{Mv} = \lambda\mathbfit{v}$, so
        \begin{equation*}
            \mathbfit{N}^{-1}\mathbfit{MN} = \begin{bmatrix}
                \lambda & \mathbfit{\alpha} \\
                \zero & \mathbfit{P}
            \end{bmatrix},
        \end{equation*}
        where $\mathbfit{P}$ is a $k \times k$ complex-valued matrix. By the inductive hypothesis, there exists a matrix $\mathbfit{Q}$ and an upper-triangular matrix $\mathbfit{R}$ such that $\mathbfit{Q}^{-1}\mathbfit{PQ} = \mathbfit{R}$. Consider
        \begin{align*}
            \begin{bmatrix}
                1 & \zero \\
                \zero & \mathbfit{Q}^{-1}
            \end{bmatrix}\begin{bmatrix}
                1 & \zero \\
                \zero & \mathbfit{Q}
            \end{bmatrix} & = \begin{bmatrix}
                1 & \zero \\
                \zero & \mathbfit{I}
            \end{bmatrix},
        \end{align*}
        so $\left[\begin{smallmatrix}
            1 & \zero \\
                \zero & \mathbfit{Q}^{-1}
        \end{smallmatrix}\right] = \left[\begin{smallmatrix}
            1 & \zero \\
                \zero & \mathbfit{Q}
        \end{smallmatrix}\right]^{-1}$. Therefore,
        \begin{align*}
            \begin{bmatrix}
                1 & \zero \\
                \zero & \mathbfit{Q}
            \end{bmatrix}^{-1}\begin{bmatrix}
                \lambda & \mathbfit{\alpha} \\
                \zero & \mathbfit{P}
            \end{bmatrix}\begin{bmatrix}
                1 & \zero \\
                \zero & \mathbfit{Q}
            \end{bmatrix} & = \begin{bmatrix}
                \lambda & \mathbfit{\alpha}\mathbfit{Q}\\
                \zero & \mathbfit{Q}^{-1}\mathbfit{PQ}
            \end{bmatrix} = \begin{bmatrix}
                \lambda & \mathbfit{\alpha}\mathbfit{Q}\\
                \zero & \mathbfit{R}
            \end{bmatrix},
        \end{align*}
        which is upper-triangular. This means that $\mathbfit{M}$ is similar to an upper-triangular matrix.
    \end{proof}
\end{thmbox}
Note that the matrix $\mathbfit{N}$ in the proof for Theorem \ref{thm:triable} is basically the matrix representation of some basis mapping $z$. Let $T$ be an operator with matrix representation $\mathbfit{M}$, then $z^{-1} \circ T \circ z$ is an operator whose matrix representation is upper-triangular. However, in Section \ref{sectionChangeBasis}, we have shown that any operator $T \colon V \to V$ with respect to any basis $z$ has the same matrix representation as the operator $T^*_z \coloneqq z^{-1} \circ T \circ z$ on the coordinate vectors of $V$. Therefore, we can indeed write the matrix representation of any operator as an upper-triangular matrix by choosing an appropriate basis $z$.

Interestingly, we realise that if $T$ is made to have an upper-triangular matrix representation by the method described in Theorem \ref{thm:triable}, then the $(1, 1)$ entry of that matrix will be an eigenvalue of $T$. The following result is trivial:
\begin{probox}{Invertibility of Upper Triangular Matrices}{invUpTri}
    An upper-triangular matrix is invertible if and only if every entry along its diagonal is non-zero.
\end{probox}
To characterise triangularised mapping, we consider the following proposition:
\begin{probox}{Eigenvalues of Upper Triangular Matrices}{upTriEigen}
    Let $\mathbfit{N}$ be an upper-triangular matrix, then the diagonal entries of $\mathbfit{N}$ are its eigenvalues.
    \tcblower
    \begin{proof}
        Let $\lambda$ be any of the diagonal entries of $\mathbfit{N}$ and consider $\mathbfit{N} - \lambda\mathbfit{I}$ which is also upper-triangular. Note that by Proposition \ref{pro:invUpTri}, $\mathbfit{N} - \lambda\mathbfit{I}$ is not invertible, and so there exists some $\mathbfit{v} \neq \zero$ such that $(\mathbfit{N} - \lambda\mathbfit{I})\mathbfit{v} = \zero$. Therefore, $\mathbfit{Nv} = \lambda\mathbfit{v}$ and so $\lambda$ is an eigenvalue of $\mathbfit{N}$.
    \end{proof}
\end{probox}
\section{Diagonalisation}
We have seen in the previous section that every operator has an upper-triangular matrix representation, but can we improve this further?
\begin{dfnbox}{Diagonalisability}{diagable}
    An operator $T$ is said to be {\color{red} \textbf{diagonalisable}} if there is a basis $z$ such that $M_z\left(T\right)$ is diagonal.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Another way of stating the definition is that a matrix is diagonalisable if it is similar to a diagonal matrix.
    \end{remark}
\end{notebox}
By Proposition \ref{pro:upTriEigen}, since a diagonal matrix is upper-triangular, its diagonal entries must be the eigenvalues. Denote the $(i, i)$ entry of the matrix by $\lambda_i$, then there is some basic vector $\mathbfit{u}_i$ such that $T(\mathbfit{u}_i) = \lambda_i\mathbfit{u}_i$. Therefore, it is clear that an operator is diagonalisable if and only if there exists a basis consisting of its eigenvectors.

An easy conclusion is that if a vector space $V$ has $\dim(V)$ distinct eigenvalues, then it has $\dim(V)$ linearly independent eigenvectors which form a basis, so any $V$ with $\dim(V)$ distinct eigenvalues is diagonalisable.

Of course, we know that it is not possible for every operator to be diagonalisable, i.e., not all operators have some basis $z$ such that the operator under $z$ maps every basic vector to a multiple of itself. Hence, we attempt to make a compromise here: that is, can we always find a basis for every operator such that the operator, under this basis, maps every basic vector to a linear combination of at most $2$ distinct basic vectors?
\begin{dfnbox}{Jordan Block}{jordan}
    Let $\mathbfit{J}$ be an $m \times m$ matrix such that 
    \begin{equation*}
        \mathbfit{J}^i_j = \begin{cases}
            1 & \textrm{if } i = j - 1 \\
            0 & \textrm{otherwise}
        \end{cases}.
    \end{equation*}
    A {\color{red} \textbf{Jordan block}} of size $m$ is the matrix $\lambda\mathbfit{I} + \mathbfit{J}$ for any $\lambda \in \C$.
\end{dfnbox}
The reason that we consider the Jordan block is that any Jordan block has only one eigenspace which is one-dimensional.
\begin{probox}{Eigenspace of Jordan Block}{jordanEigen}
    Let $\lambda\mathbfit{I} + \mathbfit{J}$ be a Jordan block, then it has a unique eigenspace which is one-dimensional.
    \tcblower
    \begin{proof}
        By Propositions \ref{pro:invUpTri} and \ref{pro:upTriEigen}, we know that $\lambda$ is the only eigenvalue, so $\lambda\mathbfit{I} + \mathbfit{J}$ has only one eigenspace. Let the eigenspace be $E_\lambda$ and take any eigenvector 
        \begin{equation*}
            \mathbfit{v} = \begin{bmatrix}
                x_1 & x_2 & \cdots & x_m
            \end{bmatrix}^{\mathrm{T}} \in E_\lambda.
        \end{equation*}
        Note that 
        \begin{equation*}
            (\lambda\mathbfit{I} + \mathbfit{J})\mathbfit{v} = \begin{bmatrix}
                \lambda x_1 + x_2 \\
                \lambda x_2 + x_3 \\
                \vdots \\
                \lambda x_{m - 1} + x_m \\
                \lambda x_m
            \end{bmatrix} = \lambda\begin{bmatrix}
                \lambda x_1 \\
                \lambda x_2 \\
                \vdots \\
                \lambda x_{m - 1} \\
                \lambda x_m
            \end{bmatrix},
        \end{equation*}
        so clearly $x_2 = x_3 = \cdots = x_m = 0$. Therefore, $E_\lambda$ is spanned by $\mathbfit{e}_1$ and so is one-dimensional.
    \end{proof}
\end{probox}
We can build up an upper-triangular matrix using Jordan blocks, which by triangularisability gives the following result:
\begin{dfnbox}{Jordan Basis}{jordanBasis}
    Let $T \colon V \to V$ be an operator. A {\color{red} \textbf{Jordan basis}} of $V$ is a basis $z$ such that 
    \begin{equation*}
        M_z(T) = \begin{bmatrix}
            \mathbfit{J}_1 & \zero & \cdots & \zero \\
            \zero & \mathbfit{J}_2 & \cdots & \zero \\
            \vdots & \vdots & \ddots & \vdots \\
            \zero & \zero & \cdots & \mathbfit{J}_m
        \end{bmatrix},
    \end{equation*}
    where $\mathbfit{J}_1, \cdots, \mathbfit{J}_m$ are Jordan blocks.
\end{dfnbox}
It can be shown that every matrix is similar to some Jordan matrix, and so every operator can be represented by a Jordan basis.
\begin{dfnbox}{Jordan Canonical Form}{jordanCanonical}
    Let $j$ be a Jordan basis for some operator $T$, then $M_j(T)$ is called the {\color{red} \textbf{Jordan canonical form}} of $T$.
\end{dfnbox}
Jordan matrices have a few interesting applications. We can relate them to the eigenvalues of a matrix by considering the following definition:
\begin{dfnbox}{Multiplicity}{multiplicity}
    Let $\mathbfit{J}$ be the Jordan form of a matrix $\mathbfit{A}$. For any eigenvalue $\lambda$ of $\mathbfit{A}$, the {\color{red} \textbf{multiplicity}} of $\lambda$ is defined as the sum of the sizes of Jordan blocks corresponding to $\lambda$.
\end{dfnbox}
In other words, we can interpret the multiplicity of $\lambda$ as the number of occurrences of it along the diagonal of the Jordan form. Recall that previously we define the \textit{characteristic polynomial} of a matrix $\mathbfit{A}$ to be $\det(\mathbfit{A} - \lambda\mathbfit{I})$. Now we propose another definition using the Jordan form.
\begin{dfnbox}{Characteristic Polynomial}{charP}
    Let $T$ be a linear operator with eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_p$, each with multiplicity $m_1, m_2, \cdots, m_p$ respectively, then the {\color{red} \textbf{characteristic polynomial}} of $T$ is defined as 
    \begin{equation*}
        \chi_T(x) = \prod_{i = 1}^{p}(x - \lambda_i)^{m_i}.
    \end{equation*}
\end{dfnbox}
Obviously, $\chi_T(\lambda_i) = 0$ for all $i = 1, 2, \cdots, p$, which leads us to the famous theorem below:
\begin{thmbox}{Cayley-Hamilton Theorem}{CayleyHamilton}
    For any linear operator $T$ with characteristic polynomial $\chi_T$, we have $\chi_T(T) = 0$.
\end{thmbox}
\chapter{Linear Algebra and Geometry}
\section{Bilinear Forms}
\begin{dfnbox}{Dot Product}{dotProd}
    Let $\mathcal{F}$ be a field, the {\color{red} \textbf{dot product}} is a binary operation $\cdot \colon \mathcal{F}^n \times \mathcal{F}^n \to \mathcal{F}$ such that 
    \begin{enumerate}
        \item $\mathbfit{u} \cdot \mathbfit{u} \geq 0$ for all $\mathbfit{u} \in \mathcal{F}^n$;
        \item $\mathbfit{u} \cdot \mathbfit{u} = 0$ if and only if $\mathbfit{u} = \zero$;
        \item $\mathbfit{u} \cdot \mathbfit{v} = \mathbfit{v} \cdot \mathbfit{u}$ for all $\mathbfit{u}, \mathbfit{v} \in \mathcal{F}^n$;
        \item $(\alpha\mathbfit{u}_1 + \beta\mathbfit{u}_2) \cdot \mathbfit{v} = \alpha(\mathbfit{u}_1 \cdot \mathbfit{v}) + \beta(\mathbfit{u}_2 \cdot \mathbfit{v})$ and $\mathbfit{u} \cdot (\alpha\mathbfit{v}_1 + \beta\mathbfit{v}_2) = \alpha(\mathbfit{u} \cdot \mathbfit{v}_1) + \beta(\mathbfit{u} \cdot \mathbfit{v}_1)$ for all $\mathbfit{u}, \mathbfit{v} \in \mathcal{F}^n$ and $\alpha, \beta \in \mathcal{F}$.
    \end{enumerate}
\end{dfnbox}
The dot product is an example of what is known as \textit{bilinear forms}.
\begin{dfnbox}{Bilinear Form}{bilinear}
    A {\color{red} \textbf{bilinear form}} on a finite-dimensional space $V$ is defined to be a mapping $b \colon V \times V \to \mathcal{F}$ such that 
    \begin{align*}
        b(\alpha\mathbfit{u}_1 + \beta\mathbfit{u}_2, \mathbfit{v}) & = \alpha b(\mathbfit{u}_1, \mathbfit{v}) + \beta b(\mathbfit{u}_2, \mathbfit{v}), \\
        b(\mathbfit{u}, \alpha\mathbfit{v}_1 + \beta\mathbfit{v}_2) & = \alpha b(\mathbfit{u}, \mathbfit{v}_1) + \beta b(\mathbfit{u}, \mathbfit{v}_1)
    \end{align*}
    for all for all $\mathbfit{u}, \mathbfit{v} \in V$ and $\alpha, \beta \in \mathcal{F}$.
\end{dfnbox}
Let $\mathbb{B}(V)$ denote the set of all bilinear forms over a finite-dimensional vector space $V$, then $\mathbb{B}(V)$ itself is also a vector space with the usual definitions of addition and scalar multiplication over mappings.
\begin{dfnbox}{Tensor Product between Dual Vectors}{dualVecTensor}
    Let $V$ be a finite-dimensional vector space. For any $\alpha, \beta \in \hat{V}$, their {\color{red} \textbf{tensor product}} is defined as the mapping 
    \begin{equation*}
        \alpha \otimes \beta \colon V \times V \to \mathcal{F}
    \end{equation*}
    such that $(\alpha \otimes \beta)(\mathbfit{u}, \mathbfit{v}) = \alpha(\mathbfit{u})\beta(\mathbfit{v})$.
\end{dfnbox}
It is easy to see that for any basic vectors $\zeta^i, \zeta^j$ for $\hat{V}$, the mapping $\zeta^i \otimes \zeta^j$ is a bilinear form over $V$.
\begin{dfnbox}{Inner Product Space}{innerProd}
    An {\color{red} \textbf{inner product space}} is a finite-dimensional vector space $V$ with a bilinear form $g \colon V \times V \to \mathcal{F}$, known as an {\color{red} \textbf{inner product}} on $V$, such that
    \begin{enumerate}
        \item $g(\mathbfit{u}, \mathbfit{u}) \geq 0$ for all $\mathbfit{u} \in V$;
        \item $g(\mathbfit{u}, \mathbfit{u}) = 0$ if and only if $\mathbfit{u} = \zero$;
        \item $g(\mathbfit{u}, \mathbfit{v}) = g(\mathbfit{v}, \mathbfit{u})$ for all $\mathbfit{u}, \mathbfit{v} \in V$.
    \end{enumerate}
\end{dfnbox}
\section{Geometry in Vector Spaces}
\begin{dfnbox}{Length}{len}
    Let $(V, g)$ be an inner product space, then for any $\mathbfit{v} \in V$, its {\color{red} \textbf{length}} is defined as 
    \begin{equation*}
        \abs{\mathbfit{v}} = \sqrt{g(\mathbfit{v}, \mathbfit{v})}.
    \end{equation*}
\end{dfnbox}
\begin{dfnbox}{Orthogonality}{ortho}
    Let $(V, g)$ be an inner product space. $\mathbfit{u}, \mathbfit{v} \in V$ are {\color{red} \textbf{orthogonal}} if $g(\mathbfit{u}, \mathbfit{v}) = 0$.
\end{dfnbox}
\begin{thmbox}{Cauchy-Schwarz Inequality}{CauchySchwarz}
    Let $(V, g)$ be an inner product space, then $g(\mathbfit{u}, \mathbfit{v}) \leq \abs{\mathbfit{u}}\abs{\mathbfit{v}}$ for any $\mathbfit{u}, \mathbfit{v} \in V$.
\end{thmbox}
\begin{corbox}{Triangle Inequality}{triIneq}
    Let $V$ be a finite-dimensional vector space, then for any $\mathbfit{u}, \mathbfit{v} \in V$, $\abs{\mathbfit{u + v}} \leq \abs{\mathbfit{u}} + \abs{\mathbfit{v}}$. 
\end{corbox}
\end{document}