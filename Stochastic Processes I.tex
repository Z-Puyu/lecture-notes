\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{yhmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}
\DeclareSymbolFont{yhlargesymbols}{OMX}{yhex}{m}{n} \DeclareMathAccent{\yhwidehat}{\mathord}{yhlargesymbols}{"62}

\usepackage{scalerel}[2014/03/10]
\usepackage{stackengine}

\renewcommand\widetilde[1]{\ThisStyle{%
  \setbox0=\hbox{$\SavedStyle#1$}%
  \stackengine{1pt-\LMpt}{$\SavedStyle#1$}{%
    \stretchto{\scaleto{\SavedStyle\mkern.2mu\sim}{.5467\wd0}}{.5\ht0}%
%    .2mu is the kern imbalance when clipping white space
%    .5467++++ is \ht/[kerned \wd] aspect ratio for \sim glyph
  }{O}{c}{F}{T}{S}%
}}
\makeatletter
\let\save@mathaccent\mathaccent
\newcommand*\if@single[3]{%
  \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
  \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
  \ifdim\ht0=\ht2 #3\else #2\fi
  }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
  \begingroup
  \def\mathaccent##1##2{%
%Enable nesting of accents:
    \let\mathaccent\save@mathaccent
%If there's more than a single symbol, use the first character instead \left(see below\right):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = \left(italic correction / 3\right) - \left(Breite / 10\right)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
      \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
      \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern \left(-\dimen@\right) if it is >0 or if a superscript follows:
      \let\final@kern#2%
      \ifdim\dimen@<\z@ \let\final@kern1\fi
      \if\final@kern1 \kern-\dimen@\fi
    \else
      \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
  \if#31
    \macc@nested@a\relax111{#1}%
  \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
      \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
  \fi
  \endgroup
}
\makeatother

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\I}{\mathbfit{I}}
\newcommand{\e}{\mathrm{e}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\im}{\mathrm{i}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\begin{document}
\fancyhead[L]{
    Stochastic Processes I
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Probability}
\section{Probability Spaces}
In an elementary level, we have been viewing probability as the quotient between the number of desired outcomes and the number of all possible outcomes. This definition, though intuitive, is not very solid when it comes to an infinite sample space. In this introductory chapter, we would establish the theories of probability using a more modern and rigorous structure.

Suppose we perform an experiment. This experiment might have many possible outcomes, but we are interested in only one or some of them. This leads to the following notions:
\begin{dfnbox}{Sample Space and Events}{sampleEvents}
    A {\color{red} \textbf{sample space}} of some experiment is the set of all possible outcomes of the experiment. An {\color{red} \textbf{event}} is a subset of the sample space.
\end{dfnbox}
In a na\"{i}ve attempt to devise a probability model, if the sample space $S$ is countable, then it suffices to define a \textit{probability mass function} $P \colon S \to \R$ such that $\sum_{\omega \in S}P\left(\omega\right) = 1$. Naturally, the probability for an event $E \subseteq S$ is defined as $P\left(E\right) = \sum_{\omega \in E}P\left(\omega\right)$. This summation is compatible with the infinite case because if we have countably many pairwise disjoint events $E_1, E_2, \cdots$, we can compute 
\begin{equation*}
    P\left(\bigcup_{i = 1}^{\infty}E_i\right) = \sum_{i = 1}^{\infty}P\left(E_i\right) = \lim_{n \to \infty}\sum_{i = 1}^{n}E_i,
\end{equation*}
which is clearly convergent by monotone-convergent theorem.

However, when $S$ is uncountable, this construction leads to weird behaviours. For example, suppose $S$ is the sample space for the experiment of tossing a fair coin for uncountably many times. It is clear that for any $\omega \in S$, we have 
\begin{equation*}
    P\left(\omega\right) = \lim_{n \to \infty}\frac{1}{2^{n}} = 0,
\end{equation*}
but at the same time we must have 
\begin{equation*}
    1 = P\left(S\right) = \sum_{\omega \in S}P\left(\omega\right) = 0,
\end{equation*}
which is ridiculous. Therefore, we need to find a better way to construct the probability model. Notice that here the incompatibility arises because we build our model by considering the probabilities of individual outcomes. Our next attempt try to bypass this issue by considering the probabilities of events only.

Since the set of all events in a sample space $S$ is simply $\mathcal{P}\left(S\right)$, let us instead consider a more generalisable algebraic structure for this collection of subsets.
\begin{dfnbox}{Set Algebra}{setAlgebra}
    Let $X$ be a set. A {\color{red} \textbf{set algebra}} over $X$ is a family $\mathcal{F} \subseteq \mathcal{P}\left(X\right)$ such that 
    \begin{itemize}
        \item $X \backslash F \in \mathcal{F}$ for all $F \in \mathcal{F}$ (closed under complementation);
        \item $X \in \mathcal{F}$;
        \item $X_1 \cup X_2 \in \mathcal{F}$ for any $X_1, X_2 \in \mathcal{F}$ (closed under binary union).
    \end{itemize}
\end{dfnbox}
There are several immediate implications from the above definition. 

First, by closure under complementation, we know that an algebra over any set $X$ must contain the empty set. 

Second, by De Morgan's Law, one can easily check that if the first $2$ axioms hold, the closure under binary union is equivalent to 
\begin{itemize}
    \item $X_1 \cap X_2 \in \mathcal{F}$ for any $X_1, X_2 \in \mathcal{F}$;
    \item $\bigcup_{i = 1}^{n}X_i \in \mathcal{F}$ for any $X_1, X_2, \cdots, X_n \in \mathcal{F}$ for all $n \in \N$;
    \item $\bigcap_{i = 1}^{n}X_i \in \mathcal{F}$ for any $X_1, X_2, \cdots, X_n \in \mathcal{F}$ for all $n \in \N$.
\end{itemize}
$\left(X, \mathcal{F}\right)$ is known as a \textit{field of sets}, where the elements of $X$ are called \textit{points} and those of $\mathcal{F}$, \textit{complexes} or \textit{admissible sets} of $X$.

In probability theory, what we are interested in is a special type of set algebras known as $\sigma$-\textit{algebras}.
\begin{dfnbox}{$\sigma$-Algebra}{sigmaAlgebra}
    A {\color{red} \textbf{$\sigma$-Algebra}} over a set $A$ is a non-empty set algebra over $A$ that is closed under countable union.
\end{dfnbox}
Of course, by the same argument as above, we known that any $\sigma$-algebra is closed under countable intersection as well.

Roughly speaking, we could now define the probability of an event $E \subseteq S$ as the ratio of the size of $E$ to that of $S$. The remaining question now is: how do we define the size of a set (and in particular, an infinite set) properly?
\begin{dfnbox}{Measure}{measure}
    Let $X$ be a set and $\Sigma$ be a $\sigma$-algebra over $X$. A {\color{red} \textbf{measure}} over $\Sigma$ is a function 
    \begin{equation*}
        \mu \colon \Sigma \to \R \cup \{-\infty, +\infty\}
    \end{equation*}
    such that 
    \begin{itemize}
        \item $\mu\left(E\right) \geq 0$ for all $E \in \Sigma$ (non-negativity);
        \item $\mu\left(\varnothing\right) = 0$;
        \item $\mu\left(\bigcup_{i = 1}^{\infty}E_i\right) = \sum_{i = 1}^{\infty}\mu\left(E_i\right)$ for any countable collection of pairwise disjoint elements of $\Sigma$ (countable additivity or $\sigma$-additivity).
    \end{itemize}
    The triple $\left(X, \Sigma, \mu\right)$ is known as a {\color{red} \textbf{measure space}} and the pair $\left(X, \Sigma\right)$, a {\color{red} \textbf{measurable space}}.
\end{dfnbox}
One thing to note here is that if at least one $E \in \Sigma$ has a finite measure, then $\mu\left(\varnothing\right) = 0$ is automatically guaranteed for obvious reasons.
\begin{dfnbox}{Probability Measure}{prob}
    Let $\mathcal{F}$ be a $\sigma$-algebra over a sample space $S$. A {\color{red} \textbf{probability measure}} on $S$ is a measure $P \colon \mathcal{F} \to \left[0, 1\right]$ such that $P\left(S\right) = 1$.
\end{dfnbox}
Obviously, the above definition immediately guarantees that 
\begin{enumerate}
    \item $P\left(A^c\right) = 1 - P\left(A\right)$;
    \item $P\left(A\right) \leq P\left(B\right)$ if $P\left(A\right) \subseteq P\left(A\right)$;
    \item $P\left(A \cup B\right) \leq P\left(A\right) + P\left(B\right)$.
\end{enumerate}
The third result follows from a direct application of the principle of inclusion and exclusion. By induction, one can easily check that 
\begin{equation*}
    P\left(\bigcup_{i = 1}^{n}E_i\right) \leq \sum_{i = 1}^{n}P\left(E_i\right)
\end{equation*}
for any finitely many events. The following proposition extends this result to countable collections of events:
\begin{probox}{Union Bound of Countable Collections of Events}{unionBound}
    Let $\left(S, \mathcal{F}, P\right)$ be a probability space and $E_1, E_2, \cdots, E_n, \cdots \in \mathcal{F}$ is any countable sequence of events, then 
    \begin{equation*}
        P\left(\bigcup_{i = 1}^{\infty}E_i\right) \leq \sum_{i = 1}^{\infty}P\left(E_i\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        Define $F_1 \coloneqq E_1$ and $F_k \coloneqq E_k \backslash \bigcup_{i = 1}^{k - 1}E_i$ for $k \geq 2$. Clearly, the $F_i$'s are pairwise disjoint. By Definition \ref{dfn:sigmaAlgebra}, the $F_i$'s are elements of $\mathcal{F}$. Note that $P\left(F_i\right) \leq \mathbb{E_i}$ for all~$i \in \N^+$, so 
        \begin{align*}
            P\left(\bigcup_{i = 1}^{\infty}E_i\right) & = P\left(\bigcup_{i = 1}^{\infty}F_i\right) \\
            & = \sum_{i = 1}^{\infty}P\left(F_i\right) \\
            & \leq \sum_{i = 1}^{\infty}P\left(E_i\right).
        \end{align*}
    \end{proof}
\end{probox}
Intuitively, the equality is attained if and only if the events are pairwise disjoint. 
\section{Conditional Probability}
Suppose $E$ and $F$ are events in the same sample space. We should reassess the probability of $E$ given that $F$ has occurred because now we have gained some new information which could alter our prediction for future events.

Notice that by given the condition on the occurrence of $F$, we have effectively reduced the sample space to $F$ and the event to $E \cap F$.
\begin{dfnbox}{Conditional Probability}{condProb}
    Let $P$ be a probability measure on a sample space $S$. For any events $E, F \subseteq S$, the {\color{red} \textbf{conditional probability}} of $E$ given $F$ is defined as 
    \begin{equation*}
        P\left(E \mid F\right) = \frac{P\left(E \cap F\right)}{P\left(F\right)}.
    \end{equation*}
\end{dfnbox}
Clearly, the above definition is equivalent to $P\left(E \cap F\right) = P\left(F\right)P\left(E \mid F\right)$, which is natural in a sense that if we wish both $E$ and $F$ to happen, we just need $F$ to happen first and $E$ to happen given the occurrence of $F$. This can be generalised into the following result:
\begin{thmbox}{Law of Total Probabilities}{totalProb}
    Let $F_1, F_2, \cdots, F_n$ be a partition of a sample space $S$ with probability measure $P$. For any event $A \subseteq S$, 
    \begin{equation*}
        P\left(A\right) = \sum_{i = 1}^{n}P\left(A \mid F_i\right)P\left(F_i\right).
    \end{equation*}
\end{thmbox}
We can generalise the formula in Definition \ref{dfn:condProb} into any finite number of events.
\begin{probox}{Generalised Formula for Conditional Probability}{genCondProb}
    Let $E_1, E_2, \cdots, E_n$ be events in a sample space $S$ with probability measure $P$, then 
    \begin{equation*}
        P\left(\bigcap_{i = 1}^nE_i\right) = P\left(E_1\right)\prod_{i = 1}^{n - 1}P\left(E_{i + 1} \mid E_1, \cdots, E_i\right).
    \end{equation*}
\end{probox}
Additionally, recall that the Bayes' theorem states the following:
\begin{thmbox}{Bayes' Theorem}{Bayes}
    Let $A$ and $B$ be events in a sample space with probability measure $P$, then 
    \begin{equation*}
        P\left(A \mid B\right) = \frac{P\left(B \mid A\right)P\left(A\right)}{P\left(B\right)}.
    \end{equation*}
\end{thmbox}
Note that it is not necessary that $P\left(E \mid F\right) < P\left(E\right)$. In some cases, the occurrence of $F$ does not affect the occurrence of $E$.
\begin{dfnbox}{Independent Events}{Independence}
    Let $S$ be a sample space with probability measure $P$. Two events $E, F \subseteq S$ are {\color{red} \textbf{independent}} if $P\left(E \mid F\right) = P\left(E\right)$, or equivalently, $P\left(E \cap F\right) = P\left(E\right)P\left(F\right)$. A collection of events $E_1, E_2, \cdots, E_n$ are said to be {\color{red} \textbf{jointly independent}} if for any $I \subseteq \left\{1, 2, \cdots, n\right\}$, 
    \begin{equation*}
        P\left(\bigcap_{i \in I}E_i\right) = \prod_{i \in I}P\left(E_i\right),
    \end{equation*}
    or equivalently, $P\left(E_1 \mid E_2, \cdots, E_n\right) = P\left(E_1\right)$.
\end{dfnbox}
Note that $E$ and $F$ are independent if and only if $E, F, E^c, F^c$ are pairwise independent. Moreover, for any jointly independent collection of events $E_1, E_2, \cdots, E_n$ and any disjoint index sets $I, J \subseteq \left\{1, 2, \cdots, n\right\}$,
\begin{equation*}
    P\left(\bigcap_{i \in I}E_i \cap \bigcap_{j \in J}E_j^c\right) = \left(\prod_{i \in I}P\left(E_i\right)\right)\left(\prod_{j \in J}P\left(E_j^c\right)\right).
\end{equation*}
\begin{notebox}
    \begin{remark}
        Joint independence is a strictly stronger result than pairwise independence, i.e., there exists pairwise independent events $E_1, E_2, E_3$ such that 
        \begin{equation*}
            P\left(E_1 \cap E_2 \cap E_3\right) \neq P\left(E_1\right)P\left(E_2\right)P\left(E_3\right).
        \end{equation*}
    \end{remark}
\end{notebox}
\section{Random Variables}
A random variable can be viewed as a function that maps the outcomes in a sample space to some measurable co-domain. We first introduce a few preliminary definitions.
\begin{dfnbox}{Probability Space}{probSpace}
    A {\color{red} \textbf{probability space}} is a tuple $\left(S, \mathcal{F}, P\right)$ where $S$ is a sample space, $\mathcal{F}$ is a $\sigma$-algebra on $S$ and $P$ is a probability measure on $S$.
\end{dfnbox}
It can be troublesome to consider different sample spaces for different experiments. Therefore, we define the \textit{abstract probability space} as $\left(\Omega, \mathcal{F}, P\right)$ with a uniform random variable~$Z \colon \Omega \to \mathcal{Z}$ such that for every random variable $X \colon S \to \mathcal{X}$, there exists a function~$f \colon \mathcal{Z} \to \mathcal{X}$ such that $X$ is simply $f\left(Z\right)$ restricted to $S$. For convenience, we often choose $\Omega = \left[0, 1\right]$ and $P$ to be the uniform measure on $\left[0, 1\right]$.

One important property of a probability space is \textbf{countable additivity}. Specifically, if $P$ is a probability measure on a sample space $\Omega$ and $A_1 \subseteq A_2 \subseteq \cdots \subseteq \Omega$, then 
\begin{equation*}
    P\left(\bigcup_{i \in \N}A_i\right) = \lim_{n \to \infty}A_n.
\end{equation*}
On the other hand, if $\Omega \supseteq A_1 \supseteq A_2 \supseteq \cdots$, then 
\begin{equation*}
    P\left(\bigcap_{i \in \N}A_i\right) = \lim_{n \to \infty}A_n.
\end{equation*}
It is important that the co-domain of a random variable is measurable. For this purpose, we construct some structure to generalise open intervals in $\R$.
\begin{dfnbox}{Borel Algebra}{borelAlgebra}
    Let $X$ be a topological space. A {\color{red} \textbf{Borel set}} on $X$ is a set which can be formed via countable union, countable intersection and relative complementation of open sets in $X$. The smallest $\sigma$-algebra over $X$ containing all Borel sets on $X$ is known as the {\color{red} \textbf{Borel algebra}} over $X$.
\end{dfnbox}
Note that the Borel set over $\R$ is just the family of all open intervals. 

Clearly, the Borel algebra over $X$ contains all open sets in $X$ according to the above axioms from Definition \ref{dfn:sigmaAlgebra}. This helps us define the following:
\begin{dfnbox}{Random Variable}{RV}
    Let $\left(\Omega, \mathcal{F}, P\right)$ be the abstract probability space and $\left(\mathcal{X}, \mathcal{B}\right)$ be a measurable space where~$\mathcal{B}$ is the Borel algebra over $\mathcal{X}$. A {\color{red} \textbf{random variable}} is a function $X \colon S \to \mathcal{X}$ such that 
    \begin{equation*}
        \left\{\omega \in S \colon X\left(\omega\right) \in B\right\} \in \mathcal{F} 
    \end{equation*}
    for all $B \in \mathcal{B}$. The probability measure $P_X$ on $\mathcal{X}$ induced by $P$ with 
    \begin{equation*}
        P_X\left(A\right) \coloneqq P\left(X \in A\right) \coloneqq P\bigl(\left\{\omega \in \Omega \colon X\left(\omega\right) \in A\right\}\bigr)
    \end{equation*}
    is known as the {\color{red} \textbf{distribution}} of $X$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Rigorously, such a random variable $X$ is a \textit{measurable function} or \textit{measurable mapping} from $\left(S, \mathcal{F}\right)$ to $\left(\mathcal{X}, \mathcal{B}\right)$.
    \end{remark}
\end{notebox}
A random variable describes the random outcome of an ``experiment'' or ``phenomenon''. When we perform a series of experiments (for example, model the weather for $n$ consecutive days), it is reasonable to use one random variable to capture the outcome for each experiment. In this way, we obtain a collection of random variables denoted as 
\begin{equation*}
    X_i^n \coloneqq \left(X_1, X_2, \cdots, X_n\right).
\end{equation*}
Clearly, if $X$ is a real-valued random variable, we have $\left\{\omega \in S \colon X\left(\omega\right) > x\right\} \in \mathcal{F}$. Moreover, we claim that 
\begin{equation*}
    \left\{\omega \in S \colon X\left(\omega\right) < x\right\} = \bigcup_{y < x}\left\{\omega \in S \colon X\left(\omega\right) \leq y\right\}.
\end{equation*}
The proof is quite straightforward and is left to the reader as an exercise. By Definition \ref{dfn:sigmaAlgebra}, this means that 
\begin{equation*}
    \left\{\omega \in S \colon X\left(\omega\right) < x\right\} \cup \left\{\omega \in S \colon X\left(\omega\right) > x\right\} \in \mathcal{F}.
\end{equation*}
Therefore, $\left\{\omega \in S \colon X\left(\omega\right) = x\right\} \in \mathcal{F}$. This argument justifies the probabilities $P\left(X < x\right)$ and $P\left(X = x\right)$. 

When a collection of many random variables is concerned, we may consider their \textit{joint distribution}.
\begin{dfnbox}{Joint Distribution}{joint}
    Let $\left(\Omega, \mathcal{F}, P\right)$ be the abstract probability space and $X_i \colon \Omega \to \mathcal{X}_i$ be random variables for $i = 1, 2, \cdots, n$. The {\color{red} \textbf{joint distribution}} of $\mathbfit{X} \coloneqq \left(X_1, X_2, \cdots, X_n\right)$ is the probability measure $P_{\mathbfit{X}}$ with domain $\mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_n$ such that 
    \begin{align*}
        P_{\mathbfit{X}}\left(A_1 \times A_2 \times \cdots \times A_n\right) & \coloneqq P\bigl(\left\{\omega \in \Omega \colon X_1\left(\omega\right) \in A_1, X_2\left(\omega\right) \in A_2, \cdots, X_n\left(\omega\right) \in A_n\right\}\bigr) \\
        & = P\left(X_1 \in A_1, X_2 \in A_2, \cdots, X_n \in A_n\right).
    \end{align*}
\end{dfnbox}
Now we can define independence between random variables in a manner similar to Definition \ref{dfn:Independence}.

In this course, we focus on real-valued random variables, which can be fully determined by their \textit{distribution functions}.
\begin{dfnbox}{Distribution Function}{distribution}
    Let $X$ be a real-valued random variable over the abstract probability space, the {\color{red} \textbf{distribution function}} of $X$ is a function $F_X \colon \Omega \to \left[0, 1\right]$ such that 
    \begin{equation*}
        F_X\left(x\right) = P\left(X \leq x\right).
    \end{equation*}
\end{dfnbox}
Note that for all $a < b \in \R$, 
\begin{equation*}
    P\bigl(X \in \left(a, b\right]\bigr) = F_X\left(b\right) - F_X\left(a\right).
\end{equation*}
It can be shown from here that $F_X$ fully determines the distribution of $X$ (which is non-trivial).

A distribution function $F_X$ has the following properties:
\begin{enumerate}
    \item $F_X$ is \textbf{non-decreasing}.
    \item $\lim_{x \to -\infty}F_X\left(x\right) = 0$ and $\lim_{x \to +\infty}F_X\left(x\right) = 1$.
    \item For all $x \in \R$, $F_X\left(x\right) = \lim_{y \to x^+}F_X\left(y\right)$ and $F_X\left(x^-\right) \coloneqq \lim_{y \to x^-}F_X\left(y\right)$ exists. In particular, $P\left(X = x\right) = F_X\left(x\right) - F_X\left(x^-\right)$.
\end{enumerate}
\begin{notebox}
    \begin{remark}
        Conversely, every function $F \colon \R \to \left[0, 1\right]$ satisfying the above properties induces a probability measure $P$ on $\R$ with $P\bigl(\left(-\infty, x\right]\bigr) = F_X\left(x\right)$.
    \end{remark}
\end{notebox}
In computer programs, a random number is often generated via the uniform random variable $Z$ over $\left[0, 1\right]$. $Z$ can be used to generate a random variable $X$ associated to any given distribution function $F$.
\begin{thmbox}{Distribution Simulation}{simulate}
    Let $F \colon \R \to \left[0, 1\right]$ be any distribution function. Define $F' \colon \left[0, 1\right] \to \R$ by
    \begin{equation*}
        F'\left(y\right) = \inf \left\{x \in \R \colon F\left(x\right) \geq y\right\}.
    \end{equation*}
    Let $Z$ be the uniform random variable on $\left[0, 1\right]$, then $X \coloneqq F'\left(Z\right)$ is a random variable with distribution function $F$.
\end{thmbox}
A random variable can be discrete or continuous. We first define the discrete case.
\begin{dfnbox}{Discrete Random Variable}{discreteRV}
    A random variable is {\color{red} \textbf{discrete}} if its range is countable.
\end{dfnbox}
Here we list down some commonly used discrete random variables and their distributions:
\begin{itemize}
    \item $X \sim \mathrm{Bernoulli}\left(p\right)$ where $0 < p < 1$:
    \begin{equation*}
        P\left(X = i\right) = \begin{cases}
            1 - p & \quad\textrm{if } i = 0 \\
            p & \quad\textrm{if } i = 1
        \end{cases}.
    \end{equation*}
    \item $X \sim \mathrm{B}\left(n, p\right)$ where $0 < p < 1$ and $n \in \N^+$:
    \begin{equation*}
        P\left(X = x\right) = \begin{pmatrix}
            n \\
            x
        \end{pmatrix}p^x\left(1 - p\right)^{n - x} = \frac{n!}{x!\left(n - x\right)!}p^x\left(1 - p\right)^{n - x}.
    \end{equation*}
    \item $X \sim \mathrm{Geo}\left(p\right)$ where $0 < p < 1$:
    \begin{equation*}
        P\left(X = x\right) = p\left(1 - p\right)^{x - 1}.
    \end{equation*}
    \item $X \sim \mathrm{Pois}\left(\lambda\right)$ where $\lambda > 0$:
    \begin{equation*}
        P\left(X = x\right) = \e^{-\lambda}\frac{\lambda^x}{x!}.
    \end{equation*}
\end{itemize}
Correspondingly, we give the definition for continuous random variables.
\begin{dfnbox}{Continuous Random Variables}{continuousRV}
    A random variable $X$ is {\color{red} \textbf{continuous}} if there exists a function $f_X \colon \R \to \left[0, \infty\right)$ called the {\color{red} \textbf{probability density function}} such that for all $a < b$, 
    \begin{equation*}
        P\bigl(X \in \left(a, b\right]\bigr) = \int_{a}^{b}\!f_X\left(x\right)\,\d x.
    \end{equation*}
\end{dfnbox}
The commonly used continuous random variables are as follows:
\begin{itemize}
    \item $X \sim \mathrm{U}\left(a, b\right)$ where $b \geq a$:
    \begin{equation*}
        f_X\left(x\right) = \frac{x - a}{b - a}.
    \end{equation*}
    \item $X \sim \mathrm{Exp}\left(\lambda\right)$ where $\lambda > 0$:
    \begin{equation*}
        f_X\left(x\right) = \lambda \e^{-\lambda x}\mathbf{1}_{\left[0, \infty\right)}\left(x\right).
    \end{equation*}
    \item $X \sim \mathcal{N}\left(\mu, \sigma^2\right)$ where $\sigma^2 > 0$:
    \begin{equation*}
        f_X\left(x\right) = \frac{1}{\sqrt{2\pi\sigma^2}}\e^{-\frac{\left(x - \mu\right)^2}{2\sigma^2}}.
    \end{equation*}
\end{itemize}
A random variable can be neither discrete nor continuous. For example, let $Y$ be the result of rolling a fair die, $Z \sim \mathrm{U}\left(0, 1\right)$ and $W \sim \mathrm{Bernoulli}\left(p\right)$. Define 
\begin{equation*}
    X \coloneqq \mathbf{1}_{W = 1}Y + \mathbf{1}_{W = 2}Z,
\end{equation*}
then $P\left(X \in A\right) = pP\left(Y \in A\right) + \left(1 - p\right)P\left(Z \in A\right)$.
\subsection{Expectation}
Recall that we have defined expectations for discrete and continuous random variables in elementary probability theory. In terms of measure theory, the two formulae can be unified as the Lebesgue integral
\begin{equation*}
    \mathbb{E}[X] = \int_{S}\!X\left(\omega\right)\,\d P\left(\omega\right).
\end{equation*}
In the discrete case, we have 
\begin{equation*}
    \mathbb{E}\left[g\left(x\right)\right] = \sum_{x \in \mathcal{X}}g\left(x\right)P\left(X = x\right).
\end{equation*}
If $X$ is non-negative integer-valued, this is equivalent to 
\begin{equation*}
    \mathbb{E}\left[g\left(x\right)\right] = \sum_{n = 0}^{\infty}P\left(X \geq n\right).
\end{equation*}
In the real-valued continuous case, we have 
\begin{equation*}
    \mathbb{E}\left[g\left(x\right)\right] = \int_{-\infty}^{\infty}\!g\left(x\right)f_X\left(x\right)\,\d x.
\end{equation*}
The following result gives a way to approximate probabilities by the expectation of a random variable:
\begin{thmbox}{Markov's Inequality}{MarkovIneq}
    If $X$ is a non-negative random variable, then $P\left(X \geq a\right) \leq \frac{\mathbb{E}[X]}{a}$ for all $a > 0$.
    \tcblower
    \begin{proof}
        It suffices to prove for the continuous case. Notice that 
        \begin{align*}
            \mathbb{E}[X] & = \int_{0}^{\infty}\!xf_X\left(x\right)\,\d x \\
            & \geq \int_{a}^{\infty}\!xf_X\left(x\right)\,\d x \\
            & \geq a\int_{0}^{\infty}\!f_X\left(x\right)\,\d x \\
            & = P\left(X \geq a\right).
        \end{align*}
        Therefore, $P\left(X \geq a\right) \leq \frac{\mathbb{E}[X]}{a}$.
    \end{proof}
\end{thmbox}
Note that $\mathbb{E}[X]$ is a real number while $\mathbb{E}[X \mid Y]$ is a \textbf{random variable} formed as a function of $Y$. In a way, $Y$ partitions the sample space into regions where $\mathbb{E}[X \mid Y = y_i]$ gives the expectation of $X$ in the region induced by $Y = y_i$ for each $y_i \in \mathcal{Y}$. In general, the following result holds:
\begin{thmbox}{Law of Iterated Expectations}{iterExpectations}
    Let $X$ and $Y$ be random variables, then $\mathbb{E}\bigl[\mathbb{E}[X \mid Y]\bigr] = \mathbb{E}[X]$.
\end{thmbox}
The above formula can be interpreted as the fact that $\mathbb{E}[X \mid Y]$ is a best estimator for $X$.
\subsection{Variance}
Note that the expectation is insufficient in describing a random variable because probability mass on exceptionally large values can influence the expectation significantly. For example, let $X_n$ be a random variable with $\Pr\left(X_n = n\right) = \frac{1}{n}$ and $\Pr\left(X_n = 0\right) = 1 - \frac{1}{n}$. Notice that by taking the limit, $\lim_{n \to \infty}\Pr\left(X_n = 0\right) = 1$ but $\mathbb{E}\left[X_n\right] = 1$ for all $n \in \N^+$. Therefore, we define the \textit{variance} as another parameter to specify a distribution.
\begin{dfnbox}{Variance}{var}
    Let $X$ be a random variable. The {\color{red} \textbf{variance}} of $X$ is defined as 
    \begin{equation*}
        \mathrm{Var}\left(X\right) \coloneqq \mathbb{E}\left[\left(X - \mathbb{E}\left[X\right]\right)^2\right] = \mathbb{E}\left[X^2\right] - \mathbb{E}\left[X\right]^2.
    \end{equation*}
    $\sqrt{\mathrm{Var}\left(X\right)}$ is called the {\color{red} \textbf{standard deviation}} of $X$.
\end{dfnbox}
Note that the variance might not be finite. Consider a continuous random variable $X$ with probability density function $f\left(x\right) = \frac{c}{1 + x^3}\mathbf{1}_{\left[0, \infty\right)}\left(x\right)$ where $c > 0$ is appropriately chosen. One can check that $\mathbb{E}\left[X\right]$ is finite but $\mathbb{E}\left[X^2\right]$ is unbounded.

The following result is important:
\begin{thmbox}{Chebyshev's Inequality}{ChebyshevIneq}
    For any real-valued random variable $X$ with finite variance, 
    \begin{equation*}
        P\left(\abs{X - \mathbb{E}[X]} > a\sqrt{\mathrm{Var}\left(X\right)}\right) \leq \frac{1}{a^2}
    \end{equation*}
    for all $a > 0$.
    \tcblower
    \begin{proof}
        Define $g\left(X\right) \colon \left(X - \mathbb{E}[X]\right)^2$, which is clearly non-negative. By Theorem \ref{thm:MarkovIneq}, we have 
        \begin{equation*}
            P\bigl(g\left(X\right) > a^2\mathrm{Var}\left(X\right)\bigr) \leq \frac{\mathbb{E}[g\left(X\right)]}{a^2\mathrm{Var}\left(X\right)}.
        \end{equation*}
        Note that $\mathbb{E}[g\left(X\right)] = \mathrm{Var}\left(X\right)$, so 
        \begin{equation*}
            P\left(\abs{X - \mathbb{E}[X]} > a\sqrt{\mathrm{Var}\left(X\right)}\right) = P\bigl(g\left(X\right) > a^2\mathrm{Var}\left(X\right)\bigr) \leq \frac{1}{a^2}.
        \end{equation*}
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        In general, 
        \begin{equation*}
            P\left(\abs{X - \mathbb{E}[X]} > a\right) \leq \frac{\mathrm{Var}\left(X\right)}{a^2}.
        \end{equation*}
    \end{remark}
\end{notebox}
\subsection{Correlation}
Given any random variables $X$ and $Y$, they are not necessarily independent in general. We wish to investigate how correlated they are to each other. For this purpose, we introduce the following notion:
\begin{dfnbox}{Covaraince}{cov}
    If $X$ and $Y$ are real-valued random variables, the {\color{red} \textbf{covariance}} between $X$ and $Y$ is defined as
    \begin{equation*}
        \mathrm{Cov}\left(X, Y\right) \coloneqq \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right] = \mathbb{E}\bigl[\left(X - \mathbb{E}\left[X\right]\right)\left(Y - \mathbb{E}\left[Y\right]\right)\bigr].
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $\mathrm{Cov}\left(X, Y\right) = \mathrm{Cov}\left(Y, X\right)$ and $\mathrm{Cov}\left(X, X\right) = \mathrm{Var}\left(X\right)$.
    \end{remark}
\end{notebox}
\chapter{Stochastic Process}
\section{Markov Chains}
\begin{dfnbox}{Stochastic Process}{stochastic}
    A {\color{red} \textbf{stochastic process}} is a collection of random variables $\left\{X\left(t\right) \colon t \in T\right\}$ where $T$ is an {\color{red} \textbf{index set}} and $X\left(t\right)$ is known as the {\color{red} \textbf{current state}}. The set of all possible states is known as the {\color{red} \textbf{state space}}.
\end{dfnbox}
Let $S$ be a sample space, a stochastic process defined over the space can be thought of a sequence of random variables where $X\left(t\right)$ describes the distribution of an outcome $\omega \in S$ at timestamp $t$. The state space $S$ is simply the co-domain of the $X\left(t\right)$'s.

A stochastic process is said to be 
\begin{itemize}
    \item \textit{discrete-time} if the index set is countable;
    \item \textit{continuous-time} if the index set is a continuum;
    \item \textit{discrete-state} if the state space is countable;
    \item \textit{finite-state} if the state space is finite;
    \item \textit{continuous-state} if the state space is a continuum.
\end{itemize}
The term ``continuum'' refers to a \textbf{non-empty compact connected metric space}. 

In this course, we focus on discrete-time discrete-state stochastic processes. One important property we will discuss now is the \textit{Markovian property}.
\begin{dfnbox}{Markovian Property}{markovian}
    Let $\left\{X_n \colon n \in T\right\}$ be a discrete-time stochastic process over some probability space~$\left(S, \mathcal{F}, P\right)$. The stochastic process is called {\color{red} \textbf{Markovian}} if 
    \begin{equation*}
        P\bigl(X_{n + 1} = x_{n + 1} \mid X_0^n = \left(x_0, x_1, \cdots, x_n\right)\bigr) = P\left(X_{n + 1} = x_{n + 1} \mid X_n = x_n\right)
    \end{equation*}
    for all $n \in \N$.
\end{dfnbox}
The Markovian property essentially says that, given $X_n$, what has happened before, i.e., $X_k$ for all $k < n$, is independent of what happens afterwards, i.e., $X_{n + m}$ for $m \in \N^+$.

As the name suggests, the Markovian property is closely related to the Markov chains, which can be defined rigorously as follows:
\begin{dfnbox}{Discrete-Time Markov Chain}{MarkovChain}
    A {\color{red} \textbf{Markov chain}} is a discrete-time discrete-state stochastic process satisfying the Markovian property.
\end{dfnbox}
Recall that the \textit{Bayes's Theorem} states the following:
\begin{thmbox}{Bayes's Theorem}{Bayes}
    For any random variables $X$ and $Y$, 
    \begin{equation*}
        p_{X \mid Y}\left(x \mid y\right) = \frac{p_{Y \mid X}\left(y \mid x\right)p_X\left(x\right)}{\sum_{x' \in \mathcal{X}}p_{Y \mid X}\left(y \mid x'\right)p_X\left(x'\right)}.
    \end{equation*}
\end{thmbox}
Theorem \ref{thm:Bayes} leads to the following important result:
\begin{corbox}{Bayes' Rule for Markov Chains}{MarkovBayes}
    Let $X_1, X_2, \cdots, X_n$ be any $n$ random variables forming a Markov chain, then 
    \begin{equation*}
        p_{X_1^n}\left(x_1, x_2, \cdots, x_n\right) = p_{X_1}\left(x_1\right)\prod_{i = 1}^{n - 1}p_{X_{i + 1} \mid X_i}\left(x_{i + 1} \mid x_i\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        If $n = 2$, by Theorem \ref{thm:Bayes}, we know that 
        \begin{align*}
            p_{X_1, X_2}\left(x_1, x_2\right) & = p_{X_1 \mid X_2}\left(x_1 \mid x_2\right)p_{X_2}\left(x_2\right) \\
            & = p_{X_1}\left(x_1\right)p_{X_2 \mid X_1}\left(x_2 \mid x_1\right).
        \end{align*}
        Suppose that there exists some integer $k \geq 2$ such that 
        \begin{equation*}
            p_{X_1^k}\left(x_1, x_2, \cdots, x_k\right) = p_{X_1}\left(x_1\right)\prod_{i = 1}^{k - 1}p_{X_{i + 1} \mid X_i}\left(x_{i + 1} \mid x_i\right)
        \end{equation*}
        For any $k$ random variables $X_1^k$ forming a Markov chain. Let $X_{k + 1}$ be any random variable such that $X_1^{k + 1}$ forms a Markov chain, then 
        \begin{equation*}
            p_{X_{k + 1} \mid X_1^{k}}\left(x_{k + 1} \mid x_1, x_2, \cdots, x_k\right) = p_{X_{k + 1} \mid X_k}\left(x_{k + 1} \mid x_k\right).
        \end{equation*}
        By using Theorem \ref{thm:Bayes}, we have
        \begin{align*}
            p_{X_1^{k + 1}}\left(x_1, x_2, \cdots, x_{k + 1}\right) & = p_{X_{k + 1} \mid X_1^{k}}\left(x_{k + 1} \mid x_1, x_2, \cdots, x_k\right)p_{X_1^k}\left(x_1, x_2, \cdots, x_k\right) \\
            & = p_{X_{k + 1} \mid X_k}\left(x_{k + 1} \mid x_k\right)p_{X_1}\left(x_1\right)\prod_{i = 1}^{k - 1}p_{X_{i + 1} \mid X_i}\left(x_{i + 1} \mid x_i\right) \\
            & = p_{X_1}\left(x_1\right)\prod_{i = 1}^{k}p_{X_{i + 1} \mid X_i}\left(x_{i + 1} \mid x_i\right).
        \end{align*}
    \end{proof}
\end{corbox}
\subsection{Transition Probabilities}
Consider a discrete-time discrete-state stochastic process $\left\{X_n \colon n \in T\right\}$ with state space $S$ over some probability space $\left(S, \mathcal{F}, P\right)$. Here, the $\sigma$-algebra $\mathcal{F}$ can be generated using simple events $\left\{\omega \in S \colon X_n\left(\omega\right) = s\right\}$ for all $n \in T$ and $s \in S$. Notice that this means that we need to find the joint distribution 
\begin{equation*}
    p_{X_{n_1}^{n_k}}\left(s_1, s_2, \cdots, s_k\right)
\end{equation*}
for any tuple of random variables $X_{n_1}^{n_k}$ in the stochastic process, where $k \in \N$ and $k \leq \abs{T}$ if $T$ is finite, and any $\left(s_1, s_2, \cdots, s_k\right) \in S^k$. In general, this joint distribution might be hard to find, but things become easier if the stochastic process is a Markov chain because by Corollary \ref{cor:MarkovBayes} we have
\begin{align*}
    p_{X_{n_1}^{n_k}}\left(s_1, s_2, \cdots, s_k\right) = p_{X_{n_1}}\left(s_1\right)\prod_{i = 1}^{k - 1}p_{X_{n_{i + 1}} \mid X_{n_i}}\left(s_{i + 1} \mid s_i\right).
\end{align*}
If we can find $p_{X_m \mid X_n}\left(s_m \mid s_n\right)$ for any $m > n$, we could simplify this expression further! 
\begin{dfnbox}{Transition Probability}{transProb}
    The {\color{red} \textbf{transition probability}} is defined as 
    \begin{equation*}
        p_{ij}^{n, m} \coloneqq P\left(X_{m} = j \mid X_n = i\right).
    \end{equation*}
    In particular, $p_{ij}^{n, n + 1}$ is known as the {\color{red} \textbf{one-step transition probability}} or {\color{red} \textbf{jump probability}}.
\end{dfnbox}
Take some $k \in \N^+$ and consider
\begin{align*}
    p_{ij}^{n, n + k} = P\left(X_{n + k} = j \mid X_n = i\right).
\end{align*}
We first marginalise $P\left(X_{n + k} = j \mid X_n = i\right)$ with respect to $X_{n + 1}$ to obtain
\begin{equation*}
    P\left(X_{n + k} = j \mid X_n = i\right) = \sum_{s \in S}P\left(X_{n + k} = j \mid X_n = i, X_{n + 1} = s\right)P\left(X_{n + 1} = s \mid X_n = i\right).
\end{equation*}
Since $X_n$, $X_{n + 1}$ and $X_{n + k}$ form a Markov chain, we have
\begin{equation*}
    P\left(X_{n + k} = j \mid X_n = i, X_{n + 1} = s\right) = P\left(X_{n + k} = j \mid X_{n + 1} = s\right).
\end{equation*}
Therefore, 
\begin{align*}
    p_{ij}^{n, n + k} & = P\left(X_{n + k} = j \mid X_n = i\right) \\
    & = \sum_{s \in S}P\left(X_{n + k} = j \mid X_{n + 1} = s\right)P\left(X_{n + 1} = s \mid X_n = i\right) \\
    & = \sum_{s \in S}p_{sj}^{n + 1, n + k}p_{is}^{n, n + 1}.
\end{align*}
Notice that now we have reduced the gap by $1$. By repeatedly applying this process to $p_{sj}^{n + 1, n + k}$, we eventually arrive at 
\begin{equation*}
    p_{ij}^{n, n + k} = \sum_{s_1, s_2, \cdots, s_{k - 1} \in S}p_{is_1}^{n, n + 1}\left(\prod_{r = 1}^{m - n - 2}p_{s_rs_{r + 1}}^{n + r, n + r + 1}\right)p_{s_{m - 1}j}^{n + k - 1, n + k}
\end{equation*}
It is useful to see the one-step transition probability $p_{ij}^{n, n + 1}$ as a function 
\begin{equation*}
    f \colon T \times S \times S \to \R.
\end{equation*}
Thus far, we have basically shown that to specify a Markov chain fully, we will need to define the \textbf{index set} $T$, the \textbf{state space} $S$ and the \textbf{one-step transition probabilities} $p_{ij}^{n, n + 1}$ for all $i, j \in S$. 

We can write the transition probabilities as a matrix.
\begin{dfnbox}{Transition Probability Matrix}{TPM}
    For any Markov chain, the {\color{red} \textbf{transition probability matrix}} is a matrix $\mathbfit{P}^{n, n + 1}$ such that~$P^{n, n + 1}_{ij} = p_{ij}^{n, n + 1}$.
\end{dfnbox}
Let $\pi_t$ be the distribution at time $t$ for a Markov chain $\left\{X_i\right\}_{i \in \N^+}$, then we can write 
\begin{equation*}
    \pi_t \coloneqq \begin{bmatrix}
        P\left(X_t = x_1\right), P\left(X_t = x_2\right), \cdots, P\left(X_t = x_{\abs{\mathcal{X}}}\right).
    \end{bmatrix}
\end{equation*}
Therefore, the distribution at time $t + 1$ is given by 
\begin{equation*}
    \pi_{t + 1} = \pi_t\mathbfit{P}^{n, n + 1}.
\end{equation*}
Iterate this process and we have 
\begin{equation*}
    \pi_t = \pi_0\prod_{i = 0}^{t - 1}\mathbfit{P}^{i, i + 1}.
\end{equation*}
Generally, the $\left(i, j\right)$ entry of $\prod_{i = n}^{m - 1}\mathbfit{P}^{i, i + 1}$ is exactly $p_{ij}^{n, m}$.

Notice that in general, $p_{ij}^{n, n + 1}$ is dependent on $n$, but when the transition probability is independent of $n$, the computation will become much easier.
\begin{dfnbox}{Stationary Markov Chain}{timeHomo}
    A Markov chain is {\color{red} \textbf{stationary}} if $p_{ij}^{n, n + 1} = p_{ij}^{1, 2}$ for all $n \in \N^+$.
\end{dfnbox}
For a stationary Markov chain $X$, suppose $\mathbfit{P}^{\left(m\right)}$ is the $m$-step transition probability matrix, then we can write its distribution as 
\begin{equation*}
    \pi_{t + m} = \pi_t\mathbfit{P}^{\left(m\right)} = \pi_t\mathbfit{P}^{m}.
\end{equation*}
This means that any stationary Markov chain is fully determined by its state space, transition probability matrix $\mathbfit{P}$ and the starting distribution $\pi_0$.
\begin{dfnbox}{Stochastic Matrix}{stochasticMatrix}
    Let $S$ be a countable index set. A matrix $\mathbfit{P}$ is called a {\color{red} \textbf{stochastic matrix}} if $P_{ij} \geq 0$ for all $i, j \in S$ and $\sum_{j \in S}P_{ij} = 1$ for all $i \in S$.
\end{dfnbox}
The above computation can be summarised into the following result:
\begin{thmbox}{Chapman-Kolmogorov Equations}{ChapmanKolmogorov}
    Let $X$ be a Markov chain with state space $S$, then 
    \begin{align*}
        \mathbfit{P}^{\left(m\right)} & = \mathbfit{P}\mathbfit{P}^{\left(m - 1\right)} \\
        & = \mathbfit{P}^{\left(m - 1\right)}\mathbfit{P}.
    \end{align*}
    In particular, if $X$ is a stationary Markov chain with transition probability matrix $\mathbfit{P}$, then for any $m, n \in \N$, 
    \begin{equation*}
        P\left(X_{m + n} = j \mid X_0 = i\right) = \sum_{k \in S}P\left(X_m = k \mid X_0 = i\right)P\left(X_n = j \mid X_0 = k\right).
    \end{equation*}
    Furthermore, for all $n \in \N$,\
    \begin{equation*}
        P\left(X_n = j \mid X_0 = i\right) = \mathbfit{P}^n_{ij}.
    \end{equation*}
\end{thmbox}
Take any state space $S \coloneqq \left\{x_1, x_2, \cdots, x_s\right\}$. Notice that if we have a column vector 
\begin{equation*}
    \mu \coloneqq \begin{bmatrix}
        f\left(x_1\right) \\
        f\left(x_2\right) \\
        \vdots \\
        f\left(x_s\right)
    \end{bmatrix}
\end{equation*}
for some function $f$, then 
\begin{align*}
    \left(\mathbfit{P}^n\mu\right)_i & = \sum_{j = 1}^{s}\mathbfit{P}^n_{ij}\mu_j \\
    & = \sum_{j = 1}^{s}P\left(X_n = x_j \mid X_0 = x_i\right)f\left(x_j\right) \\
    & = \mathbb{E}\left[f\left(X_n\right) \mid X_0 = x_i\right].
\end{align*}
Suppose $X_0 \sim \lambda$, then clearly 
\begin{align*}
    \mathbb{E}\left[f\left(X_n\right)\right] & = \sum_{i = 1}^{s}\mathbb{E}\left[f\left(X_n\right) \mid X_0 = x_i\right]\lambda_i \\
    & = \sum_{i = 1}^{s}\lambda_i\left(\mathbfit{P}^n\mu\right)_i \\
    & = \lambda\mathbfit{P}^n\mu.
\end{align*}
\subsection{Stationary Distribution}
One classic problem related to Markov chains is \textbf{Gambler's Ruin}, which states the following scenario:
\begin{quote}
    Two gamblers $A$ and $B$ bet against each other by flipping a fair coin. Upon each flip, if a head shows up, $B$ gives $1$ dollar to $A$ and if a tail shows up, $A$ gives $1$ dollar to $B$. Let $X_i$ and $Y_i$ be the amount of money $A$ and $B$ have after the $i$-th coin flip respectively. If $X_0 = m$ and $Y_0 = n$, what is the probability that $A$ is ruined first? 
\end{quote}
We can model this problem with a discrete-time Markov chain $X$ with finite state space $S$. Fix some $a, b \in S$ with $a \neq b$, and define 
\begin{equation*}
    H \coloneqq \begin{cases}
        1 & \quad\textrm{if there exists } i < j \textrm{ such that } X_i = a \textrm{ and } X_j = b \\
        0 & \quad\textrm{otherwise}
    \end{cases}.
\end{equation*}
Define $f\left(x\right) = P\left(H = 1 \mid X_0 = x\right)$. Notice that 
\begin{align*}
    \mathbb{E}\left[f\left(X_1\right) \mid X_0 = x\right] & = \sum_{x \in S}P\left(H = 1 \mid X_0 = X_1, X_0 = x\right)P\left(X_1 = x \mid X_0 = x\right) \\
    & = \sum_{x \in S}P\left(H = 1 \mid X_1 = x, X_0 = x\right)P\left(X_1 = x \mid X_0 = x\right) \\
    & = P\left(H = 1 \mid X_0 = x\right).
\end{align*}
Therefore, $f\left(x\right) = \left(\mathbfit{P}\mu\right)\left(x\right)$. In other words, $\mathbfit{P}\mu$ gives the conditional distribution for the event that $a$ is reached before $b$ given the starting state $X_0$.

Furthermore, suppose $\mu = \mathbf{1}$ is a constant vector, then clearly $\mathbfit{P}^n\mu = \mu = \mathbf{1}$ for all $n \in \N$. Therefore, $\mathbf{1}$ is an eigenvector for $\mathbfit{P}$ with eigenvalue $1$. Note that $\mathbfit{P}^{\mathrm{T}}$ and $\mathbfit{P}$ have the same eigenvalues, so there exists some row vector $\nu$ such that 
\begin{equation*}
    \nu\mathbfit{P} = \nu.
\end{equation*}
If $\nu$ is a probability vector, then if $X_0 \sim \nu$, we have $X_n \sim \nu$ for all $n \in \N$. Such $\nu$ is known as the \textit{stationary distribution}.
\begin{dfnbox}{Stationary Distribution}{stationaryDistr}
    A distribution $\pi$ for a Markov chain $X$ with transition probability matrix $\mathbfit{P}$ is a {\color{red} \textbf{stationary distribution}} if $\pi\mathbfit{P} = \pi$.
\end{dfnbox}
Intuitively, 
\begin{equation*}
    \lim_{n \to \infty}\mathbfit{P}^n = \begin{bmatrix}
        \pi \\
        \pi \\
        \vdots \\
        \pi
    \end{bmatrix}
\end{equation*}
if $\pi$ is a stationary distribution. To see this, we consider the following proposition:
\begin{probox}{Eigenvalues of Transition Probability Matrix}{eigenvalOfTPM}
    Let $\mathbfit{P}$ be the transition probability matrix for a finite-state discrete-time Markov chain $X$. If $\lambda$ is an eigenvalue of $\mathbfit{P}$, then $\abs{\lambda} \leq 1$.
    \tcblower
    \begin{proof}
        Let $\mu$ be the eigenvector associated to $\lambda$. Note that there exists some state $x_0$ such that $\abs{\mu\left(x_0\right)}$ attains the maximum. Let $S$ be the state space. Consider 
        \begin{align*}
            \abs{\lambda\mu\left(x_0\right)} & = \abs{\left(\mathbfit{P}\mu\right)\left(x_0\right)} \\
            & = \abs{\sum_{y \in S}\mathbfit{P}\left(y, x_0\right)\mu\left(y\right)} \\
            & \leq \abs{\mu\left(x_0\right)}\abs{\sum_{y \in S}\mathbfit{P}\left(y, x_0\right)} \\
            & = \abs{\mu\left(x_0\right)}.
        \end{align*}
        Therefore, $\abs{\lambda} \leq 1$.
    \end{proof}
\end{probox}
\subsection{Intercommunicating Classes}
One important property of the state space in Markov chains is the reachability between states.
\begin{dfnbox}{Accessibility}{access}
    Let $X$ be a Markov chain with probability transition matrix $\mathbfit{P}$. A state $j$ is said to be {\color{red} \textbf{accessible}} from another state $i$ if $P^{\left(m\right)}_{ij} > 0$ for some $m \in \N$.
\end{dfnbox}
In other words, $j$ is accessible from $i$ of the Markov chain can reach $j$ with \textbf{non-zero probability} when starting from $i$ using finitely many steps. We sometimes use $i \to j$ to denote such accessibility.

It is easy to check that accessibility is \textbf{transitive}.

Markov chains might exhibit some pathological behaviours.
\begin{dfnbox}{Disconnected States}{disconnected}
    Let $S$ be the state space of a discrete-time Markov chain with probability transition matrix $\mathbfit{P}$. If there exist $X, Y \subseteq S$ such that $P_{ij} = 0$ for all $\left(i, j\right) \in X \times Y$ and for all~$\left(i, j\right) \in Y \times X$, then $X$ and $Y$ are said to be {\color{red} \textbf{disconnected}}. 
\end{dfnbox}
Note the disconnected states are mutually unreachable. There is another situation where reachability is one-way.
\begin{dfnbox}{Source and Sink}{sourceSink}
    Let $S$ be the state space of a discrete-time Markov chain with probability transition matrix $\mathbfit{P}$. A state $i \in S$ is a {\color{red} \textbf{source}} if for all $j \neq i$, we have $P_{ji} = 0$, and a {\color{red} \textbf{sink}}, or {\color{red} \textbf{absorbing state}}, if for all $j \neq i$, we have $P_{ij} = 0$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Colloquially, a source is a state which cannot be reached again once left and a sink is a state which the process cannot leave once reached.
    \end{remark}
\end{notebox}
Therefore, the ideal case is that every pair of states are mutually reachable.
\begin{dfnbox}{Intercommunication}{intercom}
    For a Markov chain $X$ with state space $S$, two states $x$ and $y$ are said to {\color{red} \textbf{intercommunicate}}, denoted as $x \leftrightarrow y$, if 
    \begin{equation*}
        P\left(X_m = y \mid X_0 = x\right) > 0 \quad\textrm{and}\quad P\left(X_n = x \mid X_0 = y\right) > 0
    \end{equation*}
    for some $m, n \in \N$.
\end{dfnbox}
It is clear that intercommunication is an \textbf{equivalence relation} on the state space, the proof of which is easy enough to be left as an exercise to the reader. 

Now, the state space will be partitioned by the quotient with $\leftrightarrow$. Each equivalence class, denoted by $\mathcal{C}$, is a closed walk with states as the vertices. Clearly it suffices to study the Markov chain on a sink class to investigate the long-term behaviour.

In particular, we may consider a Markov chain such that the entire state space intercommunicates.
\begin{dfnbox}{Irreducibility}{irreducible}
    A Markov chain is {\color{red} \textbf{irreducible}} if its state space is an equivalence class under $\leftrightarrow$.
\end{dfnbox}
Note that a Markov chain is irreducible if and only if it has only one intercommunicating class. Furthermore, an irreducible chain can have finitely or infinitely many states.

Irreducible Markov chains can be further classified to \textit{transient} ones and \textit{recurrent} ones.
\begin{dfnbox}{Return Probability}{returnProb}
    Let $i$ be a state of a Markov chain $X$. The {\color{red} \textbf{return probability}} to $i$ is defined as 
    \begin{equation*}
        P^{\left(n\right)}_{ii} \coloneqq P\left(X_n = i \mid X_0 = i\right).
    \end{equation*}
\end{dfnbox}
By this definition, we have $P^{\left(n\right)}_{ii} = \left(\mathbfit{P}^n\right)_{ii}$ for stationary chains. Note that $3$ cases could be possible when a state $i$ is re-visited in $n$ steps:
\begin{enumerate}
    \item starting from $i$, the chain self-loop at $i$ for $n$ times;
    \item starting from $i$, the chain visits some different states and re-visit $i$ for the first time at the $n$-th step;
    \item starting from $i$, the chain re-visits $i$ for several times and happens to reach $i$ again at the $n$-th step.
\end{enumerate}
However, we should observe that the key to computing the probability for either case is to find the time taken for the chain to re-visit the starting state \textbf{for the first time}.
\begin{dfnbox}{First Return Probability}{1stReturn}
    Let $X$ be a Markov chain and $i$ be a state. The {\color{red} \textbf{first return probability}} to $i$ at the $n$-th transition is defined as 
    \begin{equation*}
        f_{ii}^{\left(n\right)} \coloneqq P\left(X_1 \neq i, X_2 \neq i, \cdots, X_{n - 1} \neq i, X_n = i \mid X_0 = i\right).
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        We define $f^{\left(0\right)}_{ii} = 0$.
    \end{remark}
\end{notebox}
Clearly, $f_{ii}^{\left(n\right)} \leq P_{ii}^{\left(n\right)}$ for all $n \in \N$. 
\begin{thmbox}{Formula for Return Probability}{returnProbFormula}
    Let $X$ be a Markov chain and $i$ be a state, then 
    \begin{equation*}
        P_{ii}^{\left(n\right)} = \sum_{k = 0}^{n}f_{ii}^{\left(k\right)}P_{ii}^{\left(n - k\right)}.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let 
        \begin{equation*}
            T_i \coloneqq \min\left\{n \in \N^+ \colon X_n = i\right\}
        \end{equation*}
        be the first timestamp at which $X$ returns to $i$ starting from $i$. Notice that 
        \begin{align*}
            P_{ii}^{\left(n\right)} & = \sum_{k = 0}^{n}P\left(T_i = k \mid X_0 = i\right)P\left(X_n = i \mid X_k = i\right) \\
            & = \sum_{k = 0}^{n}f_{ii}^{\left(k\right)}P_{ii}^{\left(n - k\right)}.
        \end{align*}
    \end{proof}
\end{thmbox}
Notice that since $f_{ii}^{\left(0\right)} = 0$, we only need $P_{ii}^{\left(1\right)}, \cdots, P_{ii}^{\left(n - 1\right)}$ to compute $P_{ii}^{\left(n\right)}$.

It is intuitive to classify the states based on whether we can ascertain that the Markov chain can return to the state in finite time.
\begin{dfnbox}{Transient and Recurrent States}{transientAndRecur}
    Let 
    \begin{equation*}
        f_{xx} \coloneqq \sum_{n = 0}^{\infty}f^{\left(n\right)}_{xx}
    \end{equation*}
    be the probability that $X$ returns to $x$ in finite time. The state $x$ is said to be {\color{red} \textbf{recurrent}} if $f_{xx} = 1$ and {\color{red} \textbf{transient}} if $f_{xx} < 1$.
\end{dfnbox}
In short, a recurrent state is one which the process will definitely return to in finite time, and a transient state always has a non-zero probability that the process never returns to it in finite time. Therefore, an alternative definition considers 
\begin{equation*}
    T_x \coloneqq \min\left\{n \in \N^+ \colon X_n = x\right\}
\end{equation*}
and define
\begin{equation*}
    f_{xx} \coloneqq \sum_{n = 0}^{\infty}P\left(T_x = n \mid X_0 = x\right).
\end{equation*}
It is easy to see that the two definitions are equivalent, which also follows from Definition \ref{dfn:1stReturn}.

Notice that the probability that the process cannot return to $x$ is given by~$1 - f_{xx}$.

Note that if $x$ is a recurrent state, then the probability of $X$ returning to $x$ in finite time after starting at $x$ is $1$. By the Markovian property, we know that $X$ will also return to $x$ in finite time for the second time. This implies that for any recurrent state $x$, the process will visit it for infinitely many times in the long-run with a probability of $1$. Consider $\mathbf{1}_{\left\{X_n = x\right\}}$ to be a Bernoulli random variable with parameter $f_{xx}$, then if $x$ is recurrent, we have 
\begin{equation*}
    P\left(\sum_{n = 1}^{\infty}\mathbf{1}_{\left\{X_n = x\right\}} = \infty\right) = 1.
\end{equation*}
This means that the expected number of returns to $x$ in the long-run is given by
\begin{equation*}
    \mathbb{E}\left[\sum_{n = 0}^{\infty}\mathbf{1}_{\left\{X_n = x\right\}} \mid X_0 = x\right] = \sum_{n = 0}^{\infty}\mathbfit{P}^n\left(x, x\right) = \infty.
\end{equation*}
A small note is that this does not mean that $\mathbfit{P}^n\left(x, x\right)$ must not converge to $0$.

If $x$ is a transient state, then there is a non-zero probability that $X$ never returns to $x$ in finite time. Let $N_x$ be the number of times $X$ returns to $x$ before it never returns to $x$ in finite time. It is clear that $N_x \sim \mathrm{Geo}\left(1 - f_{xx}\right)$. Therefore, the expected number of returns to $x$ is 
\begin{equation*}
    \mathbb{E}\left[N_x\right] = \frac{f_{xx}}{1 - f_{xx}}.
\end{equation*}
We formalise this observation into the following proposition:
\begin{probox}{Expected Number of Returns of Markov Chains}{expectedReturnNum}
    Let $X$ be a discrete-time finite-state irreducible Markov chain and $x$ be some state. Let~$N_x$ be the number of returns to $x$ given $X_0 = x$ before $X$ never returns to $x$ in finite time, then 
    \begin{equation*}
        \mathbb{E}\left[N_x \mid X_0 = x\right] = \frac{f_{xx}}{1 - f_{xx}}.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $T_n$ be the return time for the $n$-th return to $x$ and define $T_0 = 0$. Consider 
        \begin{align*}
            P\left(N_x = k \mid X_0 = x\right) & = P\left(T_{k + 1} = \infty, T_{k} < \infty\right) \\
            & = \sum_{n = 0}^{\infty}P\left(T_{k + 1} = \infty, T_{k} = n\right) \\
            & = \sum_{n = 0}^{\infty}P\left(T_{k + 1} = \infty \mid T_{k} = n\right)P\left(T_{k} = n\right) \\
            & = \sum_{n = 0}^{\infty}P\left(T_1 = \infty \mid X_0 = x\right)P\left(T_{k} = n\right) \\
            & = P\left(T_1 = \infty \mid X_0 = x\right)\sum_{n = 0}^{\infty}P\left(T_{k} = n\right) \\
            & = \left(1 - f_{xx}\right)P\left(T_{k} < \infty\right).
        \end{align*}
        Note that $P\left(T_n < \infty\right) = P\left(T_{n - 1} < \infty\right)f_{xx}$, so we have 
        \begin{equation*}
            P\left(T_n < \infty\right) = f_{xx}^n
        \end{equation*}
        for all $n \in \N$. Therefore, 
        \begin{equation*}
            P\left(N_x = k \mid X_0 = x\right) = \left(1 - f_{xx}\right)f_{xx}^{k}
        \end{equation*}
        and so 
        \begin{equation*}
            \mathbb{E}\left[N_x \mid X_0 = x\right] = \frac{f_{xx}}{1 - f_{xx}}.
        \end{equation*}
    \end{proof}
\end{probox}
\begin{notebox}
    \begin{remark}
        A direct consequence of this result is that the expected number of \textbf{visits} including the initial one to $x$ is $\frac{1}{1 - f_{xx}}$.
    \end{remark}
\end{notebox}
We can related this result to Definition \ref{dfn:returnProb} to derive a characterisation for recurrent and transient states.
\begin{probox}{Characterisation of Recurrent and Transient States}{characteriseStates}
    Let $X$ be a Markov chain and $i$ be a state, then $i$ is transient if and only if 
    \begin{equation*}
        \sum_{n = 1}^{\infty}P^{\left(n\right)}_{ii}
    \end{equation*}
    is finite.
    \tcblower
    \begin{proof}
        Let $N_i$ be the number of returns to $i$ starting from $i$, then
        \begin{equation*}
            N_i = \sum_{n = 1}^{\infty}\mathbf{1}_{\left\{X_n = i\right\}}.
        \end{equation*}
        Therefore, 
        \begin{align*}
            \mathbb{E}\left[N_i \mid X_0 = i\right] & = \mathbb{E}\left[\sum_{n = 1}^{\infty}\mathbf{1}_{\left\{X_n = i\right\}} \mid X_0 = i\right] \\
            & = \sum_{n = 1}^{\infty}\mathbb{E}\left[\mathbf{1}_{\left\{X_n = i\right\}} \mid X_0 = i\right] \\
            & = \sum_{n = 1}^{\infty}P\left(X_n = i \mid X_0 = i\right) \\
            & = \sum_{n = 1}^{\infty}P^{\left(n\right)}_{ii}.
        \end{align*}
        By Proposition \ref{pro:expectedReturnNum}, $i$ is transient if and only if $\sum_{n = 1}^{\infty}P^{\left(n\right)}_{ii}$ is finite.
    \end{proof}
\end{probox}
An important implication of this result is that, if $i$ is a transient state, then $\sum_{n = m}^{\infty}P^{\left(n\right)}_{ii}$ is non-increasing as $m$ increases. Therefore, by monotone convergence theorem, 
\begin{equation*}
    \lim_{m \to \infty}\sum_{n = m}^{\infty}P^{\left(n\right)}_{ii} = 0,
\end{equation*}
which means that the probability of returning to $i$ is \textbf{vanishing} in the long-run.

Intuitively, if a transient state $x$ intercommunicates with some other state $y$, then $y$ will also be transient because we can return to $y$ as long as we return to $x$. In general, we propose the following result:
\begin{probox}{Recurrence and Transience as a Class Property}{classRecurTrans}
    Let $x$ and $y$ be intercommunicating states of a Markov chain, then they are either both transient or both recurrent.
    \tcblower
    \begin{proof}
        It suffices to show that $x$ is recurrent if and only if $y$ is recurrent. Let $\mathbfit{P}$ be the transition probability matrix of the Markov chain. Since $x \leftrightarrow y$, there exists $k, \ell \in \N$ such that
        \begin{equation*}
            \mathbfit{P}^k\left(x, y\right) > 0, \qquad \mathbfit{P}^{\ell}\left(x, y\right) > 0.
        \end{equation*}
        Notice that 
        \begin{equation*}
            \mathbfit{P}^{k + \ell + n}\left(x, x\right) \geq \mathbfit{P}^{k}\left(x, y\right)\mathbfit{P}^{n}\left(y, y\right)\mathbfit{P}^{\ell}\left(y, x\right)
        \end{equation*}
        for all $n \in \N$. Notice that the expected number of returns to $x$ is given by 
        \begin{align*}
            \sum_{i = 0}^{\infty}\mathbfit{P}^i\left(x, x\right) & \geq \sum_{n = 0}^{\infty}\mathbfit{P}^{k + \ell + n}\left(x, x\right) \\
            & \geq \sum_{n = 0}^{\infty}\mathbfit{P}^{k}\left(x, y\right)\mathbfit{P}^{n}\left(y, y\right)\mathbfit{P}^{\ell}\left(y, x\right) \\
            & = \mathbfit{P}^{k}\left(x, y\right)\mathbfit{P}^{\ell}\left(y, x\right)\sum_{n = 0}^{\infty}\mathbfit{P}^{n}\left(y, y\right).
        \end{align*}
        Therefore, $x$ is recurrent if and only if $y$ is recurrent.
    \end{proof}
\end{probox}
Therefore, for each intercommunicating class in a Markov chain, we can classify it as either a ``recurrent'' class or a ``transient'' class. This motivates the following definition:
\begin{dfnbox}{Recurrent and Transient Markov Chains}{recurTransMarkov}
    An irreducible Markov chain with a countable state space is called {\color{red} \textbf{recurrent}} if one of its states is recurrent, and {\color{red} \textbf{transient}} if one of its states is transient.
\end{dfnbox}
Now we states a result about finite-state irreducible chains:
\begin{probox}{Recurrence of Finite-State Irreducible Markov Chains}{finiteMarkovRecur}
    All finite-state irreducible Markov chains are recurrent.
\end{probox}
\end{document}