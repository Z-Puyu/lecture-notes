\documentclass[math]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\begin{document}
\fancyhead[L]{
    Linear Algebra
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents
\chapter{Vector Spaces}
\section{Vector Spaces}
In this section, we introduce the notion of \textbf{vector spaces}.
\begin{dfnbox}{Vector space}{vecspace}
    A vector space is a {\color{red} \textbf{non-empty}} set $V$, whose elements are known as {\color{red} \textbf{vectors}}, over a field $K$, whose elements are known as {\color{red} \textbf{scalars}}, with two binary operations, namely addition and scalar multiplication, which satisfies the following axioms:
    \begin{itemize}
        \item Closure under vector addition: for all $\symbfit{u}$, $\symbfit{v} \in V$, $\symbfit{u + v} \in V$.
        \item Closure under scalar multiplication: for all $\symbfit{v} \in V$ and $c \in K$, $c\symbfit{v} \in V$.
        \item Associativity of vector addition: for all $\symbfit{u}$, $\symbfit{v}$, $\symbfit{w} \in V$, $\symbfit{u + (v + w)} = \symbfit{(u + v) + w}$.
        \item Commutativity of vector addition: for all $\symbfit{u}$, $\symbfit{v} \in V$, $\symbfit{u + v} = \symbfit{v + u}$.
        \item Existence of additive identity: there exists a zero vector $\symbf{0}_V \in V$ known as the the additive identity such that $\forall \symbfit{v} \in V$, $\symbfit{v} + \symbf{0}_V = \symbfit{v}$.
        \item Existence of additive inverse: for all $\symbfit{v} \in V$, there exists a vector $-\symbfit{v} \in V$ known as the additive inverse of $\symbfit{v}$ such that $\symbfit{v} + (-\symbfit{v}) = \symbfit{0}$.
        \item Associativity of scalar multiplication: for all $\symbfit{v} \in V$ and $a$, $b \in K$, $a(b\symbfit{v}) = (ab)\symbfit{v}$.
        \item Distributivity of scalar multiplication: for all $\symbfit{v}$, $\symbfit{u} \in V$ and $a$, $b \in K$, $(a + b)\symbfit{v} = a\symbfit{v} + b\symbfit{v}$ and $a(\symbfit{u} + \symbfit{v}) = a\symbfit{u} + a\symbfit{v}$.
        \item Existence of multiplicative identity: there exists a scalar $1_V$ known as the multiplicative identity such that for all $\symbfit{v} \in V$, $1_V \symbfit{v} = \symbfit{v}$.
    \end{itemize}
\end{dfnbox}
It is worth noting that the ``vectors" here might not refer to actual vectors in $\mathbb{R}^n$, i.e., these could be abstract structures which behave like vectors. For example, consider the set of all polynomials of degrees at most $n$. One can check that this set fullfils all axioms from Definition \ref{dfn:vecspace}.

Without establishing further theorems, there are already some interesting properties we can derive from the above definition. For instance:
\begin{thmbox}{Uniqueness of the zero vector}{zerovec}
    For every vector space $V$, there is a unique zero vector $\symbf{0}_V \in V$.
    \tcblower
    \begin{proof}
        Suppose there exist $\symbf{0}_V$, $\symbf{0}'_V \in V$ both being zero vectors in $V$, then we have
        \begin{align*}
            \symbfit{v} + \symbf{0}_V  & = \symbfit{v}, \\
            \symbfit{v} + \symbf{0}'_V & = \symbfit{v}
        \end{align*}
        for all $\symbfit{v} \in V$. Therefore, $\symbfit{v} + \symbf{0}_V = \symbfit{v} + \symbf{0}'_V$ which means $\symbf{0}_V = \symbf{0}'_V$, i.e., $\symbf{0}_V$ is unique.
    \end{proof}
\end{thmbox}
\begin{thmbox}{Uniqueness of the inverse element}{invelem}
    For every vector space $V$ and every vector $\symbfit{v} \in V$, the inverse of $\symbfit{v}$ is unique.
    \tcblower
    \begin{proof}
        Suppose there exist $\symbfit{u}_1$, $\symbfit{u}_2 \in V$ such that both are inverses of $\symbfit{v}$, then we have
        \begin{equation*}
            \symbfit{v} + \symbfit{u}_1 = \symbfit{0}_V = \symbfit{v} + \symbfit{u}_2.
        \end{equation*}
        Therefore, $\symbfit{u}_1 = \symbfit{u}_2 = -\symbfit{v}$, i.e., $-\symbfit{v}$ is unique.
    \end{proof}
\end{thmbox}
One might be tempted to think that the multiplicative identity of a vector space is also unique, and produce the following proof:
\begin{genbox}{Proof?}
    Suppose there exist scalars $a$ and $b$ such that $a\symbfit{v} = b\symbfit{v} = \symbfit{v}$ for all $\symbfit{v} \in V$, then we have
    \begin{equation*}
        (a - b)\symbfit{v} = \symbf{0}_V.
    \end{equation*}
    Therefore, $a = b$, and so the multiplicative identity is unique.
\end{genbox}
Unfortunately, the above proof is wrong, but can you see why?
\begin{notebox}
    \begin{remark}
        Note that for the above proof to hold, one crucial condition is that there exists some non-zero~$\symbfit{v} \in V$, which might not be true! What if $V = \{\symbf{0}_V\}$, meaning that every element in $V$ is now effectively $\symbf{0}_V$? In that case, $a$ and $b$ can take arbitrary values, which makes the proof fail.
    \end{remark}
\end{notebox}
\subsection*{A Special Case: the Euclidean Space}
We focus on a special type of vector spaces which is known as the Euclidean Space. We shall first give the definition.
\begin{dfnbox}{Euclidean space}{euspace}
    The Euclidean $n$-space is the set of all {\color{red} \textbf{ordered}} $n$-tuple of {\color{red} \textbf{real numbers}} $\symbfit{x} = \left(x_1, x_2, x_3, \cdots, x_n\right)$, known as an $n$-vector, where $x_i$ is called the $i$-th component or the $i$-th coordinate of $\symbfit{x}$. Denoting the Euclidean $n$-space by $\mathbb{R}^n$, we have
    \begin{displaymath}
        \mathbb{R}^n = \left\{\,\left(x_1, x_2, x_3, \cdots, x_n\right) \,\colon x_i \in \mathbb{R},\, i \in \mathbb{N}^+,\, i \leq n \,\right\}.
    \end{displaymath}
    Note that we can write $\symbfit{x}$ either as a row vector:
    \begin{displaymath}
        \left[\begin{array}{ccccc}
                x_1 & x_2 & x_3 & \cdots & x_n
            \end{array}\right],
    \end{displaymath}
    or a column vector:
    \begin{displaymath}
        \left[\begin{array}{c}
                x_1    \\
                x_2    \\
                x_3    \\
                \vdots \\
                x_n
            \end{array}\right].
    \end{displaymath}
\end{dfnbox}
We also define the following relations for Euclidean spaces:
\begin{genbox}{Relations and operations in the Euclidean $n$-space}
    \begin{itemize}
        \item Equality: $\symbfit{x} = \symbfit{y}$ if and only if $x_i = y_i$ for all $i$.
        \item Zero vector: the $n$-vector $\symbfit{0} = \left(0, 0, 0, \cdots, 0\right)$ is the zero vector of the Euclidean $n$-space.
        \item Scalar multiplication: for $c \in \mathbb{R}$, $c\symbfit{x} = \left(cx_1, cx_2, cx_3, \cdot, cx_n\right)$.
        \item Inverse: $(-1)\symbfit{x} = -\symbfit{x}$ is the inverse of $\symbfit{x}$.
        \item Vector Addition: $\symbfit{x + y} = \left(x_1 + y_1, x_2 + y_2, x_3 + y_3, \cdots, x_n + y_n\right)$.
    \end{itemize}
\end{genbox}
\section{Vector Subspaces}
Notice that a vector space is just an infinite set which satisfies the axioms listed in Definition \ref{dfn:vecspace}. Therefore, just like any sets, we can define the notion of {\color{red} \textbf{vector subspaces}} as the analogue of subsets.
\begin{dfnbox}{Vector subspace}{vecsubspace}
    Let $V$ be a vector space over the field $K$ and let $W \subseteq V$. $W$ is known as a {\color{red} \textbf{vector subspace}} (or simply subspace) of $V$ if it is a vector space over $K$ under the operations defined in $V$.
\end{dfnbox}
The definition above may sound like a mouthful, but in reality, it suffices to check only two conditions for $W$ to determine whether it is a subspace: first, check that $W$ is {\color{red} \textbf{non-empty}}; second, check that $W$ is {\color{red} \textbf{closed under vector addition and scalar multiplication}}.

To check that $W$ is non-empty, we simply check that $\symbf{0}_V \in W$. In fact, if $\symbf{0}_V \notin W$, then $W$ cannot be a subspace of $V$, because $\symbfit{w} + (-\symbfit{w}) \notin W$!

Concisely, the above conditions are written as:
\begin{thmbox}{Conditions for a subspace}
    A set $W \subseteq V$ is a subspace if and only if 
    \begin{itemize}
        \item $\symbf{0}_V \in W$, and
        \item for all $\symbfit{w}_1$, $\symbfit{w}_2 \in W$, $\alpha \symbfit{w}_1 + \symbfit{w}_2 \in W$ for an arbitrary scalar $\alpha$.
    \end{itemize}
\end{thmbox}
Additionally, we can see that for any vector space $V$, there exist two trivial subspaces to it: the space $V$ itself and the space containing only the zero vector, i.e., $\left\{ \symbf{0}_V \right\}$.
\chapter{Orthogonality}
\section{Dot Product}
Recall that in a Euclidean $n$-space, we can define the {\color{red} \textbf{dot product}} between two vectors:
\begin{dfnbox}{Dot Product}{dotprod}
    Let $\symbfit{u}$, $\symbfit{v} \in \R^n$, then the dot product between $\symbfit{u}$ and $\symbfit{v}$ is defined as
    \begin{equation*}
        \symbfit{u} \cdot \symbfit{v} = \left\lVert \symbfit{u} \right\rVert \left\lVert \symbfit{v} \right\rVert \cos{\theta},
    \end{equation*}
    where $\theta$ is the acute angle between $\symbfit{u}$ and $\symbfit{v}$.

    In particular, if $\symbfit{u} = \left(u_1, u_2, u_3, \cdots, u_n\right)$, $\symbfit{v} = \left(v_1, v_2, v_3, \cdots, v_n\right)$, we have
    \begin{equation*}
        \symbfit{u} \cdot \symbfit{v} = \sum_{i = 1}^{n}u_i v_i.
    \end{equation*}
\end{dfnbox}
Note that we can either view $\symbfit{u}$ and $\symbfit{v}$ as both row vectors or as both column vectors. Either way, we are able to obtain the following matrix representations:
\begin{genbox}{Matrix representations of the dot product}
    If we write $\symbfit{u}$ and $\symbfit{v}$ both as row vectors, we have
    \begin{equation*}
        \symbfit{u} \cdot \symbfit{v} = \left[\begin{array}{ccccc}
                u_1 & u_2 & u_3 & \cdots & u_n
            \end{array}\right] \left[\begin{array}{c}
                v_1    \\
                v_2    \\
                v_3    \\
                \vdots \\
                v_n
            \end{array}\right] = \symbfit{u}\symbfit{v}^{\mathrm{T}};
    \end{equation*}
    if we write $\symbfit{u}$ and $\symbfit{v}$ both as column vectors, we have
    \begin{equation*}
        \symbfit{u} \cdot \symbfit{v} = \left[\begin{array}{ccccc}
                u_1 & u_2 & u_3 & \cdots & u_n
            \end{array}\right] \left[\begin{array}{c}
                v_1    \\
                v_2    \\
                v_3    \\
                \vdots \\
                v_n
            \end{array}\right] = \symbfit{u}^{\mathrm{T}}\symbfit{v}.
    \end{equation*}
\end{genbox}
\section{Orthogonal and Orthonormal Bases}
\begin{dfnbox}{Orthogonality}{ortho}
    Let $\symbfit{u}$, $\symbfit{v} \in \R^n$, they are said to be {\color{red} \textbf{orthogonal}} if $\symbfit{u} \cdot \symbfit{v} = \symbf{0}$, denoted as $\symbfit{u} \perp \symbfit{v}$.
\end{dfnbox}
\begin{dfnbox}{Orthogonal and orthonormal sets}{orthoset}
    Let $S \subseteq \R^n$, if the elements in $S$ are {\color{red} \textbf{pairwise orthogonal}}, then $S$ is called an {\color{red} \textbf{orthogonal set}}.

    In particular, an orthogonal set $S$ is called an {\color{red} \textbf{orthonormal set}} if every element in $S$ is a {\color{red} \textbf{unit vector}}.
\end{dfnbox}
There are some trivial conclusions regarding orthogonal and orthonormal sets which shall be stated without proof:
\begin{genbox}{Some trivial results}
    \begin{itemize}
        \item Every orthonormal set is orthogonal.
        \item If $S$ is an orthogonal set, then every set $T \in \mathcal{P}\left(S\right)$ is orthogonal.
        \item If $S$ is an orthonormal set, the every set $T \in \mathcal{P}\left(S\right)$ is orthonormal.
        \item If $S$ is an orthogonal set, then $S \cup \{\symbf{0}\}$ is orthogonal.
        \item If $S$ is an orthonormal set, then $\symbf{0} \notin S$.
    \end{itemize}
\end{genbox}
We can easily convert an orthogonal set of {\color{red} \textbf{non-zero}} vectors into an orthonormal set:
\begin{thmbox}{Normalisation}{gonaltonormal}
    If $S$ is an orthogonal set and $\symbf{0} \notin S$, then the set
    \begin{displaymath}
        \left\{\,\symbfit{t} \,\colon \symbfit{t} = \frac{\symbfit{s}}{\left\lVert \symbfit{s} \right\rVert},\, \symbfit{s} \in S \,\right\}
    \end{displaymath}
    is an orthonormal set.
\end{thmbox}
It is obvious that a {\color{red} \textbf{non-zero}} orthogonal set must also be a linearly independent set:
\begin{thmbox}{Linear independency of orthonormal sets}{}
    Let $S \subseteq \R^n$ be a non-zero orthogonal set, then $S$ is linearly independent.
    \tcblower
    \begin{proof}
        Let $\symbfit{s}_i \,\, (i = 1, 2, 3, \cdots, k)$ be the elements of $S$.

        Suppose $\sum_{i = 1}^{k} c_i \symbfit{s}_i = \symbf{0}$ for some sequence of real numbers $\left\{c_j\right\}_{j = 1}^k$, then for all $n\,\, (1 \leq n \leq k)$, we have
        \begin{align*}
            0 & = \symbfit{s}_n \cdot \symbf{0}                                       \\
              & = \symbfit{s}_n \cdot \left(\sum_{i = 1}^{k} c_i \symbfit{s}_i\right) \\
              & = \sum_{i = 1}^{k} c_i (\symbfit{s}_n \cdot \symbfit{s}_i)            \\
              & = c_n (\symbfit{s}_n \cdot \symbfit{s}_n)
        \end{align*}
        Note that $\symbfit{s}_n \neq \symbf{0}$, so $c_n = 0$ for all $n\,\, (1 \leq n \leq k)$. Therefore, $S$ is linearly independent.
    \end{proof}
\end{thmbox}
\begin{corbox}{Linear independency of orthogonal sets}{}
    Any orthonormal set is linearly independent.
\end{corbox}
With these preliminary definitions and theorems, we now define the notion of orthogonal and orthonormal bases.
\begin{dfnbox}{Orthogonal and orthonormal bases}{orthobasis}
    A basis $S$ for a vector space $V$ is called an orthogonal basis if it is orthogonal, and orthonormal basis if it is orthornormal.
\end{dfnbox}
There are several advantages of using orthogonal bases, one of which is the ease of finding coordinate vectors.
\begin{thmbox}{Coordinate vectors under orthogonal bases}{orthocoord}
    Let $S = \left\{\, \symbfit{s}_1, \symbfit{s}_2, \symbfit{s}_3, \cdots, \symbfit{s}_k \,\right\}$ be an orthogonal basis for a vector space $V$. Let $\symbfit{v} \in V$ be an arbitrary vector, then
    \begin{displaymath}
        [\symbfit{v}]_S = \left[\begin{array}{c}
                \frac{\symbfit{v} \cdot \symbfit{s_1}}{\left\lVert \symbfit{s}_1 \right\rVert^2} \\
                \frac{\symbfit{v} \cdot \symbfit{s_2}}{\left\lVert \symbfit{s}_2 \right\rVert^2} \\
                \frac{\symbfit{v} \cdot \symbfit{s_3}}{\left\lVert \symbfit{s}_3 \right\rVert^2} \\
                \vdots                                                                           \\
                \frac{\symbfit{v} \cdot \symbfit{s_k}}{\left\lVert \symbfit{s}_k \right\rVert^2} \\
            \end{array}\right].
    \end{displaymath}
    \tcblower
    \begin{proof}
        Since $V = \symrm{span}(S)$, we can write $\symbfit{v} = \sum_{i = 1}^{k} c_i \symbfit{s}_i$.

        Let $n$ be an integer such that $1 \leq n \leq k$, consider
        \begin{align*}
            \symbfit{s}_n \cdot \symbfit{v} & = \symbfit{s}_n \cdot \sum_{i = 1}^{k} c_i \symbfit{s}_i  \\
                                            & = \sum_{i = 1}^{k} c_i(\symbfit{s}_n \cdot \symbfit{s}_i) \\
                                            & = c_n \left\lVert \symbfit{s}_n \right\rVert^2.
        \end{align*}
        Therefore, $c_n = \frac{\symbfit{v} \cdot \symbfit{s}_n}{\left\lVert \symbfit{s}_n \right\rVert^2}$ for all $n$, and so
        \begin{displaymath}
            [\symbfit{v}]_S = \left[\begin{array}{c}
                    \frac{\symbfit{v} \cdot \symbfit{s_1}}{\left\lVert \symbfit{s}_1 \right\rVert^2} \\
                    \frac{\symbfit{v} \cdot \symbfit{s_2}}{\left\lVert \symbfit{s}_2 \right\rVert^2} \\
                    \frac{\symbfit{v} \cdot \symbfit{s_3}}{\left\lVert \symbfit{s}_3 \right\rVert^2} \\
                    \vdots                                                                           \\
                    \frac{\symbfit{v} \cdot \symbfit{s_k}}{\left\lVert \symbfit{s}_k \right\rVert^2} \\
                \end{array}\right].
        \end{displaymath}
    \end{proof}
\end{thmbox}
Based on Theorem \ref{thm:orthocoord}, for every orthogonal set $S = \left\{\, \symbfit{s}_1, \symbfit{s}_2, \symbfit{s}_3, \cdots, \symbfit{s}_k \,\right\}$ and any $\symbfit{v} \in \symrm{span}(S)$, we can write $\symbfit{v} = \sum_{i = 1}^{k} \frac{\symbfit{v} \cdot \symbfit{s}_i}{\left\lVert \symbfit{s}_i \right\rVert^2}\symbfit{s}_i$.

\section{Orthogonality in \textit{n}-Dimensional Spaces}
In 2D and 3D Euclidean spaces, we define the notion of perpendicularity as having an angle of $\frac{\pi}{2}$ between two vectors. Being perpendicular is actually a special case of orthogonality in 2D and 3D Euclidean spaces, which we can generalise to any vector spaces of dimension $n$.
\begin{dfnbox}{Orthogonality in $n$-dimenional vector spaces}{orthogonality}
    Let $V$ be a subspace of $U$ and $\symbfit{u} \in U$ be an arbitrary vector. We say that $\symbfit{u}$ is orthogonal to $V$ if it is orthogonal to every vector in $V$, i.e.,
    \begin{displaymath}
        \symbfit{u} \perp V \Leftrightarrow \forall \symbfit{v} \in V(\symbfit{u} \perp \symbfit{v}).
    \end{displaymath}
\end{dfnbox}
The following statement follows trivially from the definition above:
\begin{thmbox}{A trivial observation}{trivialthm}
    Let $V$ be a vector space and $W \subseteq V$. If $\symbfit{u} \perp V$, then $\symbfit{u} \perp W$.
\end{thmbox}
Definition \ref{dfn:orthogonality} may seem not very useful, as it is not realistically possible to literally iterate through every vector in $V$ (Note that $V$ is infinite!) and check whether it is orthogonal to~$\symbfit{u}$.

However, notice that for every $\symbfit{v} \in V$, we can write $\symbfit{v}$ as a linear combination of the vectors in some spanning set of $V$. Therefore, to check whether a vector $\symbfit{u}$ is orthogonal to $V$, it suffices to just check the orthogonality between $\symbfit{u}$ and each vector in any spanning set of~$V$.
\begin{thmbox}{Condition for orthogonality}{orthocond}
    Let $V = \symrm{span}(S)$ and $\symbfit{u}$ be any vector, then $\symbfit{u} \perp V$ if and only if $\symbfit{u} \perp \symbfit{s}$ for all $\symbfit{s} \in S$.
    \tcblower
    \begin{proof}
        Note that if $\symbfit{u} \perp V$, then by Theorem \ref{thm:trivialthm} it holds trivially that $\symbfit{u} \perp \symbfit{s}$ for all~ $\symbfit{s} \in S \subseteq V$.

        We then prove that if $\symbfit{u} \perp \symbfit{s}$ for all $\symbfit{s} \in S$, then $\symbfit{u} \perp V$.

        Write $S = \left\{\, \symbfit{s}_1, \symbfit{s}_2, \symbfit{s}_3, \cdots, \symbfit{s}_n \,\right\}$. Let $\symbfit{v}$ be any vector in $V$. Note that we can write $\symbfit{v} = \sum_{i = 1}^n c_i \symbfit{s}_i$ for some sequence of real numbers $\left\{c_i\right\}_{i = 1}^n$. Consider
        \begin{align*}
            \symbfit{u} \cdot \symbfit{v} & = \symbfit{u} \cdot \symbfit{\sum_{i = 1}^n c_i \symbfit{s}_i} \\
                                          & = \sum_{i = 1}^n c_i \symbfit{u} \cdot \symbfit{s}_i           \\
                                          & = 0.
        \end{align*}
        Therefore, $\symbfit{u} \perp V$.
    \end{proof}
\end{thmbox}
\section{Projection}
\subsection{Projections in \textit{n}-Dimensional Spaces}
Similar to perpendicularity, we can also extend and generalise the notion of projections from 2D and 3D Euclidean spaces to any $n$-dimensional vector spaces.
\begin{dfnbox}{Projection}{projdef}
    Let $V$ be a subspace of $\R^n$ and $\symbfit{w} \in \R^n$ be a vector. The {\color{red} \textbf{projection}} of $\symbfit{w}$ in $V$ is the vector $\symbfit{p} \in V$ such that $(\symbfit{w - p}) \perp V$.
\end{dfnbox}
Just like how we can compute projections in 2D and 3D Euclidean spaces, we also want to derive a general formula to compute the projection of some vector onto a vector space. We achieve this by utilising the {\color{red} \textbf{orthogonal basis}}.
\begin{thmbox}{General formula for projections}{orthoprojform}
    Let $\left\{\, \symbfit{v}_1, \symbfit{v}_2, \symbfit{v}_3, \cdots, \symbfit{v}_k \,\right\}$ be an orthogonal basis for a vector space $V \subseteq \R^n$. The projection of $\symbfit{w} \in \R^n$ onto $V$ is given by
    \begin{displaymath}
        \sum_{i = 1}^{k} \frac{\symbfit{w} \cdot \symbfit{v}_i}{\left\lVert \symbfit{v}_i \right\rVert^2}\symbfit{v}_i.
    \end{displaymath}
    \tcblower
    \begin{proof}
        Let $\symbfit{p} \in V$ be the projection of $\symbfit{w}$ onto $V$, then there is some $\symbfit{n} \perp V$ such that~$\symbfit{w} = \symbfit{p + n}$.

        For $i = 1, 2, 3, \cdots, k$, note that $\symbfit{(w - p)} \cdot \symbfit{v}_i = \symbfit{n} \cdot \symbfit{v}_i = 0$, so we have $\symbfit{w} \cdot \symbfit{v}_i = \symbfit{p} \cdot \symbfit{v}_i$.

        By Theorem \ref{thm:orthocoord}, we have
        \begin{align*}
            \symbfit{p} & = \sum_{i = 1}^{k} \frac{\symbfit{p} \cdot \symbfit{v}_i}{\left\lVert \symbfit{v}_i \right\rVert^2}\symbfit{v}_i  \\
                        & = \sum_{i = 1}^{k} \frac{\symbfit{w} \cdot \symbfit{v}_i}{\left\lVert \symbfit{v}_i \right\rVert^2}\symbfit{v}_i.
        \end{align*}
    \end{proof}
\end{thmbox}
\begin{corbox}{Alternative formula using orthonormal bases}{normprojform}
    Let $\left\{\, \symbfit{v}_1, \symbfit{v}_2, \symbfit{v}_3, \cdots, \symbfit{v}_k \,\right\}$ be an orthonormal basis for a vector space $V \subseteq \R^n$. The projection of $\symbfit{w} \in \R^n$ onto $V$ is given by
    \begin{displaymath}
        \sum_{i = 1}^{k} (\symbfit{w} \cdot \symbfit{v}_i)\symbfit{v}_i.
    \end{displaymath}
\end{corbox}

\subsection{Gram-Schmidt Process}
Now we have derived a useful formula to compute the projection of any vector onto a vector space given an orthogonal basis for that vector space. The remaining question is: can we find a way to acquire an orthogonal basis for any vector space?

We can consider some simple cases to get some intuition:

\begin{itemize}
    \item Suppose $V_1 = \symrm{span}\left\{\, \symbfit{u}_1 \,\right\}$, then $\left\{\, \symbfit{u}_1 \,\right\}$ is already orthogonal.
    \item Suppose $V_2 = \symrm{span}\left\{\, \symbfit{u}_1, \symbfit{u}_2 \,\right\}$. We can first find the projection of $\symbfit{u}_2$ onto $\symbfit{u}_1$ as $\symbfit{p} = \frac{\symbfit{u}_1 \cdot \symbfit{u}_2}{\left\lVert \symbfit{u}_1 \right\rVert^2}\symbfit{u}_1$. Note that $\left\{\, \symbfit{u}_1, \symbfit{u_2 - p} \,\right\}$ is an orthogonal set, so it is an orthogonal basis for~$V_2$.
    \item Suppose $V_3 = \symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3} \,\right\}$. Similar to the previous case, we can first find the projection of $\symbfit{u_3}$ onto $\symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2} \,\right\} = \symrm{span}\left\{\, \symbfit{u}_1, \symbfit{u_2 - p} \,\right\}$ as $\symbfit{q} = \frac{\symbfit{u_1} \cdot \symbfit{u_3}}{\left\lVert \symbfit{u_1} \right\rVert^2}\symbfit{u_3} + \frac{(\symbfit{u_2 - p}) \cdot \symbfit{u_3}}{\left\lVert \symbfit{u_2 - p}\right\rVert^2}$. Note that $\left\{\, \symbfit{u_1}, \symbfit{u_2 - p}, \symbfit{u_3 - q} \,\right\}$ is an orthogonal set, so it is an orthogonal basis for $V_3$.
\end{itemize}
Notice that a recursive pattern starts to form, which we can describe as follows:
\begin{genbox}{A recursive algorithm (with wishful thinking)}{}
    To find an orthogonal basis for $\symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3}, \cdots, \symbfit{u}_k \,\right\}$:
    \begin{enumerate}
        \item Suppose we already have an orthogonal basis $\left\{\, \symbfit{v_1}, \symbfit{v_2}, \symbfit{v_3}, \cdots, \symbfit{v}_{k - 1} \,\right\}$ for $\symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3}, \cdots, \symbfit{u}_{k - 1} \,\right\}$.
        \item We obtain the projection of $\symbfit{u}_k$ onto $\symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3}, \cdots, \symbfit{u}_k \,\right\}$ by Theorem \ref{thm:orthoprojform}:
              \begin{displaymath}
                  \symbfit{p} = \sum_{i = 1}^{k - 1} \frac{\symbfit{u}_k \cdot \symbfit{v}_i}{\left\lVert \symbfit{v}_i \right\rVert^2}\symbfit{v}_i.
              \end{displaymath}
        \item Now the vector $\symbfit{u_k - p}$ is orthogonal to $\symrm{span}\left\{\, \symbfit{v_1}, \symbfit{v_2}, \symbfit{v_3}, \cdots, \symbfit{v}_{k - 1} \,\right\}$. Setting $\symbfit{v_k} = \symbfit{u}_k - \symbfit{p}$, we have $\left\{\, \symbfit{v_1}, \symbfit{v_2}, \symbfit{v_3}, \cdots, \symbfit{v}_k \,\right\}$ to be an orthogonal basis for $\symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3}, \cdots, \symbfit{u}_k \,\right\}$.
        \item However, in order to obtain the orthogonal set $\left\{\, \symbfit{v_1}, \symbfit{v_2}, \symbfit{v_3}, \cdots, \symbfit{v}_{k - 1} \,\right\}$, we need to find $\symbfit{v}_{k - 1}$ while assuming we already have an orthogonal basis $\left\{\, \symbfit{v_1}, \symbfit{v_2}, \symbfit{v_3}, \cdots, \symbfit{v}_{k - 2} \,\right\}$ for $\symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3}, \cdots, \symbfit{u}_{k - 2} \,\right\}$.
        \item The recursion continues until we reach $\left\{\, \symbfit{u_1} \,\right\}$ which is trivially orthogonal, where we stop and compute backwards to arrive at the final result.
    \end{enumerate}
\end{genbox}
The above algorithm can be described more rigorously as the {\color{red} \textbf{Gram-Schmidt process}}.
\begin{thmbox}{Gram-Schmidt process}{gramschmidt}
    Let $V_k = \symrm{span}\left\{\, \symbfit{u_1}, \symbfit{u_2}, \symbfit{u_3}, \cdots, \symbfit{u}_k \,\right\}$. Define
    \begin{align*}
        \symbfit{v_1} & = \symbfit{u_1},                                                                                                          \\
        \symbfit{v}_n & = \symbfit{u}_n - \sum_{i = 1}^{n - 1}\frac{\symbfit{u}_n \cdot \symbfit{v}_i}{\left\lVert \symbfit{v}_i \right\rVert^2},
    \end{align*}
    then $\left\{\, \symbfit{v_1}, \symbfit{v_2}, \symbfit{v_3}, \cdots, \symbfit{v}_k \,\right\}$ is an orthogonal basis for $V_k$.

    In particular, take $\symbfit{w}_n = \frac{\symbfit{v}_n}{\left\lVert \symbfit{v}_n \right\rVert^2}$, then $\left\{\, \symbfit{w_1}, \symbfit{w_2}, \symbfit{w_3}, \cdots, \symbfit{w}_k \,\right\}$ is an orthonormal basis for $V_k$.
\end{thmbox}
Note that the Gram-Schmidt process can be abstracted as a transition from an arbitrary basis to an orthonormal basis for a given vector space. This means we can also find some transition matrix between any basis for a given vector space and an orthonormal basis for said space, which leads to the following theorem.
\begin{thmbox}{Decomposition}{decomp}
    Let $\symbfit{A}$ be an $m \times n$ matrix whose columns are linearly independent, then there exist an $m \times n$ matrix $\symbfit{Q}$ whose columns form an orthonormal set and an $n \times n$ {\color{red} \textbf{upper-triangular}} matrix $\symbfit{R}$ such that $\symbfit{A} = \symbfit{QR}$.
    \tcblower
    \begin{proof}
        Let the $i$-th column of $\symbfit{A}$ be $\symbfit{u}_i$, then $\left\{ \, \symbfit{u}_1, \symbfit{u}_2, \symbfit{u}_3, \cdots, \symbfit{u}_n \, \right\}$ is a basis for some vector space $V$. By \nameref{thm:gramschmidt}, there is some orthonormal basis $\left\{ \, \symbfit{w}_1, \symbfit{w}_2, \symbfit{w}_3, \cdots, \symbfit{w}_n \, \right\}$ that spans $V$ such that $\symrm{span}\left\{ \, \symbfit{w}_1, \symbfit{w}_2, \symbfit{w}_3, \cdots, \symbfit{w}_j \, \right\} = \symrm{span}\left\{ \, \symbfit{u}_1, \symbfit{u}_2, \symbfit{u}_3, \cdots, \symbfit{u}_j \, \right\}$ for $j = 1, 2, 3, \cdots, n$. Therefore, we can write
        \begin{displaymath}
            \symbfit{u}_j = \sum_{i = 1}^{j} c_{ij}\symbfit{w}_i \label{u=cw} \tag{$*$}
        \end{displaymath}
        for $j = 1, 2, 3, \cdots, n$. Notice that equation \eqref{u=cw} can be written as matrix multiplication:
        \begin{displaymath}
            \symbfit{u}_j = \left[\begin{array}{ccccc}
                    \symbfit{w}_1 & \symbfit{w}_2 & \symbfit{w}_3 & \cdots & \symbfit{w}_n
                \end{array}\right] \left[\begin{array}{c}
                    c_{1j} \\
                    c_{2j} \\
                    \vdots \\
                    c_{jj} \\
                    0      \\
                    \vdots \\
                    0
                \end{array}\right].
        \end{displaymath}
        Therefore, we obtain the following expression:
        \begin{align*}
            \symbfit{A} & = \left[\begin{array}{ccccc}
                                          \symbfit{u}_1 & \symbfit{u}_2 & \symbfit{u}_3 & \cdots & \symbfit{u}_n
                                      \end{array}\right] \\
                        & = \left[\begin{array}{ccccc}
                                          \symbfit{w}_1 & \symbfit{w}_2 & \symbfit{w}_3 & \cdots & \symbfit{w}_n
                                      \end{array}\right] \left[\begin{array}{ccccc}
                                                                   c_{11} & c_{12} & c_{13} & \cdots & c_{1n} \\
                                                                   0      & c_{22} & c_{23} & \cdots & c_{2n} \\
                                                                   \vdots & \vdots & \vdots & \ddots & \vdots \\
                                                                   0      & 0      & 0      & \cdots & c_{nn}
                                                               \end{array}\right] \\
                        & = \symbfit{Q}\symbfit{R},
        \end{align*}
        where $\symbfit{Q}$ is an matrix whose columns form an orthonormal set and $\symbfit{R}$ is an upper-triangular matrix.
    \end{proof}
\end{thmbox}

\subsection{Least Squares Solutions}
Recall that in 2D and 3D Euclidean spaces, we can find the distance between two points using the Pythagoras' Theorem. In terms of vectors, the distance can be viewed as the {\color{red} \textbf{modulus of the difference}} between the position vectors of the corresponding points. With that, now we are ready to abstract further the notion of distance to any $n$-dimensional vector space.
\begin{dfnbox}{Distance}{dist}
    Let $S$ be a vector space with $\dim(S) = n$, and let $\symbfit{u}$, $\symbfit{v} \in S$. The {\color{red} \textbf{distance}} between $\symbfit{u}$ and $\symbfit{v}$ is defined as
    \begin{equation*}
        d(\symbfit{u}, \symbfit{v}) = \left\lVert \symbfit{u - v} \right\rVert.
    \end{equation*}
\end{dfnbox}
A classic problem in mathematics is the optimisation problem, one of the scenarios of which is the minimisation of the distance between to objects. We introduce the following definition regarding minimising distances:
\begin{dfnbox}{Best approximation}{bestapprox}
    Let $S$, $V$ be vectors spaces such that $V \subseteq S$. The {\color{red} \textbf{best approximation}} of $\symbfit{u} \in S$ in $V$ is the vector $\symbfit{p} \in V$ such that
    \begin{displaymath}
        d(\symbfit{u}, \symbfit{p}) \leq d(\symbfit{u}, \symbfit{v}) \quad \textrm{for all } \symbfit{v} \in V.
    \end{displaymath}
\end{dfnbox}
A result that follows immediately from the above definition is:
\begin{thmbox}{Uniqueness of best approximation}{uniqapprox}
    If $\symbfit{p} \in V \subseteq S$ is a best approximation of $\symbfit{u} \in S$ in $V$, then $\symbfit{p}$ is unique.
    \tcblower
    \begin{proof}
        Suppose there is some $\symbfit{p}' \in V$ which is another best approximation of $\symbfit{u}$ in $V$. By Definition \ref{dfn:bestapprox}, we have
        \begin{align*}
            d(\symbfit{u}, \symbfit{p})  & \leq d(\symbfit{u}, \symbfit{v}), \\
            d(\symbfit{u}, \symbfit{p}') & \leq d(\symbfit{u}, \symbfit{v})
        \end{align*}
        for all $\symbfit{v} \in V$. Since $\symbfit{p}$, $\symbfit{p}' \in V$, it follows that
        \begin{align*}
            d(\symbfit{u}, \symbfit{p})  & \leq d(\symbfit{u}, \symbfit{p}'), \\
            d(\symbfit{u}, \symbfit{p}') & \leq d(\symbfit{u}, \symbfit{p}),
        \end{align*}
        which means $d(\symbfit{u}, \symbfit{p}) = d(\symbfit{u}, \symbfit{p}')$.

        Since $\symbfit{p} \neq \symbfit{p}'$, we have $\symbfit{u - p'} = (\symbfit{u - p}) + (\symbfit{p - p'})$. It then follows that
        \begin{align*}
            d(\symbfit{u}, \symbfit{p}') & = \left\lVert \symbfit{u - p'} \right\rVert                                                                                                                      \\
                                         & = \left\lVert (\symbfit{u - p}) + (\symbfit{p - p'}) \right\rVert                                                                                                \\
                                         & = \sqrt{\left[ (\symbfit{u - p}) + (\symbfit{p - p'}) \right] \cdot \left[ (\symbfit{u - p}) + (\symbfit{p - p'}) \right]}                                       \\
                                         & = \sqrt{\left\lVert\symbfit{u - p}\right\rVert^2 + 2 \symbfit{(\symbfit{u - p})} \cdot \symbfit{(\symbfit{p - p'})} + \left\lVert\symbfit{p - p'}\right\rVert^2} \\
                                         & = \sqrt{\left\lVert\symbfit{u - p}\right\rVert^2 + \left\lVert\symbfit{p - p'}\right\rVert^2}                                                                    \\
                                         & \geq \left\lVert\symbfit{u - p}\right\rVert                                                                                                                      \\
                                         & = d(\symbfit{u}, \symbfit{p}),
        \end{align*}
        which is a contradiction, so $\symbfit{p}' = \symbfit{p}$, i.e., $\symbfit{p}$ is unique.
    \end{proof}
\end{thmbox}
Suppose there exists a $\symbfit{p} \in V$ which is the best approximation of $\symbfit{u}$ in $V$, what we are interested now is to compute this best approximation. With some manipulation, we realise that this best approximation $\symbfit{p}$ is exactly the {\color{red} \textbf{projection}} of $\symbfit{u}$ onto $V$!
\begin{thmbox}{Projection as best approximation}{projapprox}
    Let $S$, $V$ be vectors spaces such that $V \subseteq S$. Let $\symbfit{u} \in S$ and $\symbfit{p} \in V$ be vectors. If $\symbfit{p}$ is the projection of $\symbfit{u}$ onto $V$, then $\symbfit{p}$ is the best approximation of $\symbfit{u}$ in $V$.
    \tcblower
    \begin{proof}
        This is equivalent to proving that $d(\symbfit{u}, \symbfit{p}) \leq d(\symbfit{u}, \symbfit{v})$ for all $\symbfit{v} \in V$.

        Let $\symbfit{n} = \symbfit{u - p}$ and $\symbfit{x} = \symbfit{u - v}$ for an arbitrary $\symbfit{v} \in V$. Note that $\symbfit{n} \perp V$. Set $\symbfit{w} = {p - v} \in V$, then $\symbfit{n} \cdot \symbfit{w} = 0$ and $\symbfit{n = w} = \symbfit{x}$. Consider
        \begin{align*}
            d(\symbfit{u}, \symbfit{v}) & = \left\lVert \symbfit{x} \right\rVert                                                                                      \\
                                        & = \sqrt{\symbfit{(\symbfit{n + w})} \cdot \symbfit{(\symbfit{n + w})}}                                                      \\
                                        & = \sqrt{\left\lVert \symbfit{n} \right\rVert^2 + 2(\symbfit{n} \cdot \symbfit{w}) + \left\lVert \symbfit{w} \right\rVert^2} \\
                                        & \geq \left\lVert \symbfit{n} \right\rVert                                                                                   \\
                                        & = d(\symbfit{u}, \symbfit{p}).
        \end{align*}
        Therefore, $\symbfit{p}$ is the best approximation of $\symbfit{u}$ in $V$.
    \end{proof}
\end{thmbox}
Best approximations are useful in dealing with situations where an exact solution is not attainable. For example, suppose the linear system $\symbfit{Ax} = \symbfit{b}$ is consistent, then this is equvalent to saying that there is some $\symbfit{x}$ such that $\symbfit{Ax - b} = \symbf{0}$, i.e., $\left\lVert\symbfit{b - Ax}\right\rVert = 0$. Note that this also means that $\symbfit{b}$ is in the column space of $\symbfit{A}$. Set this column space as $V$.

Now, consider the situation where $\symbfit{b} \notin V$. In this case, the linear system $\symbfit{Ax} = \symbfit{b}$ is inconsistent and thus unsolvable. However, can we still find an $\symbfit{x}$ such that $\symbfit{Ax}$ produces a vector which is {\color{red} \textbf{closest}} to $\symbfit{b}$? In other words, what is the vector $\symbfit{x}$ for the error term $\left\lVert\symbfit{b - Ax}\right\rVert$ to be minimised?
\begin{dfnbox}{Least Squares Solution}{lss}
    Let $\symbfit{A}$ be an $m \times n$ matrix and $\symbfit{b}$ be an $m \times 1$ matrix. A {\color{red} \textbf{least squares solution}} to the linear system $\symbfit{Ax} = \symbfit{b}$ is an $n \times 1$ matrix $\symbfit{u}$ such that $\left\lVert \symbfit{b - Au} \right\rVert \leq \left\lVert \symbfit{b - Av} \right\rVert$ for all $n \times 1$ matrices $\symbfit{v}$.
\end{dfnbox}
Note that $\symbfit{Ax}$ is a vector in the column space of $\symbfit{A}$ and $\left\lVert\symbfit{b - Ax}\right\rVert = d(\symbfit{b}, \symbfit{Ax})$. Naturally, this least squares solution $\symbfit{u}$ is such that $\symbfit{Au}$ is the best approximation of $\symbfit{b}$ in the column space of $\symbfit{A}$!
\begin{thmbox}{Finding the least squares solution to $\symbfit{Ax} = \symbfit{b}$}{findlss}
    Let $\symbfit{A}$ be an $m \times n$ matrix with column space $V$ and $\symbfit{b}$ be an $m \times 1$ matrix. Let $\symbfit{p}$ be the projection of $\symbfit{b}$ onto $V$, then $\symbfit{u}$ is a least squares solution to the linear system $\symbfit{Ax} = \symbfit{b}$ if and only if $\symbfit{u}$ is a solution to the linear system $\symbfit{Ax} = \symbfit{p}$.
    \tcblower
    \begin{proof}
        Since $\symbfit{p}$ is the projection of $\symbfit{b}$ onto $V$, by Theorem \ref{thm:projapprox} and Definition \ref{dfn:bestapprox} we have:
        \begin{displaymath}
            d(\symbfit{b}, \symbfit{p}) \leq d(\symbfit{b}, \symbfit{w}) \quad \textrm{for all } \symbfit{w} \in V.
        \end{displaymath}
        Note that $V = \left\{ \, \symbfit{Av} \,\colon \symbfit{v} \in \mathcal{M}_{n \times 1} \, \right\}$, where $\mathcal{M}_{n \times 1}$ is the set of all $n \times 1$ matrices, so for all $\symbfit{w} \in V$, there is some $\symbfit{v} \in \mathcal{M}_{n \times 1}$ such that $\symbfit{w} = \symbfit{Av}$. Therefore, we have:
        \begin{displaymath}
            d(\symbfit{b}, \symbfit{p}) \leq d(\symbfit{b}, \symbfit{Av}) \quad \textrm{for all } \symbfit{v} \in \mathcal{M}_{n \times 1}.
        \end{displaymath}
        We first prove that if $\symbfit{u}$ is a solution to the linear system $\symbfit{Ax} = \symbfit{p}$, then $\symbfit{u}$ is a least squares solution to the linear system $\symbfit{Ax} = \symbfit{b}$.

        Write $\symbfit{Au} = \symbfit{p}$, then we have:
        \begin{align*}
            \left\lVert \symbfit{b - Au} \right\rVert & = d(\symbfit{b}, \symbfit{p})                \\
                                                      & \leq d(\symbfit{b}, \symbfit{Av})            \\
                                                      & = \left\lVert \symbfit{b - Av} \right\rVert,
        \end{align*}
        which means that $\symbfit{u}$ is the least squares solution to the linear system $\symbfit{Ax} = \symbfit{b}$.

        We then prove that if $\symbfit{u}$ is a least squares solution to the linear system $\symbfit{Ax} = \symbfit{b}$, then $\symbfit{u}$ is a solution to the linear system $\symbfit{Ax} = \symbfit{p}$.

        By Definition \ref{dfn:lss}, $\left\lVert \symbfit{b - Au} \right\rVert \leq \left\lVert \symbfit{b - Av} \right\rVert$ for all $\symbfit{v} \in \mathcal{M}_{n \times 1}$. Note that $\symbfit{Au}$, $\symbfit{Av} \in V$, so $\symbfit{Au}$ is the projection of $\symbfit{b}$ onto $V$, i.e. $\symbfit{Au} = \symbfit{p}$. Therefore, $\symbfit{u}$ is a solution to the linear system $\symbfit{Ax} = \symbfit{p}$.
    \end{proof}
\end{thmbox}
Thus, we can conclude the following algorithm for finding least squares solutions:
\begin{genbox}{A general algorithm for finding a least squares solution to $\symbfit{Ax} = \symbfit{b}$}{}
    \begin{enumerate}
        \item Check if the linear system $\symbfit{Ax} = \symbfit{b}$ is consistent. If the system is consistent, then any solution to it is a least squares solution.
        \item If the system is inconsistent, find an orthogonal basis for the column space of~$\symbfit{A}$.
        \item Find the projection $\symbfit{p}$ of $\symbfit{b}$ onto the column space of $\symbfit{A}$. Note that the linear system $\symbfit{Ax} = \symbfit{p}$ is consistent as $\symbfit{p}$ is in the column space of $\symbfit{A}$.
        \item Solve the linear system $\symbfit{Ax} = \symbfit{p}$ to find a least squares solution to $\symbfit{Ax} = \symbfit{b}$.
    \end{enumerate}
\end{genbox}
Note that in the above algorithm, we have established a way to derive a least square solution using the projection. This gives us motivation to consider the reverse: can we derive the projection of some vector $\symbfit{b}$ onto the column space of some matrix $\symbfit{A}$ given a least squares solution to $\symbfit{Ax} = \symbfit{b}$? To do that, we first introduce the following alternative method to compute a least squares solution:
\begin{thmbox}{An alternative way to find the least squares solution to $\symbfit{Ax} = \symbfit{b}$}{altfindlss}
    Let $\symbfit{A}$ be an $m \times n$ matrix with column space $V$ and $\symbfit{b}$ be an $m \times 1$ matrix. A vector $\symbfit{u}$ is the least squares solution to the linear system $\symbfit{Ax} = \symbfit{b}$ if and only if $\symbfit{A}^{\mathrm{T}}\symbfit{Au} = \symbfit{A}^{\mathrm{T}}\symbfit{b}$.
    \tcblower
    \begin{proof}
        Let $\symbfit{a}_i$ be the $i$-th column of $\symbfit{A}$, so $V = \symrm{span}\left\{ \, \symbfit{a}_1, \symbfit{a}_2, \symbfit{a}_3, \cdots, \symbfit{a}_n \, \right\}$ is the column space of $\symbfit{A}$.

        By Theorem \ref{thm:findlss}, $\symbfit{u}$ is a least squares solution to $\symbfit{Ax} = \symbfit{b}$ if and only if $\symbfit{Au}$ is the projection of $\symbfit{b}$ onto $V$, i.e., $\symbfit{Au - b} \perp V$. By Theorem \ref{thm:orthocond}, this is equivalent to
        \begin{displaymath}
            \symbfit{a}_i \cdot (\symbfit{Au - b}) = 0 \quad \textrm{for } i = 1, 2, 3, \cdots, n.
        \end{displaymath} 
        The above dot product is equivalent to
        \begin{align*}
            \symbf{0} & = \left[\begin{array}{c}
                \symbfit{a}_1^{\mathrm{T}} \\
                \symbfit{a}_2^{\mathrm{T}} \\
                \vdots \\
                \symbfit{a}_n^{\mathrm{T}} \\
            \end{array}\right](\symbfit{Au - b}) \\
            & = \symbfit{A}^{\mathrm{T}}(\symbfit{Au - b}),
        \end{align*}
        which simplifies to
        \begin{equation*}
            \symbfit{A}^{\mathrm{T}}\symbfit{Au} = \symbfit{A}^{\mathrm{T}}\symbfit{b}.
        \end{equation*}
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        Be mindful that in Theorem \ref{thm:altfindlss}, $\symbfit{A}^{\mathrm{T}}\symbfit{Au} = \symbfit{A}^{\mathrm{T}}\symbfit{b}$ {\color{red} \textbf{does not imply}} that $\symbfit{Au} = \symbfit{b}$ because $\symbfit{A}$ may be {\color{red} \textbf{uninvertible}}!
    \end{remark}
\end{notebox}
Recall that if $\symbfit{u}$ is a least squares solution to $\symbfit{Ax} = \symbfit{b}$, then $\symbfit{Au}$ is the projection of $\symbfit{b}$ onto the column space of $\symbfit{A}$, so we have found another way to compute the projection of a vector onto a vector space.
\begin{genbox}{An alternative algorithm to compute the projection of $\symbfit{b}$ onto $V$}
    \begin{enumerate}
        \item Write $V =\symrm{span}\left\{ \, \symbfit{a}_1, \symbfit{a}_2, \symbfit{a}_3, \cdots, \symbfit{a}_n \, \right\}$ for some arbitrary spanning set.
        \item Construct a matrix $\symbfit{A}$ with $\symbfit{a}_i$ as its $i$-th column.
        \item Find a solution $\symbfit{u}$ to the linear system $\symbfit{A}^{\mathrm{T}}\symbfit{Au} = \symbfit{A}^{\mathrm{T}}\symbfit{b}$.
        \item The projection of $\symbfit{b}$ onto $V$ is given by $\symbfit{p} = \symbfit{Au}$.
    \end{enumerate}
\end{genbox}
Furthermore, we can also consider a special case where the columns of $\symbfit{A}$ forms an {\color{red} \textbf{orthonormal}} set.
\begin{thmbox}{Least squares solution with orthonormal sets}{ortholss}
    If $\symbfit{A}$ is a matrix whose columns form an orthonormal set, then a least squares solution to $\symbfit{Ax} = \symbfit{b}$ is given by $\symbfit{u} = \symbfit{A}^{\mathrm{T}}\symbfit{b}$.
    \tcblower
    \begin{proof}
        Let $\symbfit{A} = \left[\begin{array}{ccccc}
            \symbfit{a}_1 & \symbfit{a}_2 & \symbfit{a}_3 & \cdots & \symbfit{a}_n
        \end{array}\right]$, then 
        \begin{displaymath}
            \symbfit{a}_i \cdot \symbfit{a}_j = \begin{cases}
                1 & \textrm{if } i = j \\
                0 & \textrm{otherwise}
            \end{cases}.
        \end{displaymath}
        Consider
        \begin{align*}
            \symbfit{A}^{\mathrm{T}}\symbfit{A} & = \left[\begin{array}{c}
                \symbfit{a}_1 ^{\mathrm{T}} \\
                \symbfit{a}_2 ^{\mathrm{T}}\\
                \symbfit{a}_3 ^{\mathrm{T}}\\
                \vdots \\
                \symbfit{a}_n ^{\mathrm{T}}
            \end{array}\right] \left[\begin{array}{ccccc}
                \symbfit{a}_1 & \symbfit{a}_2 & \symbfit{a}_3 & \cdots & \symbfit{a}_n
            \end{array}\right] \\
            & = \left[\begin{array}{cccc}
                \symbfit{a}_1 \cdot \symbfit{a}_1 & \symbfit{a}_1 \cdot \symbfit{a}_2 & \cdots & \symbfit{a}_1 \cdot \symbfit{a}_n \\
                \symbfit{a}_2 \cdot \symbfit{a}_1 & \symbfit{a}_2 \cdot \symbfit{a}_2 & \cdots & \symbfit{a}_2 \cdot \symbfit{a}_n \\
                \vdots & \vdots & \ddots & \vdots \\
                \symbfit{a}_n \cdot \symbfit{a}_1 & \symbfit{a}_n \cdot \symbfit{a}_2 & \cdots & \symbfit{a}_n \cdot \symbfit{a}_n \\
            \end{array}\right] \\
            & = \symbfit{I}.
        \end{align*}
        Let $\symbfit{u}$ be a least squares solution to $\symbfit{Ax} = \symbfit{b}$, then $\symbfit{u} = \symbfit{Iu} = \symbfit{A}^{\mathrm{T}}\symbfit{Au} = \symbfit{A}^{\mathrm{T}}\symbfit{b}$.
    \end{proof}
\end{thmbox}
The above theorem demonstrates yet another advantage of an orthonormal set, which leads us to the next section.

\section{Orthogonal Matrices}
\begin{dfnbox}{Orthogonal matrix}{orthomat}
    A {\color{red} \textbf{square}} matrix $\symbfit{A}$ is called an {\color{red} \textbf{orthogonal matrix}} if $\symbfit{A}^{\mathrm{T}}\symbfit{A} = \symbfit{I}$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        There are a few things worth noting from the above definition.
        \begin{enumerate}
            \item The columns of an orthogonal matrix forms an {\color{red} \textbf{orthonormal}} set.
            \item Equivalently, a square matrix $\symbfit{A}$ is an orthogonal matrix if $\symbfit{A}^{\mathrm{T}} = \symbfit{A}^{-1}$.
            \item An $n \times n$ orthogonal matrix $\symbfit{A}$ is always full-rank, i.e., $\symrm{rank}(\symbfit{A}) = n$.
        \end{enumerate}
    \end{remark}
\end{notebox}
We also have some simple properties regarding orthogonal matrices:
\begin{thmbox}{Transposes of orthogonal matrices are orthogonal}{orthotp}
    If $\symbfit{A}$ is an orthogonal matrix, then $\symbfit{A}^{\mathrm{T}}$ is an orthogonal matrix.
    \tcblower
    \begin{proof}
        Let $\symbfit{B} = \symbfit{A}^{\mathrm{T}}$, then $\symbfit{BA} = \symbfit{I}$. This means $\symbfit{A} = \symbfit{B}^{-1}$. However, $\symbfit{A} = \symbfit{B}^{\mathrm{T}}$, so $\symbfit{B}^{\mathrm{T}} = \symbfit{B}^{-1}$, i.e., $\symbfit{A}^{\mathrm{T}}$ is an orthogonal matrix.
    \end{proof}
\end{thmbox}
\begin{thmbox}{Product of orthogonal matrices is orthogonal}{orthoprod}
    If $\symbfit{A}$, $\symbfit{B}$ are orthogonal matrices, then $\symbfit{AB}$ is an orthogonal matrix.
    \tcblower
    \begin{proof}
        Consider $(\symbfit{AB})^{\mathrm{T}}(\symbfit{AB}) = \symbfit{B}^{\mathrm{T}}\symbfit{A}^{\mathrm{T}}\symbfit{AB} = \symbfit{B}^{\mathrm{T}}\symbfit{B} = \symbfit{I}$, so $\symbfit{AB}$ is an orthogonal matrix.
    \end{proof}
\end{thmbox}
Notice that the both the rows and the columns of an orthogonal matrix are linearly independent, i.e., both form an {\color{red} \textbf{orthonormal basis}} for a vector space.
\begin{thmbox}{Rows and columns of orthogonal matrices}{orthoset}
    If $\symbfit{A}$ is an $n \times n$ orthogonal matrix, then the rows and columns of $\symbfit{A}$ respectively form an orthonormal set.
    \tcblower
    \begin{proof}
        Write 
        \begin{displaymath}
            \symbfit{A} = \left[\begin{array}{cccc}
                \symbfit{c}_1 & \symbfit{c}_2 & \cdots & \symbfit{c}_n 
            \end{array}\right]
        \end{displaymath}
        where $\symbfit{c}_i$ is the $i$-th column of $\symbfit{A}$. Consider
        \begin{equation*}
            \symbfit{A}^{\mathrm{T}}\symbfit{A} = (\symbfit{c}_i \cdot \symbfit{c}_j)_{n \times n} = \symbfit{I},
        \end{equation*}
        which is equivalent to
        \begin{displaymath}
            \symbfit{c}_i \cdot \symbfit{c}_j = \begin{cases}
                1 & \textrm{if } i = j \\
                0 & \textrm{otherwise}
            \end{cases}.
        \end{displaymath}
        Therefore, the columns of $\symbfit{A}$ forms an orthonormal set.

        Write 
        \begin{displaymath}
            \symbfit{A} = \left[\begin{array}{c}
                \symbfit{r}_1 \\
                \symbfit{r}_2 \\
                \vdots \\
                \symbfit{r}_n
            \end{array}\right]
        \end{displaymath}
        where $\symbfit{r}_i$ is the $i$-th row of $\symbfit{A}$. Consider
        \begin{equation*}
            \symbfit{A}\symbfit{A}^{\mathrm{T}} = (\symbfit{r}_i \cdot \symbfit{r}_j)_{n \times n} = \symbfit{I},
        \end{equation*}
        which is equivalent to
        \begin{displaymath}
            \symbfit{r}_i \cdot \symbfit{r}_j = \begin{cases}
                1 & \textrm{if } i = j \\
                0 & \textrm{otherwise}
            \end{cases}.
        \end{displaymath}
        Therefore, the rows of $\symbfit{A}$ forms an orthonormal set.
    \end{proof}
\end{thmbox}
\begin{corbox}{A generalised statement}{genortho}
    Let $\symbfit{A}$ be an $m \times n$ matrix. The columns of $\symbfit{A}$ form an orthonormal set if and only if $\symbfit{A}^{\mathrm{T}}\symbfit{A} = \symbfit{I}_n$, and the rows of $\symbfit{A}$ form an orthonormal set if and only if $\symbfit{A}\symbfit{A}^{\mathrm{T}} = \symbfit{I}_m$.
\end{corbox}
Using the above theorems, we can easily establish a bijection the matches each orthonormal basis to a matrix. The next question is how do we then establish a transition matrix between two orthonormal bases.
\begin{thmbox}{Transition matrix between orthonormal bases}{orthotrans}
    Let $\symbfit{A}$ and $\symbfit{B}$ be matrices whose columns form the orthonormal bases for $S$ and $T$ respectively for the same vector space $V$, then $\symbfit{P} = \symbfit{B}^{\mathrm{T}}\symbfit{A}$ is the transition matrix from $S$ to $T$ and $\symbfit{Q} = \symbfit{A}^{\mathrm{T}}\symbfit{B}$ is the transition matrix from $T$ to $S$.
    \tcblower
    \begin{proof}
        Take $\symbfit{v} \in V$ to be an arbitrary vector. Note that $\symbfit{v} = \symbfit{A}[\symbfit{v}]_S = \symbfit{B}[\symbfit{v}]_T$.

        By Corollary \ref{cor:genortho}, $\symbfit{B}^{\mathrm{T}}\symbfit{B} = \symbfit{A}^{\mathrm{T}}\symbfit{A} = \symbfit{I}$, so we have:
        \begin{align*}
            [\symbfit{v}]_T & = \symbfit{B}^{\mathrm{T}}\symbfit{B}[\symbfit{v}]_T = \symbfit{B}^{\mathrm{T}}\symbfit{A}[\symbfit{v}]_S \\
            [\symbfit{v}]_S & = \symbfit{A}^{\mathrm{T}}\symbfit{A}[\symbfit{v}]_S = \symbfit{A}^{\mathrm{T}}\symbfit{B}[\symbfit{v}]_T
        \end{align*}
        Let $\symbfit{P} = \symbfit{B}^{\mathrm{T}}\symbfit{A}$ and $\symbfit{Q} = \symbfit{A}^{\mathrm{T}}\symbfit{B}$, then $\symbfit{P}$, $\symbfit{Q}$ are the transition matrices from $S$ to $T$ and from $T$ to $S$ respectively.
    \end{proof}
\end{thmbox}
\begin{thmbox}{Orthogonal transition matrices}{}
    Let $S$ and $T$ be two orthonormal bases for a vector space $V$, and let $\symbfit{P}$ be the transition matrix from $S$ to $T$, then $\symbfit{P}$ is an orthogonal matrix.
    \tcblower
    \begin{proof}
        By Theorem \ref{thm:orthotrans}, we can find matrices $\symbfit{A}$ and $\symbfit{B}$ whose columns form the bases $S$ and $T$ respectively such that $\symbfit{P} = \symbfit{B}^{\mathrm{T}}\symbfit{A}$. Consider
        \begin{equation*}
            \symbfit{P}^{\mathrm{T}}\symbfit{P} = \left(\symbfit{B}^{\mathrm{T}}\symbfit{A}\right)^{\mathrm{T}} \left(\symbfit{B}^{\mathrm{T}}\symbfit{A}\right) = \symbfit{A}^{\mathrm{T}}\symbfit{BB}^{\mathrm{T}}\symbfit{A} = \symbfit{I},
        \end{equation*}
        so $\symbfit{P}$ is an orthogonal matrix.
    \end{proof}
\end{thmbox}

\chapter{Linear Transformations}
\section{Linear Transformations in \textit{n}-Dimensional   Spaces}
So far we have encountered many types of transformations. For example, translation, scaling, reflection and rotation are geometric transformations applied to shapes in Euclidean spaces; in the previous chapters we have also seen how to transform the coordinate system (i.e. the basis of a vector space) by means of transition matrices; in fact, integration and differentiation are also transformations which are applied to functions.

All those many instances where transformations appear share one thing in common --- the transformations involved are {\color{red} \textbf{linear}}. Though stemming from the root word ``line'', the notion of linearity goes far beyond the geometric representations of lines, which we shall define rigorously as follows:
\begin{dfnbox}{Linear Transformation}{lntrans}
    A mapping between to vector spaces is called a {\color{red} \textbf{linear transformation}} if it preserves the operations of vector addition and scalar multiplication, i.e., $T
    \colon V \rightarrow W$ is a linear transformation if
    \begin{equation*}
        T(\alpha \symbfit{u} + \beta \symbfit{v}) = \alpha T(\symbfit{u}) + \beta T(\symbfit{v}) \quad \textrm{for }\symbfit{u}, \symbfit{v} \in V.
    \end{equation*}
    In particular, a linear transformation $T \colon V  \rightarrow V$ is also called a {\color{red} \textbf{linear operator}} on~$V$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        The above definition can be concisely described as: a transformation is a linear transformation if {\color{red} \textbf{the image of linear combinations equals the linear combinations of the images}}.
    \end{remark}
\end{notebox}
We also define two special linear transformations for every vector space:
\begin{dfnbox}{Identity transformation}{idtrans}
    For every vector space $V$, there is a {\color{red} \textbf{identity transformation}} $I \colon V \rightarrow V$ defined by 
    \begin{displaymath}
        I(\symbfit{v}) = \symbfit{v} \quad \textrm{for all } \symbfit{v} \in V.
    \end{displaymath} 
\end{dfnbox}
\begin{dfnbox}{Zero transformation}{zerotrans}
    For every vector space $V$, there is a {\color{red} \textbf{zero transformation}} $O \colon V \rightarrow W$ defined by 
    \begin{displaymath}
        O(\symbfit{v}) = \symbf{0}_W \quad \textrm{for all } \symbfit{v} \in V.
    \end{displaymath} 
\end{dfnbox}
It is trivial that the identity transformation is {\color{red} \textbf{unique}} for a vector space $V$.
\begin{notebox}
    \begin{remark}
        Note that the zero transformation is {\color{red} \textbf{not unique}} because its image can be the zero vector in {\color{red} \textbf{any}} vector space.
    \end{remark}
\end{notebox}
So far, the notion of linear transformations still appear a bit abstract. Let us put it into more concrete forms with some manipulations.

Let $T \colon V \rightarrow W$ be a linear transformation where $V$ and $W$ are finite-dimensional.Let $S$, $U$ be the bases for $V$ and $W$ respectively. Note that there is a {\color{red} \textbf{bijection}} between $V$ and the set $\left\{ \, [\symbfit{v}]_S \,\colon \symbfit{v} \in V = \symrm{span}(S) \, \right\}$, as well as between $W$ and the set $\left\{ \, [\symbfit{w}]_U \,\colon \symbfit{w} \in W = \symrm{span}(U) \, \right\}$. This means that we can view $T$ as the mapping $[\symbfit{v}]_S \mapsto [\symbfit{w}]_U$. 

Based on Definition \ref{dfn:lntrans}, we can immediately arrive at the following equation:
\begin{displaymath}
    T(\symbfit{v}) = T\left(\sum_{j = 1}^{n}c_j \symbfit{v}_j\right) = \sum_{j = 1}^{n}c_jT(\symbfit{v}_j), \label{Tv = sigmacTvj} \tag{$*$}
\end{displaymath}
where the $\symbfit{v}_j$'s are the vectors in $S$ and $\left\{ c_j \right\}_{j = 1}^n$ is a sequence of scalar coefficients. Note that the right-hand side of the equation is a linear combination, which means that the image of $V$ under $T$ is {\color{red} \textbf{spanned}} by the {\color{red} \textbf{images of the vectors in the basis of $V$}}. In other words, the image of the basis of $V$ under $T$, i.e., the set $\left\{\, T(\symbfit{v}_1), T(\symbfit{v}_2), \cdots, T(\symbfit{v}_n) \,\right\}$ completely determines the output of $T(\symbfit{v})$ for any $\symbfit{v} \in V$.

Let $U = \left\{ \, \symbfit{w}_1, \symbfit{w}_2, \cdots, \symbfit{w}_m \, \right\}$, then for each of the $\symbfit{v}_j$'s in the basis of $V$, we can write
\begin{equation*}
    T(\symbfit{v}_j) = \sum_{i = 1}^{m}a_{ij} \symbfit{w}_i
\end{equation*}
for some sequence of scalar coefficients $\left\{a_{ij}\right\}_{i = 1}^m$. Notice that now Equation (\ref{Tv = sigmacTvj}) can be further expanded to
\begin{align*}
    T(\symbfit{v}) & = \sum_{j = 1}^{n}c_jT(\symbfit{v}_j) \\
                   & = \sum_{j = 1}^{n}\left(c_j\sum_{i = 1}^{m}a_{ij} \symbfit{w}_i\right) \\
                   & = \sum_{j = 1}^{n}\left(\sum_{i = 1}^{m}a_{ij}c_j \symbfit{w}_i\right) \\
                   & = \sum_{i = 1}^{m}\left(\sum_{j = 1}^{n}a_{ij}c_j\right)\symbfit{w}_i. \\
\end{align*}
Suppose $T(\symbfit{v}) = \symbfit{w}$. Construct a matrix $\symbfit{M}$ in the form of
\begin{displaymath}
    \symbfit{M} = \left[\begin{array}{cccc}
        \left[T(\symbfit{v}_1)\right]_U & \left[T(\symbfit{v}_2)\right]_U & \cdots & \left[T(\symbfit{v}_n)\right]_U
    \end{array}\right].
\end{displaymath}
Note that $\symbfit{M} [\symbfit{v}]_S = \sum_{j = 1}^{n} c_j[T(\symbfit{v}_j)]_U$ is precisely $[T(\symbfit{v})]_U$. Therefore, $T$ can be described as $[\symbfit{v}]_S \mapsto \symbfit{M}[\symbfit{v}]_S$.

In particular, if we set both $S$ and $U$ to be the standard bases, we would have $\symbfit{v} = [\symbfit{v}]_S$ and $\symbfit{w} = \symbfit{w}_U$. In this case, we have:
\begin{equation*}
    T(\symbfit{v}) = \symbfit{Mv}.
\end{equation*}
In other words, every linear transformation can be represented with a matrix, which is summarised into the following theorem:
\begin{thmbox}{Standard matrix}{smat}
    Let $T \,\colon V \rightarrow W$ be a linear transformation between finite-dimensional vector spaces, and let $S$ and $U$ be bases for $V$ and $W$ respectively, then there exists a $\dim(W) \times \dim(V)$ matrix $\symbfit{M}$, known as the {\color{red} \textbf{standard matrix}}, given by
    \begin{equation*}
        \symbfit{M} = \left[\begin{array}{cccc}
            \left[T(\symbfit{v}_1)\right]_U & \left[T(\symbfit{v}_2)\right]_U & \cdots & \left[T(\symbfit{v}_n)\right]_U
        \end{array}\right],
    \end{equation*}
    where the $\symbfit{v}_i$'s are vectors in $S$, such that $[T(\symbfit{v})]_U = \symbfit{M}[\symbfit{v}]_S$ for all $\symbfit{v} \in V$. More specifically, if $S$ and $U$ are the standard bases, then $T(\symbfit{v}) = \symbfit{Mv}$.
\end{thmbox}
The above argument has proved the existence of the standard matrix for every linear transformation between finite-dimensional vector spaces. We can further prove the uniqueness of the standard matrix.
\begin{thmbox}{Uniqueness of standard matrix}{uniqsmat}
    Let $\symbfit{M}$ be the standard matrix for the linear transformation $T \,\colon V \rightarrow W$, then $\symbfit{M}$ is unique.
    \tcblower
    \begin{proof}
        Let $\dim(V) = n$ and $\dim(W) = m$. Suppose there exist $m \times n$ matrices $\symbfit{M}$ and $\symbfit{N}$ such that $T(\symbfit{v}) = \symbfit{Mv} = \symbfit{Nv}$, then we have:
        \begin{equation*}
            (\symbfit{M - N})\symbfit{v} = \symbf{0}_{m \times 1}
        \end{equation*} 
        for all $\symbfit{v} \in V$. Specifically, let $\symbfit{v}_i \,\, (i = 1, 2, 3, \cdots, n)$ be the vectors which form the standard basis for $V$, i.e., the $i1$-th entry of $\symbfit{v}_i$ is $1$ but all other entries are $0$, then we have
        \begin{equation*}
            (\symbfit{M - N})\symbfit{v}_i = \symbf{0}_{m \times 1}
        \end{equation*} 
        for $i = 1, 2, 3, \cdots, n$. Re-writing the equations into a single matrix multiplication, we have
        \begin{equation*}
            (\symbfit{M - N})\symbfit{I}_{n \times n} = \symbf{0}_{m \times n}
        \end{equation*}
        However, $(\symbfit{M - N})\symbfit{I}_{n \times n} = \symbfit{M - N}$, which means $\symbfit{M - N} = \symbf{0}_{m \times n}$, i.e., $\symbfit{M} = \symbfit{N}$. Therefore, $\symbfit{M}$ is unique.
    \end{proof}
\end{thmbox}
\section{Change of Bases}
In the previous section, we have used the coordinate vectors under bases for different vector spaces to establish the matrix representation of linear transformation. We can further expand on the notion of ``changing the bases'' onto some other applications.
\end{document}