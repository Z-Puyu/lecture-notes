\documentclass[math]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Mathematical Analysis I
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{The Real Numbers}
\section{Fields}
\begin{dfnbox}{Field}{field}
    A set $F$ with two binary operations, namely addition and multiplication, is called a {\color{red} \textbf{field}} if it satisfies the following axioms:
    \begin{enumerate}
        \item $\forall a, b \in F$, $a +_F b = b +_F a$.
        \item $\forall a, b, c \in F$, $(a +_F b) +_F c = a +_F (b +_F c)$.
        \item $\exists 0_F \in F$ such that $\forall a \in F$, $0_F +_F a = a +_F 0_F = a$.
        \item $\forall a \in F$, $\exists a' \in F$ such that $a +_F a' = 0_F$.
        \item $\forall a, b \in F$, $a \cdot_F b = b \cdot a$.
        \item $\forall a, b, c \in F$, $(a \cdot_F b) \cdot c = a \cdot_F (b \cdot_F c)$.
        \item $\forall a, b, c \in F$, $a \cdot_F (b +_F c) = a \cdot_F b +_F a \cdot_F b$ and $(a +_F b) \cdot_F c = a \cdot_F c +_F b \cdot_F c$.
        \item $\exists 1_F \in F$ such that $\forall a \in F$, $1_F \cdot_F a = a \cdot_F 1_F = a$.
        \item $\forall a \in F$, $\exists a' \in F$ such that $a \cdot_F a' = 1_F$.
    \end{enumerate}
\end{dfnbox}
If we denote addition by ``$+_F$'' and multiplication by ``$\cdot_F$'' or ``$\times_F$'', then we can denote the field over $F$ by $(F, +_F, \cdot_F)$ or $(F, +_F, \times_F)$.

Among the commonly used number sets, one may check that $\R$, $\Q$ and $\C$ are fields, while $\N$ and $\Z$ are not.
\subsection{Ordered Fields}
\begin{dfnbox}{Total Order}{totalOrder}
    A {\color{red} \textbf{total order}} on a set $X$ is a binary relation $\leq$ over $X$ such that for all $a, b, c \in X$:
    \begin{enumerate}
        \item $a \leq a$ (reflexive).
        \item $a \leq b$ and $b \leq c$ implies $a \leq c$ (transitive).
        \item $a \leq b$ and $b \leq a$ implies $a = b$ (antisymmetric).
        \item either $a \leq b$ or $b \leq a$ (strongly connected).
    \end{enumerate}
\end{dfnbox}
\begin{dfnbox}{Strict Total Order}{strictTotalOrder}
    A {\color{red} \textbf{strict total order}} on a set $X$ is a binary relation $<$ over $X$ such that for all $a, b, c \in X$:
    \begin{enumerate}
        \item $a \not < a$ (irreflexive).
        \item $a < b$ implies $b < a$ (asymmetric).
        \item $a < b$ and $b < c$ implies $a < c$ (transitive).
        \item if $a \neq b$, then either $a < b$ or $b < a$ (connected).
    \end{enumerate}
\end{dfnbox}
It is easy to see that the real numbers form the ordered fields $(\R, +, \times, \leq)$ and $(\R, +, \times, <)$. Note that this means $\R$ satisfies trichotomy. If we choose any $x \in \R$, then exactly one of~$x = 0$, $x > 0$ and $x < 0$ is true. Therefore, we can define that if $x \in \R$ and $x > 0$, then $x$ is said to be positive. This leads to the following axiomatic results:
\begin{enumerate}
    \item If $a$ and $b$ are both positive, then $a + b$ is positive;
    \item If $a$ and $b$ are both positive, then $ab$ is positive;
    \item For any $a \in \R$, either $a = 0$, $a$ is positive, or $-a$ is positive.
\end{enumerate}
Note that $a < b$ if and only if $b - a$ is positive. So the trichotomy of $\R$ guarantees that for any $a, b \in \R$, either $a = b$, $a < b$ or $b < a$ (i.e., $a > b$).

\section{Properties of $\R$}
We can derive a few obvious minor results based on the field properties of $\R$:
\begin{enumerate}
    \item If $a, b \in \R$, then $-ab + ab = 0$;
    \item For all $a \in \R$ with $a \neq 0$, $a^2 > 0$;
    \item If $a \in \R$ is such that $0 \leq a < \epsilon$ for all $\epsilon \in \R^+$, then $a = 0$;
    \item If $a < b$, then $a + c < b + c$ for all $c \in \R$.
    \item If $a < b$, then $ac < bc$ for all $c \in \R^+$ and $ac > bc$ for all $c \in \R^-$.
    \item For all $a \in \R$, $a^2 > 0$.
\end{enumerate}
We may consider the following interesting proposition:
\begin{probox}{}{infSmall}
    If $a \in \R$ is such that $0 \leq a < \epsilon$ for all $\epsilon \in \R^+$, then $a = 0$.
    \tcblower
    \begin{proof}
        Suppose on contrary that $a > 0$, then we can take $\epsilon_0 = \frac{a}{2}$. Note that $\epsilon_0 \in \R^+$ but $\epsilon_0 < a$, which is a contradiction. So $a = 0$.
    \end{proof}
\end{probox}
The above essentially asserts that \textbf{a non-negative real number is strictly less than any positive real number if and only if it is $0$}.

The properties of $\R$ also enables us to manipulate inequalities based on the following trivial results:
\begin{enumerate}
    \item If $ab > 0$, then $a$ and $b$ are either both positive or both negative;
    \item If $ab < 0$, then exactly one of them is positive and exactly one of them is negative.
\end{enumerate}
We shall introduce a few well-known inequalities.
\begin{thmbox}{Bernoulli's Inequality}{burnoulliIneq}
    If $x > -1$, then $(1 + x)^n \geq 1 + nx$ for all $n \in \N$.
    \tcblower
    \begin{proof}
        The case where $n = 0$ is trivial.
        \\\\
        Suppose that $(1 + x)^k \geq 1 + kx$ for some $k \in \N$, consider
        \begin{align*}
            (1 + x)^{k + 1} & = (1 + x)(1 + x)^k \\
            & \geq (1 + x)(1 + kx) \\
            & = 1 + (k + 1)x + kx^2 \\
            & \geq 1 + (k + 1)x.
        \end{align*}
        Therefore, $(1 + x)^n \geq 1 + nx$ for all $n \in \N$.
    \end{proof}
\end{thmbox}
\begin{thmbox}{AM-GM-HM Inequality}{AM-GM-HM}
    Let $n \in \N^+$ and let $a_1, a_2, \cdots, a_n$ be positive real numbers, then
    \begin{equation*}
        \frac{n}{\sum_{i = 1}^{n}\frac{1}{a_i}} \leq \left(\prod_{i = 1}^{n}a_i\right)^{\frac{1}{n}} \leq \frac{\sum_{i = 1}^{n}a_i}{n}.
    \end{equation*}
\end{thmbox}
\subsection{Absolute Value}
Given any real number $x$, intuitively we sense that $x$ possesses a certain ``distance'' from $0$. This distance can be formalised as follows:
\begin{dfnbox}{Absolute Value}{abs}
    Let $x \in \R$, the {\color{red} \textbf{absolute value}} of $x$ is defined as
    \begin{displaymath}
        \abs{x} = \begin{cases}
            x \quad & \textrm{if } x > 0 \\
            0 \quad & \textrm{if } x = 0 \\
            -x \quad & \textrm{if } x < 0
        \end{cases}.
    \end{displaymath}
\end{dfnbox}
We have some trivial properties about the absolute value:
\begin{enumerate}
    \item For all $a, b \in \R$, $\abs{ab} = \abs{a}\abs{b}$;
    \item For all $a \in \R$, $\abs{a}^2 = a^2$;
    \item If $c \geq 0$, then $\abs{a} \leq c$ if and only if $-c \leq a \leq c$ for all $a \in \R$;
    \item For all $a \in \R$, $-\abs{a} \leq a \leq \abs{a}$.
\end{enumerate}
Using these basic properties, we can prove the following results:
\begin{thmbox}{Triangle Inequality}{triIneq}
    For all $a, b \in \R$, $\abs{a + b} \leq \abs{a} + \abs{b}$.
\end{thmbox}
\begin{corbox}{Extended Triangle Inequality}{triIneqEx}
    For all $a, b \in \R$, $\abs{\abs{a} - \abs{b}} \leq \abs{a - b}$ and $\abs{a - b} \leq \abs{a} + \abs{b}$.
\end{corbox}
\begin{corbox}{Generalised Triangle Inequality}{triIneqGen}
    For all $a_1, a_2, \cdots, a_n \in \R$, 
    \begin{equation*}
        \abs{\sum_{i = 1}^{n}a_i} \leq \sum_{i = 1}^{n}\abs{a_i}.
    \end{equation*}
\end{corbox}
Analogously, if $\abs{x}$ represents the ``distance'' between $x$ and $0$, then by a simple translation we can see that $\abs{x - a}$ represents the ``distance'' between $x$ and $a$. Thus, we can have the following definition:
\begin{dfnbox}{Neighbourhood}{neighbourhood}
    Let $a \in \R$ and $\epsilon \in \R^+$. The {\color{red} \textbf{$\epsilon$-neighbourhood}} of $a$ is defined to be the set
    \begin{displaymath}
        V_\epsilon(a) \coloneqq \left\{x \in \R \colon \abs{x - a} < \epsilon\right\}.
    \end{displaymath}
\end{dfnbox}
Note that $x \in V_\epsilon(a)$ if and only if $-\epsilon < x - a < \epsilon$ or $a - \epsilon < x < a + \epsilon$. Which leads to the following interesting result:
\begin{probox}{}{xIsA}
    For any $a \in \R$, if $x \in V_\epsilon(a)$ for all $\epsilon \in \R^+$, then $x = a$.
    \tcblower
    \begin{proof}
        Note that this essentially means that $\abs{x - a} < \epsilon$ for all $\epsilon \in \R^+$. By Proposition \ref{pro:infSmall}, we have $\abs{x - a} = 0$ and therefore $x = a$.
    \end{proof}
\end{probox}

\subsection{The Completeness Property of $\R$}
Intuitively, there are no ``gaps'' among the real numbers, i.e., if you take any two real numbers, between them there is nothing else than other real numbers. Therefore, we say that $\R$ is \textit{complete}. This is in contrast with $\Q$ where there are gaps in between any two rational numbers (because there always exists some irrational numbers in between).

In this section, we probe into how the completeness of $\R$ can be established, and how the real numbers themselves can be constructed. To do that, we first establish the notion of \textit{boundedness}.

\begin{dfnbox}{Boundedness}{bound}
    Let $S \subseteq \R$. We say that $S$ is:
    \begin{itemize}
        \item {\color{red} \textbf{bounded above}} if there exists some $u \in R$ (known as the {\color{red} \textbf{upper bound}} of $S$) such that $u \geq s$ for all $s \in S$; 
        \item {\color{red} \textbf{bounded below}} if there exists some $v \in R$ (known as the {\color{red} \textbf{lower bound}} of $S$) such that $v \leq s$ for all $s \in S$; 
        \item {\color{red} \textbf{bounded}} if $S$ has both an upper bound and a lower bound;
        \item {\color{red} \textbf{unbounded}} either if $S$ has no upper bound or if $S$ has no lower bound;     
    \end{itemize}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $S$ is bounded if and only if there is some $M \geq 0$ such that $\abs{s} \leq M$ for all $s \in S$.
    \end{remark}
\end{notebox}

\chapter{Sequences and Series}
\section{Sequences}
Informally, a sequence is a list of enuerable numbers. This means that we can view a sequence as a mapping from an interval of $\N^+$ to $\R$. In this course, we will mainly focus on infinite sequences.
\begin{dfnbox}{Sequence}{seq}
    A {\color{red} \textbf{sequence}} in $\R$ is a real-valued function $X \colon \N^+ \to \R$. Where $X(n)$ is called the $n$-th {\color{red} \textbf{term}} of the sequence.
\end{dfnbox}
By convention, we denote $X(n)$ by $x_n$, and the sequence $X$ by $(x_n)$ or $(x_n \colon n \in \N^+)$.

Alternatively, a sequence $(x_n)$ may be defined in the following manner: first, we define the value of $x_1$. Secondly, we define a mapping $(x_1, x_2, \cdots, x_n) \mapsto x_{n + 1}$. Sequences defined in this way are said to be \textbf{inductively} and \textbf{recursively} defined.

\subsection{Limits of Sequences}
As $n$ becomes very large, a sequence may exhibit certain limiting behaviour.
\begin{dfnbox}{Convergence of Sequences}{seqConverge}
    A sequence $(x_n)$ defined in $\R$ is said to be {\color{red} \textbf{convergent}} to $x$ if for all $\epsilon > 0$, there exists some~$N \in \N$ such that whenever $n \geq N$, $\abs{x_n - x} < \epsilon$. $x$ is known as the {\color{red} \textbf{limit}} of $(x_n)$, denoted as
    \begin{equation*}
        \lim_{n \to \infty}x_n = x.
    \end{equation*}
\end{dfnbox}
A sequence which is not convergent is said to be \textit{divergent}.

Intuitively, a sequence can not converge to different values concurrently. This idea can be formulated formally as follows:
\begin{thmbox}{Uniqueness of Limits}{uniqueLimit}
    If $(x_n)$ converges, then its limit is unique.
    \tcblower
    \begin{proof}
        Suppose that $x$ and $x'$ are both limits of $(x_n)$. For all $\epsilon > 0$, there exists $N_1 \in \N^+$ such that
        \begin{equation*}
            \abs{x_n - x} < \frac{\epsilon}{2}
        \end{equation*}
        whenever $n \geq N_1$ and there exists $N_2 \in \N^+$ such that
        \begin{equation*}
            \abs{x_n - x'} < \frac{\epsilon}{2}
        \end{equation*}
        whenever $n \geq N_2$. Take $N = \max\{N_1, N_2\}$, then for all $n \geq N$,
        \begin{align*}
            \abs{x - x'} & = \abs{x - x_n + x_n - x'} \\
            & \leq \abs{x_n - x} + \abs{x_n - x'} \\
            & < \frac{\epsilon}{2} + \frac{\epsilon}{2} \\
            & = \epsilon
        \end{align*}
        for all $\epsilon > 0$. By Proposition \ref{pro:infSmall}, $x - x' = 0$, i.e., $x = x'$. This means that $\lim_{n \to \infty}x_n$ is unique.
    \end{proof}
\end{thmbox}
Given sequences $(x_n)$ and $(y_n)$, we can form new sequences by applying arithmetic operations onto them, and we can relate the limits of these new sequences with the limits of $(x_n)$ and $(y_n)$.
\begin{thmbox}{Limit Laws}{limitLaws}
    If $\lim_{n \to \infty}x_n = x$ and $\lim_{n \to \infty}y_n = y$, then
    \begin{enumerate}
        \item $\lim_{n \to \infty}(x_n + y_n) = x + y$.
        \item $\lim_{n \to \infty}(x_ny_n) = xy$.
        \item $\lim_{n \to \infty}(cx_n) = cx$ for all $c \in \R$.
        \item $\lim_{n \to \infty}\left(\frac{x_n}{y_n}\right) = \frac{x}{y}$.
    \end{enumerate}
\end{thmbox}
In some cases, it may not be easy to prove the existence of limit or compute it directly for a sequence. Thus, the following may be useful:
\begin{thmbox}{Squeeze Theorem}{squeeze}
    Let $(x_n)$, $(y_n)$ and $(z_n)$ be sequences such thatr $x_n \leq y_n \leq z_n$ for all $n \in \N^+$. If $(x_n)$ and $(z_n)$ both converge and $\lim_{n \to \infty}x_n = \lim_{n \to \infty}z_n = \ell$, then $(y_n)$ converges and
    \begin{equation*}
        \lim_{n \to \infty}y_n = \ell.
    \end{equation*}
    \tcblower  
    \begin{proof}
        Let $\epsilon > 0$ be an arbitrary real number. Note that there exists $N \in \N^+$ such that for all $n \geq N$,
        \begin{equation*}
            \abs{x_n - \ell} < \epsilon, \qquad \abs{z_n - \ell} < \epsilon.
        \end{equation*}
        Therefore,
        \begin{equation*}
            -\epsilon < x_n - \ell \leq y_n - \ell \leq z_n - \ell < \epsilon,
        \end{equation*}
        which implies that
        \begin{equation*}
            \abs{y_n - \ell} < \epsilon
        \end{equation*}
        for all $n \geq N$. Therefore, $\lim_{n \to \infty}y_n = \ell$.
    \end{proof}
\end{thmbox}
Notice that the limit of a sequence essentially ``bounds'' the sequence. This motivates us to investigate the relation between a convergent sequence and the its bounds.
\begin{dfnbox}{Boundedness of Sequences}{seqBound}
    A sequence $(x_n)$ in $\R$ is {\color{red} \textbf{bounded}} if there exists some $M \in \R^+$ such that $\abs{x_n} \leq M$ for all $n \in \N^+$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $(x_n)$ is bounded if and only if the set $\left\{x_n \colon n \in \N^+\right\}$ is bounded.
    \end{remark}
\end{notebox}
\begin{thmbox}{Boundedness of Convergent Sequences}{convergeSeqBound}
    A sequence $(x_n)$ is bounded if it is convergent.
    \tcblower
    \begin{proof}
        Let $\lim_{n \to \infty}x_n = x$. Note that there exists some $N \in \N^+$ such that whenever~$n \geq N$, $\abs{x_n - x} < 1$. Consider
        \begin{align*}
            \abs{x_n} & = \abs{x_n - x + x} \\
            & \leq \abs{x_n - x} + \abs{x} \\
            & < 1 + \abs{x}.
        \end{align*}
        Let
        \begin{equation*}
            M \coloneqq \sup \big\{\abs{x_1}, \abs{x_2}, \cdots, \abs{x_{N - 1}}, 1 + \abs{x}\big\},
        \end{equation*}
        then $\abs{x_n} \leq M$ for all $n \in \N^+$, and so $(x_n)$ is bounded.
    \end{proof}
\end{thmbox}
The contrapositive statement to the above theorem concludes that \textbf{any unbounded sequence must be divergent}. However, note that the converse of Theorem \ref{thm:convergeSeqBound} is not true in general! As a counter example, consider $x_n = (-1)^n$.
\begin{probox}{}{positiveLimit}
    Let $(x_n)$ be a convergent sequence. If $x_n \geq 0$ for all $n \in \N^+$, then $\lim_{n \to \infty}x_n \geq 0$.
    \tcblower
    \begin{proof}
        Let $\lim_{n \to \infty}x_n = x$. Suppose on contrary $x < 0$, then $-x > 0$. Therefore, there exists some $N \in \N^+$ such that whenever $n \geq N$, $\abs{x_n - x} < -x$. This means that for all $n \geq N$,
        \begin{equation*}
            x_n < x - x = 0.
        \end{equation*}
        However, this is a contradiction, so $\lim_{n \to \infty}x_n = x > 0$.
    \end{proof}
\end{probox}
Here are two simple corollaries from Proposition \ref{pro:positiveLimit}, the proofs of which are left to the reader as an exercise.
\begin{corbox}{}{limitCorOne}
    If $(x_n)$ and $(y_n)$ are convergent sequences with $x_n \geq y_n$ for all $n \in \N^+$, then
    \begin{equation*}
        \lim_{n \to \infty}x_n \geq \lim_{n \to \infty}y_n.
    \end{equation*}
\end{corbox}
\begin{corbox}{}{limitCorTwo}
    If $(x_n)$ is a convergent sequence with $a \leq x_n \leq b$ for all $n \in \N^+$, then
    \begin{equation*}
        a \leq \lim_{n \to \infty}x_n \leq b.
    \end{equation*}
\end{corbox}
In casual languages, we may be tempted to describe the limit of a sequence as ``a value to which the terms can get as close as possible, but which is never surpassed''. This intuition gives us an idea to prove convergence for a bounded sequence.
\begin{dfnbox}{Monotone Sequences}{monotone}
    Let $(x_n)$ be a sequence. $(x_n)$ is said to be {\color{red} \textbf{increasing}} if $x_i \geq x_j$ whenever $i \geq j$, and {\color{red} \textbf{decreasing}} if $x_i \leq x_j$ whenever $i \geq j$. A sequence is said to be {\color{red} \textbf{monotone}} if it is either increasing or decreasing.
\end{dfnbox}
Note that an increasing sequence is the same as an non-decreasing sequence and vice versa. Recall that we have stated that the converse of Theorem \ref{thm:convergeSeqBound} is not true in general, but if we impose an additional constraint on the monotonicity of the bounded sequence, we will get a stronger condition.
\begin{thmbox}{Monotone Convergence Theorem}{monotoneConverge}
    Let $(x_n)$ be a monotone sequence in $\R$, then $(x_n)$ converges if and only if it is bounded.
    \tcblower
    \begin{proof}
        Suppose $(x_n)$ is convergent, then it follows from Theorem \ref{thm:convergeSeqBound} that it is bounded.
        \\\\
        Suppose conversely that $(x_n)$ is bounded. Without loss of generality, assume that $(x_n)$ is increasing, so $(x_n)$ has an upper bound. Let $\sup (x_n) = x$ and let $\epsilon > 0$ be an arbitrary real number. Note that $x - \epsilon$ is not an upper bound for $(x_n)$, so there exists some $x_N \in (x - \epsilon, x]$, which means that $0 \leq x - x_N < \epsilon$. Since $(x_n)$ is increasing, for all $n \geq N$, we have $x \geq x_n \geq x_N$, so
        \begin{equation*}
            0 \leq x - x_n \leq x - x_N < \epsilon.
        \end{equation*}
        Therefore, $\abs{x - x_n} < \epsilon$ for all $\epsilon > 0$ whenever $n \geq N$, and so $\lim_{n \to \infty}x_n = x$.
    \end{proof}
\end{thmbox}
A classic application of Theorem \ref{thm:monotoneConverge} is an approximation of $\sqrt{2}$.
\begin{exbox}{Mesopotamian Approximation of $\sqrt{2}$}{mesoApproxSqrt2}
    Define $(x_n)$ such that $x_1 = 2$ and $x_{n + 1} = \frac{1}{2}\left(x_n + \frac{2}{x_n}\right)$. Show that $\lim_{n \to \infty}x_n = \sqrt{2}$.
\end{exbox}
\subsection{Subsequences}
Recall that the sequence $x_n = (-1)^n$ is divergent. However, suppose we were to take all the odd terms from $(x_n)$ to form a new sequence, and to take all the even terms to form another new sequence. One would realise that both new sequences are convergent. This motivates us to study ``a part'' of a sequence as a new sequence.
\begin{dfnbox}{Subsequence}{subseq}
    Let $(x_n)$ be a sequence in $\R$ and let 
    \begin{displaymath}
        n_1 < n_2 < n_3 < \cdots < n_k < \cdots
    \end{displaymath}
    be an infinite sequence of strictly increasing positive integers, then the sequence $\left(x_{n_k}\right)$ is called a {\color{red} \textbf{subsequence}} of $(x_n)$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that a sequence is always a subsequence of itself.
    \end{remark}
\end{notebox}
Intuitively, if a sequence is convergent, then any of its subsequences should be convergent, too.
\begin{thmbox}{Convergence of Subsequences}{convergeSubseq}
    Let $(x_n)$ be a convergent sequence with $\lim_{n \to \infty}x_n = x$, then for any subsequence $(x_{n_k})$,
    \begin{equation*}
        \lim_{n_k \to \infty}x_{n_k} = \lim_{k \to \infty}x_{n_k} = x.
    \end{equation*}
    \tcblower
    \begin{proof}
        Note that for any $\epsilon > 0$, there exists some $N \in \N$ such that $\abs{x_n - x} < \epsilon$ for all $n \geq N$. Observe that $n_k \geq k$, so whenever $k \geq N$, we have $n_k \geq N$, and so
        \begin{equation*}
            \abs{x_{n_k} - x} < \epsilon
        \end{equation*}
        whenever $k \geq N$, i.e., 
        \begin{equation*}
            \lim_{n_k \to \infty}x_{n_k} = \lim_{k \to \infty}x_{n_k} = x.
        \end{equation*}
    \end{proof}
\end{thmbox}
Theorems \ref{thm:convergeSubseq} and \ref{thm:uniqueLimit} give rise to the following corollary. The proof is left to the reader as an exercise.
\begin{corbox}{}{subseqTwoLimits}
    Let $(x_n)$ be a sequence, then $(x_n)$ is divergent if there exists two subsequences $(x_{n_k})$ and $(x_{n_h})$ such that 
    \begin{equation*}
        \lim_{k \to \infty}x_{n_k} \neq \lim_{h \to \infty}x_{n_h}.
    \end{equation*}
\end{corbox}
We may apply Theorem \ref{thm:monotoneConverge} with respect to subsequences. Let us first introduce the notion of \textit{peak points}.
\begin{dfnbox}{Peak Point}{peakPt}
    Let $(x_n)$ be a sequence in $\R$, $x_m$ is called a {\color{red} \textbf{peak}} if for all $n \in \N$ with $n > m$, $x_m \geq x_n$.
\end{dfnbox}
Next, we shall prove that one can find a monotone subsequence from every sequence.
\begin{thmbox}{Existence of Monotone Subsequences}{existMonoSubseq}
    Every infinite sequence has an infinite monotone subsequence.
    \tcblower
    \begin{proof}
        Let $(x_n)$ be any sequence in $\R$. We consider the following cases:
        \\\\
        \textit{Case 1.} $(x_n)$ has infinitely many peak points.
        
        This means that there exists infinitely many $m_1, m_2, \cdots \in \N$ such that $m_j > m_i$ whenever $j > i$. Therefore, the subsequence $\left(x_{m_n}\right)$ is a monotone decreasing sequence.
        \\\\
        \textit{Case 2.} $(x_n)$ has finitely many peak points.
        
        This means that there exists $m_1, m_2, \cdots, m_k\in \N$ such that $x_{m_1}, x_{m_2}, \cdots, x_{m_k}$ are all the peak points of $(x_n)$. Take $N = m_k + 1$, then for all $n_{i} \geq N$, since $x_{n_i}$ is not a peak point, there exists some $n_{i + 1} > n_i$ such that $x_{n_{i + 1}} > x_{n_i}$. Therefore, $\left(x_{n_i}\right)$ is an increasing sequence.
    \end{proof}
\end{thmbox}
With these preparations done, we state the following theorem:
\begin{thmbox}{Bolzano-Weierstrass Theorem (simplified ver.)}{bolzanoWeierstrass}
    Every bounded sequence has a convergent subsequence.
    \tcblower
    \begin{proof}
        Let $(x_n)$ be a bounded sequence, then by Theorem \ref{thm:existMonoSubseq} we can find some subsequence $\left(x_{n_k}\right)$ which is monotone. Note that $\left(x_{n_k}\right)$ is also bounded, so by Theorem \ref{thm:monotoneConverge} it is convergent.
    \end{proof}
\end{thmbox}

\subsection{Cauchy Criterion}
Intuitively, if a sequence is convergent, then over a large interval of $\N$, the change in values of its terms will become smaller and smaller. Correspondingly, this means that the adjacent terms of the sequence will get closer and closer as $n$ becomes large.
\begin{dfnbox}{Cauchy Sequence}{cauchySeq}
    A sequence $(x_n)$ is said to be a {\color{red} \textbf{Cauchy sequence}} if for every $\epsilon > 0$, there exists some $H \in \N$ such that for all $n, m \in \N$ with $n, m \geq H$, $\abs{x_n - x_m} < \epsilon$.
\end{dfnbox}
We can make use of the Cauchy sequence to test for convergence.
\begin{thmbox}{Cauchy Convergence Criterion}{cauchyConvergeCri}
    A sequence in $\R$ is convergent if and only if it is a Cauchy sequence.
    \tcblower
    \begin{proof}
        Let $(x_n)$ be a sequence in $\R$. Suppose that $(x_n)$ converges to $x$, then for all~$\epsilon > 0$, there exists some $N \in \N$ such that whenever $n > N$, $\abs{x_n - x} < \frac{\epsilon}{2}$. Therefore, for all~$m, n > N$, we have
        \begin{align*}
            \abs{x_m - x_n} & = \abs{x_m - x - x_n + x} \\
            & \leq \abs{x_m - x} + \abs{x_n - x} \\
            & = \frac{\epsilon}{2} + \frac{\epsilon}{2} \\
            & = \epsilon,
        \end{align*}
        and so $(x_n)$ is a Cauchy sequence.
        \\\\
        Suppose conversely that $(x_n)$ is a Cauchy sequence on $\R$. We consider the following lemma:
        \begin{lembox}{Boundedness of Cauchy Sequences}{cauchySeqBound}
            A Cauchy sequence in $\R$ is bounded.
            \tcblower
            \begin{proof}
                Let $(x_n)$ be a Cauchy sequence, then by Definition \ref{dfn:cauchySeq}, there exists some~$H \in \N$ such that for all natural numbers $n \geq H$, $\abs{x_n - x_H} < 1$. By Corollary \ref{cor:triIneqEx}, we have
                \begin{equation*}
                    \big\lvert\abs{x_n} - \abs{x_H}\big\rvert \leq \abs{x_n - x_H} < 1,
                \end{equation*}
                and so $\abs{x_n} < \abs{x_H} + 1$. Take 
                \begin{equation*}
                    m = \max\big\{\abs{x_1}, \abs{x_2}, \cdots, \abs{x_H}, \abs{x_H} + 1\big\},
                \end{equation*}
                then $\abs{x_n} < m$ for all $n \in \N^+$.
            \end{proof}
        \end{lembox}
        Therefore, by Theorem \ref{thm:bolzanoWeierstrass} there exists a subsequence $\left(x_{m_n}\right)$ which converges to some $x \in \R$. Thus, there exists some $M \in \N$ such that $\abs{x_{m_n} - x} < \frac{\epsilon}{2}$ for all~$\epsilon > 0$ whenever $m_{n} > M$. By Definition \ref{dfn:cauchySeq}, there exists some $N \in \N$ such that~$\abs{x_n - x_{m_n}} < \frac{\epsilon}{2}$ for all $\epsilon > 0$ and for all $n, m_n > N$. Take $K = \max\{M, N\}$, then whenever $n > K$, there is some $m_n > K$ such that
        \begin{align*}
            \abs{x_n - x} & = \abs{x_n - x_{m_n} + x_{m_n} - x} \\
            & \leq \abs{x_n - x_{m_n}} + \abs{x_{m_n} - x} \\
            & = \frac{\epsilon}{2} + \frac{\epsilon}{2} \\
            & = \epsilon.
        \end{align*}
        Therefore, $\lim_{n \to \infty}x_n = x$.
    \end{proof}
\end{thmbox}

\chapter{Functions}
\section{Limits of Functions}
As a priliminary concept, we shall introduce the notion of a \textit{cluster point}.
\begin{dfnbox}{Cluster Point}{clusterPt}
    Let $A \subseteq \R$. A point $c$ is called a {\color{red} \textbf{cluster point}} of $A$ if for all $\delta > 0$, there exists at least one $x \in A$ such that $0 < \abs{x - c} < \delta$, i.e., $\bigl(V_\delta(c) - \{c\}\bigr) \cap A \neq \varnothing$ for all $\delta > 0$.
\end{dfnbox}
Intuitively, this means that the elements of a set $A$ is \textbf{densely distributed} around the cluster point, which motivates the following alternative definition:
\begin{thmbox}{Alternative Definition of Cluster Points}{altClusterPt}
    Let $A \subseteq \R$, then $c \in \R$ is a cluster point of $A$ if and only if there exists a sequence $(a_n)$ in~$A$ such that $\lim_{n \to \infty}a_n = c$ and $a_n \neq c$ for all $n \in \N$.
    \tcblower
    \begin{proof}
        Suppose that $c$ is a cluster point of $A$. Fix any $n \in \N^+$, then there exists some $a_n \in A$ such that $0 < \abs{a_n - c} < \frac{1}{n}$. This means we can obtain a sequence $(a_n)$ with $a_n \neq c$ for all $n \in \N^+$.
        \\\\
        For any $\epsilon > 0$, note that there exists some $N \in \N^+$ such that $0 < \frac{1}{N} < \epsilon$. Therefore, for all $n \geq N$, we have $\abs{a_n - c} < \frac{1}{n} \leq \frac{1}{N} < \epsilon$, which means that $\lim_{n \to \infty}a_n = c$.
        \\\\
        Conversely, suppose that there is a sequence $(a_n)$ in $A$ with $a_n \neq c$ for all $n \in \N^+$ and $\lim_{n \to \infty}a_n = c$. For any $\delta > 0$, there is some $N \in \N^+$ such that for all $n \geq N$, $0 < \abs{a_n - c} < \delta$. Note that $a_n \in A$, so $c$ is a cluster point of $A$.
    \end{proof}
\end{thmbox}
Analogously to the limit of sequences, we may describe the limiting behaviour of a function as the follows:
\begin{quote}
    As $x$ gets arbitrarily close to some point $c$, the function value $f(x)$ can get as close to a constant $L$ as possible.
\end{quote}
The notion of $x$ being ``arbitrarily close to $c$'' can be precisely captured by a cluster point. The above intuition is formulated formally as follows:
\begin{dfnbox}{Limit of Functions}{funcLim}
    Let $f$ be a function over some $A \subseteq \R$. If $c$ is a cluster point of $A$, then $L \in \R$ is called the {\color{red} \textbf{limit}} of $f$ at $c$ if for all $\epsilon > 0$, there exists some $\delta \in \R$ such that whenever $0 < \abs{x - c} < \delta$, $\abs{f(x) - L} < \epsilon$. 
\end{dfnbox}
It is worth noting that a cluster point $c$ of $A$ may not belong to $A$ itself. Therefore, $f$ can have a limit at $c$ even if it is not defined at $c$. Conversely, even if $f$ is defined at $c$, the value $f(c)$ is unrelated to $\lim_{x \to c}f(x)$ in general.

Similar to sequences and series, the limit of a function is unique.
\begin{thmbox}{Uniqueness of Limit of Functions}{funcLimUnique}
    Let $f$ be a function over some $A \subseteq \R$ and $c$ be a cluster point of $A$. If $\lim_{x \to c}f(x)$ exists, then it is unique.
    \tcblower
    \begin{proof}
        Suppose there are $L, L' \in \R$ to which $f$ converges at $c$, then for any $\epsilon > 0$, there exists $\delta_1$ such that whenever $0 < \abs{x - c} < \delta_1$, $\abs{f(x) - L} < \frac{\epsilon}{2}$ and there exists $\delta_2$ such that whenever $0 < \abs{x - c} < \delta_2$, $\abs{f(x) - L} < \frac{\epsilon}{2}$.
        \\\\
        Take $\delta = \min\{\delta_1, \delta_2\}$, then whenever $0 < \abs{x - c} < \delta$, we have
        \begin{align*}
            \abs{L - L'} & = \abs{f(x) - L' - f(x) + L} \\
            & \leq \abs{f(x) - L'} + \abs{f(x) - L} \\
            & < \epsilon
        \end{align*}
        for all $\epsilon > 0$. Therefore, $L = L'$, i.e., $\lim_{x \to c}f(x)$ is unique.
    \end{proof}
\end{thmbox}
We now proceed to discussing the boundedness of functions.
\begin{thmbox}{Boundedness of Functions}{funcBound}
    Let $f$ be a function over some $A \subseteq \R$. If $\lim_{x \to c}f(x)$ exists for some cluster point $c$ of $A$, then $f$ is bounded in some neighbourhood of $c$.
    \tcblower
    \begin{proof}
        Let $\lim_{x \to c}f(x) = L$. Note that there exists some $\delta > 0$ such that whenever $x \in V_\delta(c)$, $\abs{f(x) - L} < 1$. Consider
        \begin{align*}
            \abs{f(x)} & = \abs{f(x) - L + L} \\
            & \leq \abs{f(x) + L} + \abs{L} \\
            & < 1 + \abs{L}.
        \end{align*}
        Take
        \begin{equation*}
            M = \begin{cases}
                1 + \abs{L} & \quad \textrm{if } c \notin A \\
                \max\{f(c), 1 + \abs{L}\} & \quad \textrm{if } c \in A
            \end{cases},
        \end{equation*}
        then $\abs{f(x)} < M$ for all $x \in V_\delta(c)$, i.e., $f$ is bounded in $V_\delta(c)$.
    \end{proof}
\end{thmbox}
Now suppose $\lim_{x \to c}f(x) = L$, then it is easy to visualise that the value of $f(x)$ approaches $L$ indefinitely. Therefore, if we ``choose'' infinitely many values of $f$ near $c$, we naturally obtain a convergent sequence.
\begin{thmbox}{Sequential Criterion of Limits}{seqCriterion}
    Let $f$ be a function over some $A \subseteq \R$ and $c$ be a cluster point of $A$. $\lim_{x \to c}f(x) = L$ if and only if for every sequence $(x_n)$ in $A$ that converges to $c$, the sequence $\bigl(f(x_n)\bigr)$ converges to $L$.
    \tcblower
    \begin{proof}
        Suppose $\lim_{x \to c}f(x) = L$, then for all $\epsilon > 0$, there exists some $\delta > 0$ such that $\abs{f(x) - L} < \epsilon$ whenever $0 < \abs{x - c} < \delta$.
        \\\\
        Let $(x_n)$ be a sequence with $\lim_{n \to \infty}x_n = c$, then there exists some $N \in \N$ such that whenever $n \geq N$, $\abs{x_n - c} < \delta$. Therefore, $\abs{f(x_n) - L} < \epsilon$ for all $n \geq N$, and so $\lim_{n \to \infty}f(x_n) = L$.
        \\\\
        We shall prove the converse by considering its contrapositive. If $f$ converges to $L' \neq L$, then by the previous argument every sequence $(x_n)$ converging to $c$ is such that $\bigl(f(x_n)\bigr)$ converges to $L' \neq L$. 
        \\\\
        If $f$ diverges at $c$, then there exists some $\epsilon_0 > 0$ such that for all $\delta > 0$, there is some $x_0$ with $0 < \abs{x - c} < \delta$ such that $\abs{f(x_0) - L} \geq \epsilon_0$. Therefore, for all $n \in \N$, we can take some $x_n \in V_{1 / n}(c)$ such that $\abs{f(x_n) - L} \geq \epsilon_0$. Either way, there exists a sequence $(x_n)$ converging to $c$ such that $\bigl(f(x_n)\bigr)$ does not converge to $L$.
    \end{proof}
\end{thmbox}
Let $\lim_{x \to c}f(x) = L$ and $\lim_{x \to c}g(x) = M$. By using Theorem \ref{thm:seqCriterion}, we obtain the following limit laws:
\begin{enumerate}
    \item $\lim_{x \to c}\bigl(f(x) + g(x)\bigr) = L + M$.
    \item $\lim_{x \to c}\bigl(f(x)g(x)\bigr) = LM$.
    \item $\lim_{x \to c}\frac{f(x)}{g(x)} = \frac{L}{M}$ if $M \neq 0$.
\end{enumerate}
Additionally, Theorem \ref{thm:squeeze} also applies for limits of functions.
\subsection{One-Sided Limits}
In constrast to limits of sequences, for a function, $x$ can approach the cluster point $c$ from different directions. This may result in different limiting behaviours.
\begin{dfnbox}{One-Sided Limit}{oneSideLim}
    Let $f$ be a function over some $A \subseteq \R$ and $c$ be a cluster point of $A$. If for all $\epsilon > 0$, there exists some $\delta > 0$ such that $\abs{f(x) - L} < \epsilon$ whenever $0 < x - c < \delta$, then $L = \lim_{x \to c^+}f(x)$ is called the {\color{red} \textbf{right-hand limit}} of $f$ at $c$.
    \\\\
    If for all $\epsilon > 0$, there exists some $\delta > 0$ such that $\abs{f(x) - L} < \epsilon$ whenever $-\delta < x - c < 0$, then $L = \lim_{x \to c^-}f(x)$ is called the {\color{red} \textbf{left-hand limit}} of $f$ at $c$.
\end{dfnbox}
By restricting the values of $x_n$ to either $(-\infty, c)$ or $(c, \infty)$, we obtain the one-sided version of Theorem \ref{thm:seqCriterion}. One can check that the limit laws still hold for one-sided limits. Note that for $f$ to have a limit at $c$, its limits from both sides must be consistent.
\begin{thmbox}{One-Sided Limits and Limit}{}
    $\lim_{x \to c}f(x)$ exists if and only if both $\lim_{x \to c^+}f(x)$ and $\lim_{x \to c^-}f(x)$ exists and are equal.
\end{thmbox}
\section{Continuity}
Previously we have stated that the existence and value of $\lim_{x \to c}f(x)$ is unrelated to $f(c)$. However, when it is indeed that case that $f(c) = \lim_{x \to c}f(x)$, it is intuitive to think that $f$ has no gaps around $c$, which motivates us to define a \textit{continuous function}.
\begin{dfnbox}{Continuity}{continuity}
    Let $f$ be a function over some $A \subseteq \R$. $f$ is said to be {\color{red} \textbf{continuous}} at $c \in A$ if for all $\epsilon > 0$, there exists some $\delta > 0$ such that $\abs{f(x) - f(c)} < \epsilon$ whenever $\abs{x - c} < \delta$. Otherwise, $f$ is {\color{red} \textbf{discontinuous}} at $c$. If $f$ is continuous at all $x \in A$, we say that $f$ is continuous on $A$.
\end{dfnbox}
Specially, we consider the situation where $c$ is not a cluster point of $A$, i.e., $c$ is ``isolated''. In this case $f$ is continuous at $c$ trivially, but such continuity is of less interest.

If we wish to prove a function $f$ is continuous over a closed interval $[a, b]$, then other than proving $\lim_{x \to c}f(x) = f(c)$ for all $c \in (a, b)$, we also need to make sure $\lim_{x \to a^+}f(x) = f(a)$ and $\lim_{x \to b^-}f(x) = f(b)$.

We can make use of Theorem \ref{thm:seqCriterion} to prove continuity, the proof of which is left to the reader as an exercise.
\begin{thmbox}{Sequential Criterion of Continuity}{seqCriterionContinuity}
    Let $f$ be a function over some $A \subseteq \R$. $f$ is continuous at $c \in A$ if and only if for every sequence $(x_n)$ in $A$ that converges to $c$, the sequence $\bigl(f(x_n)\bigr)$ converges to $f(c)$.
\end{thmbox}
By negating the above, we derive a simple corollary:
\begin{corbox}{Discontinuous Criterion}{disconCriterion}
    Let $f$ be a function over some $A \subseteq \R$. $f$ is discontinuous at $c \in A$ if and only if there is a sequence $(x_n)$ in $A$ that converges to $c$ such that the sequence $\bigl(f(x_n)\bigr)$ does not converge to $f(c)$.
\end{corbox}
If $f$ and $g$ are continuous at $c$, using limit laws, we can easily prove the following results:
\begin{enumerate}
    \item $f \pm g$ is continuous at $c$.
    \item $fg$ is continuous at $c$.
    \item $kf$ is continuous at $c$ for all $k \in \R$.
    \item $\frac{f}{g}$ is continuous at $c$ if $g(c) \neq 0$.
\end{enumerate}
In particular, we also have the following conclusion:
\begin{thmbox}{Continuity of Composite Functions}{compFuncContinuity}
    Let $f \colon A \to \R$, $g \colon B \to \R$ be functions with $f(A) \subseteq B$. If $f$ is continuous at $c$ ad $g$ is continuous at $b = f(c)$, then $g \circ f$ is continuous at $c$.
    \tcblower
    \begin{proof}
        Since $g$ is continuous at $b$, for all $\epsilon > 0$ there exists some $\delta > 0$ such that $\abs{g(x) - g(b)} < \epsilon$ whenever $\abs{x - b} < \delta$.
        \\\\
        Since $f$ is continuous at $c$, there is some $\gamma > 0$ such that $\abs{f(x) - f(c)} < \delta$ whenever $\abs{x - c} < \gamma$. Therefore, for all $\epsilon > 0$, whenever $\abs{x - c} < \gamma$, we have $\abs{g\bigl(f(x)\bigr) - g\bigl(f(c)\bigr)} < \epsilon$, and so $g \circ f$ is continuous at $c$.
    \end{proof}
\end{thmbox}
Recall that in Theorem \ref{thm:funcBound}, we obtain some boundedness properties using the limiting behaviour of a function. For a continuous function, we can derive some more specific conclusion.
\begin{thmbox}{Boundedness Theorem}{boundThm}
    If $f$ is continuous on $[a, b]$, then $f$ is bounded on $[a, b]$.
    \tcblower
    \begin{proof}
        Suppose on contrary $f$ is unbounded on $[a, b]$, then for all $n \in \N$, there is some $x_n \in [a, b]$ such that $\abs{f(x_n)} > n$. Note that by this way we obtain a sequence $(x_n)$ bounded on $[a, b]$. By Theorem \ref{thm:bolzanoWeierstrass}, this means that $(x_n)$ has a convergent subsequence $\left(x_{n_k}\right)$.
        \\\\
        Let $\lim_{k \to \infty}x_{n_k} = c$, then $c \in [a, b]$. Since $f$ is continuous on $[a, b]$, by Theorem \ref{thm:seqCriterionContinuity}, $\Bigl(f\left(x_{n_k}\right)\Bigr)$ converges to $f(c)$. However,
        \begin{equation*}
            \abs{f\left(x_{n_k}\right)} > n_k \geq k
        \end{equation*}
        for all $k \in \N$, which means that $\Bigl(f\left(x_{n_k}\right)\Bigr)$ is divergent. This is a contradiction.
    \end{proof}
\end{thmbox}
Intuitively, if $f$ is bounded on $[a, b]$, it cannot increase nor decrease indefinitely in $[a, b]$ and so it must attain a maximum and a minimum value somewhere in the interval.
\begin{thmbox}{Maximum-Minimum Theorem}{maxMin}
    If $f$ is continuous on $[a, b]$, then there exists some $x^* \in [a, b]$ such that $f(x^*) \geq f(x)$ for all $x \in [a, b]$ and there exists some $x_* \in [a, b]$ such that $f(x_*) \leq f(x)$ for all $x \in [a, b]$.
    \tcblower
    \begin{proof}
        It suffices to prove the existence of $x^*$. The existence of $x_*$ can be proved similarly by considering symmetry. By Theorem \ref{thm:boundThm}, $\sup f([a, b])$ exists. Let $M = \sup f([a, b])$, then for all $n \in \N$, there exists some $x_n \in [a, b]$ such that
        \begin{equation*}
            M - \frac{1}{n} < f(x_n) \leq M.
        \end{equation*}
        By Theorem \ref{thm:squeeze}, $\lim_{n \to \infty}f(x_n) = M$. Notice that $(x_n)$ is bounded in $[a, b]$, so by Theorem \ref{thm:bolzanoWeierstrass}, it has a subsequence $\left(x_{n_k}\right)$ that converges to $x^* \in [a, b]$. Note that $f$ is continuous on $[a, b]$, so by Theorem \ref{thm:seqCriterionContinuity},
        \begin{equation*}
            \lim_{k \to \infty}f\left(x_{n_k}\right) = f(x^*).
        \end{equation*}
        This implies that $\lim_{n \to \infty}f(x_n) = f(x^*)$, i.e., $x^* = M$. Therefore, $f(x^*) \geq f(x)$ for all $x \in [a, b]$ and is an absolute maximum.
    \end{proof}
\end{thmbox}
\subsection{Bisection Method}
Continuity of a function is a very useful property which can be exploited in root approximation. Informally, if $f(a) < 0$ and $f(b) > 0$ and $f$ is continuous on $[a, b]$, then for $f$ to ``reach'' $f(b)$ from $f(a)$, it has to cross over some point at which it evaluates to $0$.
\begin{thmbox}{Location of Roots Theorem}{rootLocation}
    If $f$ is continuous on $[a, b]$ and $f(a)f(b) < 0$, then there exists some $c \in (a, b)$ such that $f(c) = 0$.
    \tcblower
    \begin{proof}
        Without loss of generality, assume $f(a) < 0 < f(b)$. Take $a_1 = a$, $b_1 = b$ and let $I_1 = [a_1, b_1]$. Let $m_n = \frac{a_n + b_n}{2} \in (a, b)$. If $f(m_1) = 0$, then we are done. Otherwise, define recursively that
        \begin{equation*}
            I_{n + 1} = \begin{cases}
                [a_n, m_n] & \quad\textrm{if } f(m_n) > 0 \\
                [m_n, b_n] & \quad\textrm{if } f(m_n) < 0
            \end{cases}.
        \end{equation*}
        This way, we obtain a sequence of nested intervals $(I_n)$. Note that
        \begin{equation*}
            b_n - a_n = \frac{b - a}{2^{n - 1}},
        \end{equation*}
        so $\lim_{n \to \infty}(b_n - a_n) = 0$. Note that $(a_n)$ is a monotone increasing sequence and $(b_n)$ is a monotone decreasing sequence, so $\sup(a_n) = \inf(b_n)$. Therefore,
        \begin{equation*}
            \bigcap_{n = 1}^{\infty}[a_n, b_n] = \{c\} \subseteq (a, b).
        \end{equation*}
        Note that both $(a_n)$ and $(b_n)$ are bounded. Since $f$ is continuous at $c$, by Theorems \ref{thm:monotoneConverge} and \ref{thm:seqCriterionContinuity},
        \begin{equation*}
            \lim_{n \to \infty}f(a_n) = \lim_{n \to \infty}f(b_n) = f(c).
        \end{equation*}
        Notice that $f(a_n) < 0$ for all $n \in \N$, so $f(c) = \lim_{n \to \infty}f(a_n) \leq 0$. Similarly, $f(c) \geq 0$. Therefore, $f(c) = 0$.
    \end{proof}
\end{thmbox}
In particular, the construction of nested intervals $I_n$'s is known as the \textit{bisection algorithm}. Theorem \ref{thm:rootLocation} helps approximate the roots for $f(x) = 0$. Clearly, this means we can determine the existence and location of roots for $f(x) = b$ in a similar fashion.
\begin{thmbox}{Intermediate Value Theorem}{intermediateVal}
    Let $f$ be continuous over some interval $I$. If $a, b \in I$ are such that $f(a) \leq f(b)$, then for all $k \in [f(a), f(b)]$, there is some $c \in I$ with $f(c) = k$.
    \tcblower
    \begin{proof}
        Without loss of generality, assume $a \leq b$. If $k = f(a)$ or $k = f(b)$, by taking $c = a$ or $c = b$ respectively we are done. If $f(a) < k < f(b)$, consider $g(x) = f(x) - k$. Note that $g$ is continuous over $[a, b] \subseteq I$ and $g(a)g(b) < 0$, so by Theorem \ref{thm:rootLocation}, there is some $c \in (a, b) \subseteq I$ such that $g(c) = f(c) - k = 0$, i.e., $f(c) = k$.
    \end{proof}
\end{thmbox}
We can generalise the above further.
\begin{thmbox}{Preservation of Closed Intervals}{closedIntPreserve}
    If $f$ is continuous on $[a, b]$, then 
    \begin{equation*}
        f([a, b]) = \bigl[\inf f([a, b]), \sup f([a, b])\bigr].
    \end{equation*}
    \tcblower
    \begin{proof}
        Note that $f([a, b]) \subseteq \bigl[\inf f([a, b]), \sup f([a, b])\bigr]$. By Theorem \ref{thm:maxMin}, there exists $x^*, x_* \in [a, b]$ such that $f(x^*) = \sup f([a, b])$ and $f(x_*) = \inf f([a, b])$. Take any $k \in [f(x_*), f(x^*)]$, by Theorem \ref{thm:intermediateVal}, there is some $c \in [a, b]$ such that $f(c) = k$. This means $k \in f([a, b])$ and so 
        \begin{equation*}
            \bigl[\inf f([a, b]), \sup f([a, b])\bigr] = [f(x_*), f(x^*)] \subseteq f([a, b]).
        \end{equation*}
        Therefore, $f([a, b]) = \bigl[\inf f([a, b]), \sup f([a, b])\bigr]$.
    \end{proof}
\end{thmbox}
\subsection{Uniform Continuity}
To determine continuity of a function $f$ at a point $c$, we need to find a $\delta > 0$ for each $\epsilon > 0$. Note that our choice of $\delta$ here might depend on both $c$ and $\epsilon$. However, for certain functions, we can ``generate'' an appropriate $\delta$ based solely on the $\epsilon$ given, i.e., we can find some $\delta = g(\epsilon)$ which is \textbf{independent} of the value of $c$. Such functions are said to be \textit{uniformly continuous}.
\begin{dfnbox}{Uniform Continuity}{uniContinuity}
    Let $f$ be a function over some $A \subseteq \R$. $f$ is said to be {\color{red} \textbf{uniformly continuous}} if for all $\epsilon > 0$, there exists some $\delta > 0$ such that for all $x, y \in A$, 
    \begin{equation*}
        \abs{f(x) - f(y)} < \epsilon
    \end{equation*}
    whenever $\abs{x - y} <\delta$.
\end{dfnbox}
Notice that in Definition \ref{dfn:uniContinuity}, the choice of $\delta$ is independent of $x$ and $y$.

Theorem \ref{thm:seqCriterionContinuity} has a version for uniform continuity as well.
\begin{thmbox}{Sequential Criterion of Uniform Continuity}{seqCriterionUniContinuity}
    Let $f$ be a function over some $A \subseteq \R$. $f$ is uniformly continuous on $A$ if and only if for any two convergent sequences $(x_n)$ and $(y_n)$ in $A$ such that $\lim_{n \to \infty}(x_n - y_n) = 0$, 
    \begin{equation*}
        \lim_{n \to \infty}\bigl(f(x_n) - f(y_n)\bigr) = 0.
    \end{equation*}
    \tcblower
    \begin{proof}
        Suppose $f$ is uniformly continuous on $A$, then for all $\epsilon > 0$, there exists some $\delta > 0$ such that
        \begin{equation*}
            \abs{f(x) - f(y)} < \epsilon
        \end{equation*}
        for all $x, y \in A$ with $\abs{x - y} < \delta$. Let $(x_n)$ and $(y_n)$ be two convergent sequences in $A$ with $\lim_{n \to \infty}(x_n - y_n) = 0$, then there is some $N \in \N$ such that 
        \begin{equation*}
            \abs{x_n - y_n} < \delta
        \end{equation*}
        whenever $n \geq N$. Therefore, for all $\epsilon > 0$, whenever $n \geq N$, 
        \begin{equation*}
            \abs{f(x_n) - f(y_n)} < \epsilon,
        \end{equation*}
        and so $\lim_{n \to \infty}\bigl(f(x_n) - f(y_n)\bigr) = 0$.
        \\\\
        We shall prove the converse by considering its contrapositive. Suppose $f$ is not uniformly continuous on $A$, then there is some $\epsilon_0 > 0$ such that for all $\delta > 0$, there exists some $x, y \in A$ with $\abs{x - y} < \delta$ but $\abs{f(x) - f(y)} \geq \epsilon_0$.
        \\\\
        This means that for all $n \in \N$, we can take some $x_n, y_n \in A$ with $0 \leq \abs{x_n - y_n} < \frac{1}{n}$, thus obtaining two sequences $(x_n)$ and $(y_n)$. Note that $\lim_{n \to \infty}(x_n - y_n) = 0$, but 
        \begin{equation*}
            \abs{f(x_n) - f(y_n)} \geq \epsilon_0,
        \end{equation*}
        which means $\bigl(f(x_n) - f(y_n)\bigr)$ does not converge to $0$.
    \end{proof}
\end{thmbox}
The negations of Definition \ref{dfn:uniContinuity} and Theorem \ref{thm:seqCriterionUniContinuity} can be very useful when proving by contrapositive. For example, we can consider the following result:
\begin{thmbox}{Uniform Continuity Theorem}{uniContinuityThm}
    If a function $f$ is continuous on a closed and bounded interval $[a, b]$, then it is uniformly continuous on $[a, b]$.
    \tcblower
    \begin{proof}
        Suppose on contrary that $f$ is not uniformly continuous on $[a, b]$, then there is some $\epsilon_0 > 0$ such that for all $n \in \N$, we can find $x_n, y_n \in [a, b]$ with $\abs{x_n - y_n} < \frac{1}{n}$ but $\abs{f(x_n) - f(y_n)} \geq \epsilon_0$. Note that $(x_n)$ is bounded, so by Theorem \ref{thm:bolzanoWeierstrass}, it has a subsequence $\left(x_{n_k}\right)$ such that $\lim_{k \to \infty}x_{n_k} = c$. Similarly, $(y_n)$ has a convergent subsequence $\left(y_{n_k}\right)$. Therefore,
        \begin{align*}
            \lim_{k \to \infty}y_{n_k} & = \lim_{k \to \infty}\left[x_{n_k} - \left(x_{n_k} - y_{n_k}\right)\right] \\ 
            & = c - \lim_{n \to \infty}(x_n - y_n) \\
            & = c.
        \end{align*}
        By Theorem \ref{thm:seqCriterionContinuity}, since $f$ is continuous at $c$, 
        \begin{equation*}
            \lim_{k \to \infty}f\left(x_{n_k}\right) = \lim_{k \to \infty}f\left(y_{n_k}\right) = f(c).
        \end{equation*}
        This implies $\lim_{k \to \infty}\Bigl(f\left(x_{n_k}\right) - f\left(y_{n_k}\right)\Bigr) = 0$, but
        \begin{equation*}
            \abs{f\left(x_{n_k}\right) - f\left(x_{n_k}\right)} \geq \epsilon_0 > 0,
        \end{equation*}
        which is a contradiction.
    \end{proof}
\end{thmbox}
While the negations of Definition \ref{dfn:uniContinuity} and Theorem \ref{thm:seqCriterionUniContinuity} aids in many proofs, proving uniform continuity directly with them are difficult. Thus, we introduce another tool to prove uniform continuity.
\begin{dfnbox}{Lipschitz Function}{lipschitz}
    Let $f$ be a function over some $A \subseteq \R$. $f$ is said to be a {\color{red} \textbf{Lipschitz Function}} (or satisfy the {\color{red} \textbf{Lipschitz Condition}}) if there exists some $K > 0$ such that
    \begin{equation*}
        \abs{f(x) - f(y)} \leq K\abs{x - y}
    \end{equation*}
    for all $x, y \in A$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that we can re-write the Lipschitz Condition as
        \begin{equation*}
            \frac{\abs{f(x) - f(y)}}{\abs{x - y}} \leq K
        \end{equation*}
        for all $x \neq y$. Note that the limit as $y \to x$ of the left-hand side is the \textit{first order derivative} of $f$, so the above expression suggests that $f'(x)$ is bounded.
    \end{remark}
\end{notebox}
It can be easily seen that by taking $\delta = \frac{\epsilon}{K}$ for each $\epsilon > 0$, a Lipschitz function can be proven to be uniformly continuous on $A$.
\begin{thmbox}{Uniform Continuity of Lipschitz Functions}{lipschitzUniContinuous}
    If $f \colon A \to \R$ is a Lipschitz function, then it is uniformly continuous on $A$.
\end{thmbox}
Lastly, we shall state the relation between uniform continuity and Cauchy sequences.
\begin{thmbox}{Preservation of Cauchy Sequences}{preserveCauchy}
    If $f$ is uniformly continuous on $A \subseteq \R$ and $(x_n)$ is a Cauchy sequence on $A$, then $\bigl(f(x_n)\bigr)$ is also a Cauchy sequence.
    \tcblower
    \begin{proof}
        Since $f$ is uniformly continuous on $A$, for all $\epsilon > 0$ there is some $\delta > 0$ such that for all $x, y \in A$,
        \begin{equation*}
            \abs{f(x) - f(y)} < \epsilon
        \end{equation*}
        whenever $\abs{x - y} < \delta$. Since $(x_n)$ is Cauchy, there is some $N \in \N$ such that for all $m, n \geq N$, $\abs{x_m - x_n} < \delta$. Therefore,
        \begin{equation*}
            \abs{f(x_m) - f(y_n)} < \epsilon
        \end{equation*}
        whenever $m, n \geq N$. Therefore, $\bigl(f(x_n)\bigr)$ is a Cauchy sequence.
    \end{proof}
\end{thmbox}
Using the preservation of Cauchy sequences, we can prove the following result:
\begin{thmbox}{Continuous Extension Theorem}{continuousEx}
    A function $f$ is uniformly continuous on $(a, b)$ if and only if it can be defined at $a$ and $b$ such that the extended function is continuous on $[a, b]$.
    \tcblower
    \begin{proof}
        The leftward direction is obvious by Theorem \ref{thm:uniContinuityThm}. Suppose $f$ is uniformly continuous on $(a, b)$. Let $(x_n)$ be a sequence in $(a, b)$ that converges to $a$. By Theorem \ref{thm:cauchyConvergeCri}, $(x_n)$ is Cauchy, and so $\bigl(f(x_n)\bigr)$ is Cauchy by Theorem \ref{thm:preserveCauchy}. 
        \\\\
        Define $f(a) = \lim_{n \to \infty}f(x_n)$, and let $(y_n)$ be a sequence in $(a, b)$ that converges to $a$. Since $f$ is uniformly continuous on $(a, b)$, by Theorem \ref{thm:seqCriterionUniContinuity}, 
        \begin{equation*}
            \lim_{n \to \infty}\bigl(f(x_n) - f(y_n)\bigr) = 0,
        \end{equation*}
        and so 
        \begin{equation*}
            \lim_{n \to \infty}f(y_n) = \lim_{n \to \infty}\left[f(x_n) - \bigl(f(x_n) - f(y_n)\bigr)\right] = f(a).
        \end{equation*}
        Note that $y_n > a$ for all $n \in \N$, so $\lim_{x \to a^+}f(x) = f(a)$. Similarly, defining $f(b) = \lim_{n \to \infty}f(z_n)$ for some sequence $(z_n)$ that converges to $b$, we can prove that $\lim_{x \to b^-}f(x) = f(b)$. Therefore, the extended function is continuous on $[a, b]$.
    \end{proof}
\end{thmbox}
\subsection{Discontinuity}
A discontinuous function can take a number of forms, one of which is a sudden change of value at a point $c$. Such functions can be vividly depicted as a continuous function ``ripped apart'' at $x = c$. Intuitively, the left-hand and right-hand limits of $f$ would still exist despite discontinuity.
\begin{thmbox}{One-Sided Limit for Monotone Functions}
    Let $f$ be increasing on $I \subseteq \R$. If $c \in I$ is not an end point, then
    \begin{align*}
        \lim_{x \to c^-}f(x) & = \sup \left\{f(x) \colon x \in I, x < c\right\} \\
        \lim_{x \to c^+}f(x) & = \inf \left\{f(x) \colon x \in I, x > c\right\}.
    \end{align*}
    \tcblower
    \begin{proof}
        Without loss of generality, it suffices to prove for the left-hand limit. Note that $S \coloneqq \left\{f(x) \colon x \in I, x < c\right\}$ is non-empty and $f(c)$ is its upper bound. Therefore, $\sup S$ exists. Denote this supremum by $L$, then for all $\epsilon > 0$, $L - \epsilon$ is not an upper bound of $S$, which mean that there is some $y \in I$ with $y < c$ such that $L - \epsilon < f(y) \leq L$.
        \\\\
        Take $\delta = c - y > 0$, then whenever $-\delta < x - c < 0$, we have $y < x < c$. Since $f$ is increasing on $I$, this implies that \begin{equation*}
            L - \epsilon < f(y) < f(x) \leq L < L + \epsilon.
        \end{equation*}
        Therefore, $\abs{f(x) - L} <  \epsilon$ and so $\lim_{x \to c^-} = \sup \left\{f(x) \colon x \in I, x < c\right\}$.
    \end{proof}
\end{thmbox}
Note that $c$ is the only discontinuous point of $f$, therefore, $f$ is continuous if the ``gap'' at $c$ were to be closed.
\begin{corbox}{}{}
    Let $f$ be increasing on $I \subseteq \R$. If $c \in I$ is not an end point, then the followings are equivalent:
    \begin{enumerate}
        \item $f$ is continuous at $c$.
        \item $\lim_{x \to c^-}f(x) = \lim_{x \to c^+}f(x) = f(c)$.
        \item $\sup \left\{f(x) \colon x \in I, x < c\right\} = \inf \left\{f(x) \colon x \in I, x > c\right\} = f(c)$.
    \end{enumerate}
\end{corbox}
Such discontinuity as described above is known as a \textit{jump discontinuity} at $c$. The following definition gives a measure for ``how discontinuous'' $f$ is at $c$:
\begin{dfnbox}{Jump}{jump}
    Let $f$ be increasing on $[a, b]$. The {\color{red} \textbf{jump}} of $f$ at $c$ is defined as
    \begin{equation*}
        j_f(c) = \begin{cases}
            \lim_{x \to a^+}f(x) - f(a) & \quad\textrm{if } c = a \\
            f(b) - \lim_{x \to b^-}f(x) & \quad\textrm{if } c = b \\
            \lim_{x \to c^+}f(x) - \lim_{x \to c^-}f(x) & \quad\textrm{otherwise}
        \end{cases}.
    \end{equation*}
\end{dfnbox}
It can be immediately seen that $f$ is continuous at $c$ if and only if $j_f(c) = 0$.

Next, let us consider an interesting question:
\begin{quote}
    How many jump discontinuities can a monotone function have over its domain?
\end{quote}
The answer might be surprising: the number of jump discontinuities of a monotone function is at most countably infinite!
\begin{thmbox}{Discontinuous Points of Monotone Functions}{monoFuncDiscon}
    Let $f$ be a monotone function over $I \subseteq \R$, then $f$ has countably many points of discontinuity.
\end{thmbox}
Lastly, we shall state a theorem regarding inverse functions.
\begin{thmbox}{Continuous Inverse Theorem}{continuousInv}
    Let $f \colon I \to \R$ be a strictly monotone and continuous function, then $f^{-1} \colon f(I) \to I$ is also a strictly monotone and continuous function.
\end{thmbox}

\chapter{Topology and Metric Spaces}
\section{Metric Space}
So far we have been dealing with sequences and functions which are real-valued. However, other abstract objects might also exhibit limiting bahaviours. Note that when we define the notion of limits, we focus on the behaviour of an object as it ``gets arbitrarily close'' to a certain point. We would like to abstract the concept of closeness.
\begin{dfnbox}{Metric}{metric}
    A {\color{red} \textbf{metric}} on a set $S$ is a function $d \colon S \times S \to \R$ that satisfies the following:
    \begin{enumerate}
        \item $d(x, y) \geq 0$ for all $x, y \in S$ (positivity);
        \item $d(x, y) = 0$ if and only if $x = y$ (definiteness);
        \item $d(x, y) = d(y, x)$ for all $x, y \in S$ (symmetry);
        \item $d(x, y) \leq d(x, z) + d(z, y)$ for all $x, y, z \in S$ (triangular inequality).
    \end{enumerate}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A metric is sometimes also called a {\color{red} \textbf{distance function}}.
    \end{remark}
\end{notebox}
\begin{dfnbox}{Metirc Space}{metricSpace}
    A {\color{red} \textbf{metric space}} $(S, d)$ is a set $S$ together with a metric $d$ on $S$.    
\end{dfnbox}
In the Euclidean space $\R^n$, a usual definition for distance is
\begin{equation*}
    d_2(\mathbfit{x}, \mathbfit{y}) = \left[\sum_{i = 1}^{n}(y_i - x_i)^2\right]^{\frac{1}{2}}.
\end{equation*}
Note that $(\R^n, d_2)$ is a metric space, where $d_2$ is known as the \textit{Euclidean distance}. In general, we can prove that for any $p \in \N^+$,
\begin{equation*}
    d_p(\mathbfit{x}, \mathbfit{y}) = \left[\sum_{i = 1}^{n}\abs{y_i - x_i}^p\right]^{\frac{1}{p}}
\end{equation*}
is a metric over $\R^n$. Furthermore, notice that
\begin{equation*}
    \max_{i \in \N^+, i \leq n}\abs{y_i - x_i}^p \leq \sum_{i = 1}^{n}\abs{y_i - x_i}^p \leq n\max_{i \in \N^+, i \leq n}\abs{y_i - x_i}^p.
\end{equation*}
Taking the $p$-th root on all three parts, we have
\begin{equation*}
    \max_{i \in \N^+, i \leq n}\abs{y_i - x_i} \leq \left[\sum_{i = 1}^{n}\abs{y_i - x_i}^p\right]^{\frac{1}{p}} \leq n^{\frac{1}{p}}\max_{i \in \N^+, i \leq n}\abs{y_i - x_i}.
\end{equation*}
By Theorem \ref{thm:squeeze}, this allows us to define
\begin{equation*}
    d_\infty(\mathbfit{x}, \mathbfit{y}) = \lim_{p \to \infty}d_p(\mathbfit{x}, \mathbfit{y}) = \max_{i \in \N^+, i \leq n}\abs{y_i - x_i}.
\end{equation*}
$d_\infty(\mathbfit{x}, \mathbfit{y})$ can be alternatively written as $\norm{\mathbfit{x - y}}_\infty$, which is known as the \textit{infinite norm}.

Using metric spaces, we can extend and generalise many notions defined in previous chapters from $\R$ to any set.
\begin{dfnbox}{Neighbourhood}{neighbourhoodGen}
    Let $(S, d)$ be a metric space. For any $\epsilon > 0$, the {\color{red} \textbf{$\epsilon$-neighbourhood}} of $x_0 \in S$ is the set
    \begin{equation*}
        V_\epsilon(x_0) \coloneqq \left\{x \in S \colon d(x_0, x) < \epsilon\right\}.
    \end{equation*}
    Generally, a set $U$ is called a {\color{red} \textbf{neighbourhood}} of $x \in S$ if there exists some $V_\epsilon(x) \subseteq U$.
\end{dfnbox}
Now, we are able to define convergence of a sequence over any general set.
\begin{dfnbox}{Convergence}{convergenceGen}
    Let $(x_n)$ be a sequence over a metric space $(S, d)$. $(x_n)$ is said to be {\color{red} \textbf{convergent}} to $x \in S$ if for all $\epsilon > 0$, there exists some $K \in \N$ such that whenever $n \geq K$, $d(x_n, x) < \epsilon$.
\end{dfnbox}
Similarly, we can re-state the definitions and theorems regarding limits and continuity in terms of metric spaces.

\section{Compact Sets}
In real numbers, we can define the boundedness and openness of an interval. These ideas can be extended to a metric space. In particular, we would like to investigate a set which can be informally stated as ``closed and bounded''. We will define openness first.
\begin{dfnbox}{Open and Closed Sets}{openSet}
    Let $(S, d)$ be a metric space. $S$ is called an {\color{red} \textbf{open set}} if and only if for all $x \in S$, $S$ is a neighbourhood of $x$. A set whose complement is open is said to be {\color{red} \textbf{closed}}.
\end{dfnbox} 
\begin{notebox}
    \begin{remark}
        Note that $\R$ and $\varnothing$ are both closed and open.
    \end{remark}
\end{notebox}
We have the following important properties:
\begin{thmbox}{Open Set Properties}{openSetProps}
    The union of open sets is open and the intersection of finitely many open sets is open.
\end{thmbox}
\begin{thmbox}{Closed Set Properties}{closedSetProps}
    The intersection of closed sets is closed and the union of finitely many closed sets is closed.
\end{thmbox}
It turns out that openness of a set has important relations to the convergence of sequences defined over the set.
\begin{thmbox}{Characterisation of Closed Sets}{characterisationClosed}
    A set $F$ is closed if and only if for every convergent sequence $(x_n)$ in $F$, $\lim_{n \to \infty}x_n \in F$.
\end{thmbox}
\begin{thmbox}{Alternative Characterisation of Closed Sets}{characterisationClosedAlt}
    A set $F$ is closed if and only if it contains all of its cluster points.
\end{thmbox}
We can use open sets to test the continuity of a function.
\begin{thmbox}{Global Continuity Theorem}{globalContinuityThm}
    Let $(A, d_A)$ and $(B, d_B)$ be metric spaces. $f \colon A \to B$ is continuous on $A$ if and only if for every open set $G \subseteq B$, there exists an open set $H$ such that $H \cap A = f^{-1}(G)$.
\end{thmbox}
In real numbers, the above theorem gives the following corollary:
\begin{corbox}{Global Continuity Theorem on $\R$}{globalContinuityThmR}
    $f \colon \R \to \R$ is continuous over $\R$ if and only if for every open set $G \subseteq \R$, $f^{-1}(G)$ is open.
\end{corbox}
A closed and bounded set is the pre-requisite of many important theorems. Before we formally define it, we introduce the notion of an \textit{open cover}.
\begin{dfnbox}{Open Cover}{openCover}
    Let $(S, d)$ be a metric space. For $A \subseteq S$, an {\color{red} \textbf{open cover}} of $A$ is a collection
    \begin{equation*}
        \mathcal{G} \coloneqq \left\{G \in \mathcal{P}(S) \colon G \textrm{ is open}, A \subseteq \bigcup G\right\}.
    \end{equation*}
    If $\mathcal{G}' \subseteq \mathcal{G}$ and $\mathcal{G}'$ is an open cover of $A$, then $\mathcal{G}'$ is a {\color{red} \textbf{subcover}} of $\mathcal{G}$.
\end{dfnbox}
Indeed, an open cover is just a collection of open sets which ``covers up'' a set $A$. Intuitively, $A$ has a boundary if it can be ``covered'' with finitely many such open sets.
\begin{dfnbox}{Compact Set}{compact}
   Let $(S, d)$ be a metric space. A set $K \subseteq S$ is said to be {\color{red} \textbf{compact}} if every open cover $\mathcal{G}$ of $K$ has a finite subcover. 
\end{dfnbox}
In other words, Definition \ref{dfn:compact} states that if $\mathcal{G}$ is any open cover of $K$, then $K$ is compact if there are finitely many open sets $G_1, G_2, \cdots, G_n \in \mathcal{G}$ whose union is $K$.

Next, we reveal the equivalence between compactness and closed-and-boundedness.
\begin{dfnbox}{Bounded Set}{boundedSet}
    Let $(S, d)$ be a metric space. A set $A \subseteq S$ is {\color{red} \textbf{bounded}} if there exist some $M \in \R$ and $x \in S$ such that $A \subseteq V_M(x)$.
\end{dfnbox}
It can be seen that a bounded set is confined within a particular open set. Naturally, this means we only need finitely many open sets for their union to contain this set.
\begin{thmbox}{Heine-Borel Theorem}{closedBounded}
    Let $(S, d)$ be a metric space. $K \subseteq S$ is compact if and only if it is closed and bounded.
\end{thmbox}
Consider a compact set $K$ and a function $f$ whose domain contains $K$. This means that $f$ would map $K$ to another set. Intuitively, this image should be compact as well provided that $f(K)$ has no ``gaps''.
\begin{thmbox}{Preservation of Compactness}{preserveCompact}
    Let $(S, d)$ be a metric space and $f \colon S \to \R$. If $K \subseteq S$ is compact and $f$ is continuous, then $f(K)$ is compact.
\end{thmbox}
The notion of having no gaps in a set means that the set is connected, which can be defined as follows:
\begin{dfnbox}{Connected Set}{connected}
    A set $U$ is {\color{red} \textbf{disconnected}} if it has an open cover $\{A, B\}$ such that $A \cap U$, $B \cap U$ are both non-empty but disjoint. Otherwise, $U$ is {\color{red} \textbf{connected}}.
\end{dfnbox}
An interval in $\R$ is trivially connected. However, we would like to state that any connected subset of $\R$ must also be an interval.
\begin{probox}{Connectedness and Intervals}{}
    A subset of $\R$ is connected if and only if it is an interval.
\end{probox}
Lastly, we state without proof the analogue of the Intermediate Value Theorem in terms of connected sets.
\begin{thmbox}{Generalised Intermediate Value Theorem}{genIntVal}
    Let $f \colon S \to \R$ be a function. If $E \subseteq S$ is connected, then $f(E)$ is connected.
\end{thmbox}
\end{document}