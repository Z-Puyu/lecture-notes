\documentclass[math]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\e}{\mathrm{e}}
\newcommand{\dif}{\mathrm{d}}
\begin{document}
\fancyhead[L]{
    MA2116 Probability
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{How to Count}
\section{Basic Counting Principles}
An important motivation to study combinatorics is to count the \textbf{number of ways} in which an event may occur. Intuitively, we have two approaches to count.

The first approach is to categorise the event into \textbf{non-overlapping cases}. This means that we break an event into mutually exclusive sub-events, after which we can count the number of ways for each sub-event to occur. The agregate of these counts is the total number of ways for the original event to occur.

Those familiar with basic set theory may consider $E$ to be the set containing all distinct ways for an event to occur. By breaking up the event, we essentially establish a \textbf{partition} of~$E$, so that the sum of cardinalities of all the elements in that partition equals the cardinality of $E$.

This motivates us to write the following principle using set notations.
\begin{thmbox}{Addition Principle (AP)}{AP}
    Let $k \in \N^+$ and let $A_1, A_2, \cdots, A_k$ be $k$ finite sets which are pairwise disjoint, i.e. for all~$i, j$ such that $1 \leq i, j \leq k$, $A_i \cap A_j = \varnothing$ whenever~$i \neq j$, then
   \begin{equation*}
        \abs{\bigcup_{i = 1}^k A_i} = \sum_{i = 1}^{k}\abs{A_i}.
   \end{equation*} 
   \tcblower
   \begin{proof}
        The case where $k = 1$ is trivial.

        Suppose that when $k = n$, we have
        \begin{equation*}
            \abs{\bigcup_{i = 1}^n A_i} = \sum_{i = 1}^{n}\abs{A_i}
        \end{equation*} 
        for any $n$ finite sets which are pairwise disjoint. Let $A_{n + 1}$ be an arbitrary finite set which is disjoint with any of the $A_i$'s from the $n$ sets. So we have:
        \begin{align*}
            \abs{\bigcup_{i = 1}^{n + 1} A_i} & = \abs{\left(\bigcup_{i = 1}^n A_i\right) \cup A_{n + 1}} \\
            & = \abs{\bigcup_{i = 1}^n A_i} + \abs{A_{n + 1}} - \abs{\left(\bigcup_{i = 1}^n A_i\right) \cap A_{n + 1}} \\
            & = \left(\sum_{i = 1}^{n}\abs{A_i}\right) + \abs{A_{n + 1}} - \abs{\varnothing} \\
            & = \sum_{i = 1}^{n + 1}\abs{A_i}.
        \end{align*}
        Therefore, the original statement holds for all $k \in \N^+$.
   \end{proof}
\end{thmbox}
\begin{notebox}
    In more casual language, this means that if an event $E_k$ has $n_k$ distinct ways to occur, then there is $\sum_{i = 1}^{k}n_k$ ways for at least one of the events $E_1, E_2, \cdots, E_k$ to occur, provided that $E_i$ and $E_j$ can never occur concurrently whenever $i \neq j$.
\end{notebox}
Given an event $E$, the other approach to count the number of ways for it to occur is to break~$E$ up internally into \textbf{non-overlapping stages}.

With set notations, we can write the $i$-th stage for $E$ to occur as $e_i$, and so a way for $E$ to occur can be represented by an ordered tuple $(e_1, e_2, \cdots, e_k)$, where $k$ is the total number of stages to undergo for $E$ to occur.

Let $E_i$ denote the set of all distinct ways to undergo the $i$-th stage of $E$, then it is easy to see that $E$ is just the \textbf{Cartesian product} of all the $E_i$'s. Hence, we derive the following principle:
\begin{thmbox}{Multiplication Principle (MP)}{MP}
    Let $k \in \N^+$ and let $A_1, A_2, \cdots, A_k$ be $k$ pairwise disjoint finite sets, then
    \begin{equation*}
        \abs{\prod_{i = 1}^{k}A_i} = \prod_{i = 1}^{k}\abs{A_i}.
    \end{equation*}
    \tcblower
    \begin{proof}
        The case where $k = 1$ is trivial.

        Suppose that when $k = n$, we have
        \begin{equation*}
            \abs{\prod_{i = 1}^{n}A_i} = \prod_{i = 1}^{n}\abs{A_i}
        \end{equation*} 
        for any $n$ finite sets which are pairwise disjoint. Let $A_{n + 1}$ be an arbitrary finite set which is disjoint with any of the $A_i$'s from the $n$ sets. Take $a_i, a_j \in A_{n + 1}$. Note that for all $\mathbfit{a} \in \prod_{i = 1}^{n}A_i$, $(\mathbfit{a}, a_i) \neq (\mathbfit{a}, a_j)$ whenever $a_i \neq a_j$. This means that
        \begin{align*}
            \abs{\prod_{i = 1}^{n + 1}A_i} & = \abs{\prod_{i = 1}^{n}A_i \times A_{n + 1}} \\
            & = \abs{\prod_{i = 1}^{n}A_i}\abs{A_{n + 1}} \\
            & = \left(\prod_{i = 1}^{n}\abs{A_i}\right)\abs{A_{n + 1}} \\
            & = \prod_{i = 1}^{n + 1}\abs{A_i}
        \end{align*}
        Therefore, the original statement holds for all $k \in \N^+$.
    \end{proof}
\end{thmbox}
\begin{notebox}
    In more casual language, this means that if an event $E$ requires $k$ stages to be undergone before it occurs and the $i$-th stage has $n_i$ ways to complete, then there is $\prod_{i = 1}^{k}n_k$ ways for $E$ to occur, provided that no two different stages complete concurrently.
\end{notebox}
\section{Permutations}
A fundamental problem in combinatorics is described as follows: given a set $S$, how many ways are there to arrange $r$ elements in $S$, i.e. how many \textbf{distinct sequences} can be formed using the elements in $S$ without repetition? The process of selecting elements from $S$ and arranging them as a sequence is known as \textit{permutation}.

Note that forming a sequence using $r$ elements from a set $S$ is an event consisting of $r$ stages, as we need to select an element for each of the $r$ terms of the sequence. Suppose $S$ has $n$ elements. For the first term of the sequence, we can choose any of the elements in $S$, so there is $n$ ways to do it. For the second term, since we cannot repeat the elements, we are left with $(n - 1)$ choices. 

Continue choosing elements in this way, we realise that if we choose the terms sequentially, when we reach the $k$-th term we will be left with $n - k + 1$ options as the previous~$(k - 1)$ terms have taken away $(k - 1)$ elements. By Theorem \ref{thm:MP}, we know that the number of sequences which can be formed is given by $\prod_{i = 1}^{r}(n - r + i)$.
\begin{dfnbox}{Permutations}{permutations}
    Let $A$ be a finite set such that $\abs{A} = n$, an $r$-permutation of $A$ is a way to arrange $r$ elements of $A$, denoted as $P^n_r$ and given by
    \begin{equation*}
        P^n_r = \prod_{i = 1}^{r}(n - r + i) = \frac{n!}{(n - r)!}.
    \end{equation*}
\end{dfnbox}
\subsection{Permutations with Idential Objects}
\begin{thmbox}{Generalised Formula for Permutations}{}
    Let $k \in \N^+$ and let $A_1, A_2, \cdots, A_k$ be $k$ distinct objects, where $A_i$ occurs $n_i > 0$ times for~$i = 1, 2, \cdots, k$, then the number of permutations for these $k$ objects are given by
    \begin{equation*}
        \frac{\left(\sum_{i = 1}^{k}n_i\right)!}{\prod_{i = 1}^{k}\left(n_i!\right)}.
    \end{equation*}
\end{thmbox}
\section{Combinations}
\begin{dfnbox}{Combinations}{combinations}
    Let $A$ be a finite set such that $\abs{A} = n$, an $r$-combination of $A$ is a way to choose $r$ elements from $A$ regardless of the order of selection, denoted as $C^n_r$ and given by
    \begin{equation*}
        C^n_r = \frac{P^n_r}{P^r_r} = \frac{n!}{r!(n - r)!} = \begin{pmatrix}
            n \\
            r
        \end{pmatrix}.
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Two obvious results:
        \begin{enumerate}
            \item If $r > n$ or $r < 0$, $C^n_r = 0$;
            \item $C^n_r = C^n_{n - r}$.
        \end{enumerate}
    \end{remark}
\end{notebox}
\begin{thmbox}{Pascal's Triangle}{pascalTri}
    Let $n$ be an integer with $n \geq 2$ and let $r$ be an integer with $0 \leq r \leq n$, then
    \begin{equation*}
        C^n_r = C^{n - 1}_{r - 1} + C^{n - 1}_r.
    \end{equation*}
\end{thmbox}

\section{Binomial and Multinomial Coefficients}
Consider the expansion of $(x + y)^n$ where $n \in \N$. Note that this expansion is a linear combination of terms in the form of $x^ky^{n - k}$ where $k = 0, 1, 2, \cdots, n$. 

Thus, fix any $k$, to determine how many copies of $x^ky^{n - k}$ there are, it suffices to compute~$C^n_k$. Therefore, in the expanded form of $(x + y)^n$, the coefficient is exactly $C^n_r$.
\begin{thmbox}{Binomial Expansion}{binomialEx}
    Let $n \in \N$, then
    \begin{equation*}
        (x + y)^n = \sum_{k = 0}^n \left[\begin{pmatrix}
            n \\
            k
        \end{pmatrix}x^ky^{n - k}\right].
    \end{equation*}
\end{thmbox}
We can extend the idea of binomial coefficients onto multinomial expansions, i.e. expressions in the form of $\left(\sum_{i = 1}^{r}x_i\right)^n$.

Note that the binomial coefficient $C^n_r$ is essentially equivalent to dividing $n$ distinct elements into two groups with $r$ and $(n - r)$ members respectively. Now we consider dividing~$n$ distinct elements into $r$ groups with $n_1, n_2, \cdots, n_r$ members respectively for each group.

Notice that we can simply permute the $n$ distinct elements and assign them sequentially into the $r$ groups, i.e. the first $n_1$ elements will go into the first group and so on.

Since the order of elements within each group does not matter, we need to remove repeated selections by dividing by $\prod_{i = 1}^{r}(n_i!)$. So we have the following definition:
\begin{dfnbox}{Multinomial Coefficients}{multinomialCoeff}
    The {\color{red} \textbf{multinomial coefficient}} is defined by
    \begin{equation*}
        \begin{pmatrix}
            n \\
            n_1, n_2, \cdots, n_k
        \end{pmatrix} = \frac{n!}{\prod_{i = 1}^{k}\left(n_i!\right)}
    \end{equation*}
\end{dfnbox}
\begin{thmbox}{Multinomial Expansion}{multinomialEx}
    Let $n \in \N$, then 
    \begin{equation*}
        \left(\sum_{i = 1}^{r}x_i\right)^n = \sum_{
            \substack{
                n_1, n_2, \cdots, n_r \in \N \\
                \sum_{j = 1}^rn_j = n
            }
        } \left[\begin{pmatrix}
            n \\
            n_1, n_2, \cdots, n_r
        \end{pmatrix}\prod_{i = 1}^{r}x_i^{n_i}\right]
    \end{equation*}
\end{thmbox}

\chapter{Axioms of Probability}
\section{Sample Space and Events}
\begin{dfnbox}{Sample Space}{sampleSpace}
    Consider an experiment whose outcome is {\color{red} \textbf{not}} predictable, then the set of all possible outcomes of the experiment is called the {\color{red} \textbf{sample space}} of the experiment, denoted by $S$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $S \neq \varnothing$.
    \end{remark}
\end{notebox}
\begin{dfnbox}{Events}{event}
    Let $S$ be a sample space, a set $E \subseteq S$ is known as an {\color{red} \textbf{event}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        $S$ itself is known as the {\color{red} \textbf{sure event}} and $\varnothing$ is known as the {\color{red} \textbf{null event}}.
    \end{remark}
\end{notebox}
Note that since sample spaces and events are sets, we can apply operations onto events precisely in the same way for sets.

By convention, the intersection of two events $E$ and $F$ is preferably written as $EF$. Two events which are disjoint are called \textit{mutually exclusive}.

\section{Probability}
\begin{dfnbox}{Probability}{probability}
    Let $E$ be any event of an experiment and let $n(E)$ be the number of occurrences of $E$ in the first $n$ repetitions of the experiment, then the {\color{red} \textbf{probability}} of $E$ is
    \begin{equation*}
        P(E) = \lim_{n \to \infty}\frac{n(E)}{n},
    \end{equation*}
    if the limit exists.
\end{dfnbox}
However, notice that from the above, the notion of probability may not be well-defined as~$n(E)$ is not a function, which means that the limit is not defined.

To avoid this problem, we shall use an axiomatic definition instead, i.e., we define probability to be such that if it exists and is well-defined, then it satisfies a series of axioms. 
\begin{dfnbox}{Axioms of Probability}{probAxi}
    Let $S$ be a sample space and let $P(E)$ be a real number defined for every $E \subseteq S$. If
    \begin{itemize}
        \item $0 \leq P(E) \leq 1$,
        \item $P(S) = 1$, and
        \item for all mutually exclusive $E$ and $F$, $P(E \cup F) = P(E) + P(F)$,
    \end{itemize}
    then $P(E)$ is the {\color{red} \textbf{probability}} of $E$.
\end{dfnbox}
With induction, one can easily show that if $E_1, E_2, \cdots$ to be any sequence of events in a sample space $S$, then
\begin{equation*}
    P\left(\bigcup_{i = 1}^\infty E_i\right) = \sum_{i = 1}^{\infty}P(E_i).
\end{equation*}
We now follow up with proofs for two seemingly intuitive results.
\begin{thmbox}{The Null Event}{null}
    Consider the null event $\varnothing$, we have
    \begin{equation*}
        P(\varnothing) = 0.
    \end{equation*}
    \tcblower   
    \begin{proof}
        Let $S$ be a sample space and let $E_1, E_2, \cdots$ be a countably infinite sequence of events such that $E_i = \varnothing$ for all $i \in \N^+$. We can write
        \begin{equation*}
            P\left(\bigcup_{i = 1}^\infty E_i\right) = \sum_{i = 1}^{\infty}P(E_i).
        \end{equation*}
        Note that the countable union of empty sets is empty, so the above is equivalent to
        \begin{equation*}
            P\left(\bigcup_{i = 1}^\infty \varnothing\right) = P(\varnothing) = \sum_{i = 1}^{\infty}P(\varnothing).
        \end{equation*}
        This means that $P(\varnothing)$ equals the sum of a countably infinite sequence of itself, so 
        \begin{equation*}
            P(\varnothing) = 0.
        \end{equation*}
    \end{proof} 
\end{thmbox}
\begin{thmbox}{Monotonity of Probability}{monotone}
    Let $E$ and $F$ be events such that $E \subseteq F$, then 
    \begin{equation*}
        P(F) \geq P(E).
    \end{equation*}
    \tcblower   
    \begin{proof}
        Note that $E$ and $F - E$ are mutually exclusive, so 
        \begin{equation*}
            P(F) = P(E \cup (F - E)) = P(E) + P(F - E).
        \end{equation*}
        Note that $P(F - E) \geq 0$, so $P(E) + P(F - E) \geq P(E)$, which means
        \begin{equation*}
            P(F) \geq P(E).
        \end{equation*}
    \end{proof}
\end{thmbox}

\section{Inclusion-Exclusion Principle}
It is easy to compute the probability of a countable union of mutually exclusive events. However, it may get tricky when an event is the union of events which are not mutually exclusive. Intuitively, we can sum up the probabilities of all individual events and subtract the portions which are double-counted. This approach is rigorously summarised as follows:
\begin{thmbox}{Inclusion-Exclusion Principle}{in-exPrinciple}
    Let $S$ be a sample space and let $E_1, E_2, \cdots, E_n$ be a sequence of events. In general, we have
    \begin{equation*}
        P\left(\bigcup_{i = 1}^n E_i\right) = \sum_{j = 1}^{n} \left[(-1)^{j + 1}\left(\sum_{k_1 \leq k_2 \leq \cdots \leq k_j}P\left(\bigcap_{h = 1}^{j}E_{k_h}\right)\right)\right].
    \end{equation*}
    \tcblower
    \begin{proof}
        Define a function $f_S \colon S \to \{0, 1\}$ by
        \begin{equation*}
            f_S(x) = \begin{cases}
                1 \quad \textrm{if } x \in S \\
                0 \quad \textrm{if } x \notin S
            \end{cases}.
        \end{equation*}
        Let $E = \bigcup_{i = 1}^n E_i$. Consider the function $g \colon S \to \{0, 1\}$ given by
        \begin{displaymath}
            g(x) = \prod_{i = 1}^{n}\left(f_E(x) - f_{E_i}(x)\right).
        \end{displaymath}
        For any $x \in S$, if $x \in E$, then $x \in E_k$ for some $k \in \left\{x \in \N \colon x \leq n\right\}$, which means that $f_E(x) - f_{E_k}(x) = 0$; if $x \notin E$, then $f_E(x) = f_{E_i}(x) = 0$ for all $i \in \left\{x \in \N \colon x \leq n\right\}$. In either case, $g(x) = 0$.
    \end{proof}
\end{thmbox}
\begin{thmbox}{Boole's Inequality}{boole}
    Let $E_1, E_2, \cdots, E_n, \cdots$ be a countable sequence of events, then
    \begin{equation*}
        P\left(\bigcup_{i = 1}^\infty E_i\right) \leq \sum_{i = 1}^{\infty}P(E_i).
    \end{equation*}
    In particular, equality is achieved if and only if the $E_i$'s are mutually exclusive.
\end{thmbox}
\begin{thmbox}{Probability in a Finite Sample Space}
    Let $S$ be a sample space which is finite and let $E \subseteq S$ be an event, then 
    \begin{equation*}
        P(E) = \frac{\abs{E}}{\abs{S}}.
    \end{equation*}
\end{thmbox}

\chapter{Conditional Probability}
\section{Conditional Probability}
Given a sample space $S$, we may wish to find the probability of two events $E$ and $F$ both occurring, $P(EF)$. However, suppose that we already know that event $F$ \textbf{has occurred}, then necessarily, the sample space we consider would no longer be $S$. Essentially, this condition of $F$ having occurred has restricted our sample space to $F$. Thus, we give the following definition:
\begin{dfnbox}{Conditional Probability}{condProb}
    Let $S$ be a sample space and $E, F \subseteq S$ be two events. If $P(F) \geq 0$, then the {\color{red} \textbf{conditional probability}} is the probability that $E$ occurs given that $F$ has occurred, denoted by
    \begin{equation*}
        P(E|F) = \frac{P(EF)}{P(F)}.
    \end{equation*}
    In particular, if $E \subseteq F$, we have $P(E|F) = \frac{P(E)}{P(F)}$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that $P(E|F) = P(EF|F)$.
    \end{remark}
\end{notebox}
It is easy to see that $P(EF) = P(E|F)P(F)$, i.e., the probability of $E$ and $F$ both occurring is the product of the probability of $F$ occurring and the probability of $E$ occurring given the occurrence of $F$. This complies with our intuition. We can generalise this for a countable number of events:
\begin{probox}{Multiplication Rule}{multRule}
    Let $S$ be a sample space and let $E_i \subseteq S$ for $i = 1, 2, \cdots, n$ be $n$ events, where $n \geq 2$. Suppose that $P\left(\bigcap_{i = 1}^{n - 1}E_i\right) > 0$, then 
    \begin{equation*}
        P\left(\bigcap_{i = 1}^{n}E_i\right) = P(E_1)\prod_{i = 2}^{n}P\left(E_i\left\lvert\bigcap_{j = 1}^{i - 1}E_j\right.\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        The case where $n = 2$ is immediate from Definition \ref{dfn:condProb}.
        \\\\
        Suppose that there is some $k \in \N$ and $k \geq 2$ such that
        \begin{equation*}
            P\left(\bigcap_{i = 1}^{k}E_i\right) = P(E_1)\prod_{i = 2}^{k}P\left(E_i\left\lvert\bigcap_{j = 1}^{i - 1}E_j\right.\right),
        \end{equation*}
        then we consider
        \begin{align*}
            P\left(E_{k + 1}\left\lvert\bigcap_{i = 1}^{k}E_i\right.\right) & = \frac{P\left(\bigcap_{i = 1}^{k + 1}E_i\right)}{P\left(\bigcap_{i = 1}^{k}E_i\right)} \\
            & = \frac{P\left(\bigcap_{i = 1}^{k + 1}E_i\right)}{P(E_1)\prod_{i = 2}^{k}P\left(E_i\left\lvert\bigcap_{j = 1}^{i - 1}E_j\right.\right)}.
        \end{align*}
        Therefore,
        \begin{align*}
            P\left(\bigcap_{i = 1}^{k + 1}E_i\right) & = \left[P(E_1)\prod_{i = 2}^{k}P\left(E_i\left\lvert\bigcap_{j = 1}^{i - 1}E_j\right.\right)\right]P\left(E_{k + 1}\left\lvert\bigcap_{i = 1}^{k}E_i\right.\right) \\
            & = P(E_1)\prod_{i = 2}^{k + 1}P\left(E_i\left\lvert\bigcap_{j = 1}^{i - 1}E_j\right.\right)
        \end{align*}
    \end{proof}
\end{probox}
\section{Bayes's Formula}
Consider a sample space $S$ and two events $E, F \subseteq S$. Suppose that $E$ occurs, then either $F$ has occurred or $F$ has never occurred (i.e. $F^{\mathrm{c}}$ occurred). Therefore, it is easy to see that
\begin{equation*}
    P(E) = P(EF) + P(EF^{\mathrm{c}}) = P(E|F) + P(E|F^{\mathrm{c}}).
\end{equation*}
We can extend the above argument for more than two events. Suppose that $F_1, F_2, \cdots, F_n$ are $n$ mutually exclusive events such that $\bigcup_{i = 1}^n F_i = S$, then obviously $\left\{F_1, F_2, \cdots, F_n\right\}$ is a \textit{partition} of $S$.

Consider any event $E$ and let $e \in E$. Clearly, $e$ must be in one and only one of $F_1, F_2, \cdots, F_n$. It then follows that $\left\{E \cap F_1, E \cap F_2, \cdots, E \cap F_n\right\}$ is a partition of $E$. Generalising this further to a countably infinite number of mutually exclusive events $F_1, F_2, \cdots$ such that $\bigcup_{i = 1}^\infty F_i = S$, we arrive at the following formula:
\begin{equation*}
    P(E) = \sum_{i = 1}^{\infty}P(E|F_i)P(F_i).
\end{equation*}
This leads to the \textit{Bayes's Formula}:
\begin{thmbox}{Bayes's Formula}{BayesFormula}
    Let $F_1, F_2, \cdots$ be a countably infinite sequence of events over a sample space $S$ such that $\bigcup_{i = 1}^\infty F_i = S$. For any event $E \subseteq S$, we have
    \begin{equation*}
        P(F_j|E) = \frac{P(E|F_j)P(F_j)}{\sum_{i = 1}^{\infty}P(E|F_i)P(F_i)}.
    \end{equation*}
\end{thmbox}
\section{Indepedent Events}
Note that in general, for two events $E$ and $F$, $P(E|F) \neq P(E)$, i.e., the occurrence of $F$ may affect the occurrence of $E$. However, in some cases, we notice that the occurrence of $E$ is \textit{independent} of $F$, and so we introduce the following definition:
\begin{dfnbox}{Independent Events}{indEvents}
    Let $S$ be a sample space and let $E, F \subseteq S$ be two events. We say that $E$ and $F$ are {\color{red} \textbf{independent}} if $P(EF) = P(E)P(F)$, and {\color{red} \textbf{dependent}} otherwise.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        The following results are immediate:
        \begin{enumerate}
            \item If $P(E) = 0$ or $P(F) = 0$, then $E$ and $F$ are independent.
            \item If $P(E) > 0$ (respectively, $P(F) > 0$), then $E$ and $F$ are independent if and only if $P(F|E) = P(F)$ (respectively, $P(E|F) = P(E)$).
        \end{enumerate}
    \end{remark}
\end{notebox}
Intuitively, given independent events $E$ and $F$, we may believe that if the occurrence of $E$ does not affect the occurrence of $F$, then naturally the occurrence of $E$ should also not affect the ``not-occurring'' of $F$, i.e., the following is true:
\begin{probox}{}{independentComp}
    $E$ and $F$ are independent events if and only if $E$ and $F^{\mathrm{c}}$ are independent events.
    \tcblower
    \begin{proof}
        Since $F = \left(F^{\mathrm{c}}\right)^{\mathrm{c}}$, it suffices to prove for one direction. 
        \\\\
        Notice that $EF \cup EF^{\mathrm{c}} = E(F \cup F^{\mathrm{c}}) = E$, so
        \begin{equation*}
            P(E) = P(EF) + P(EF^{\mathrm{c}}) = P(E)P(F) + P(EF^{\mathrm{c}}).
        \end{equation*}
        Therefore,
        \begin{equation*}
            P(EF^{\mathrm{c}}) = P(E) - P(E)P(F) = P(E)(1 - P(F)) = P(E)P(F^{\mathrm{c}}),
        \end{equation*}
        and so $E$ and $F^{\mathrm{c}}$ are independent.
    \end{proof}
\end{probox} 

\chapter{Random Variables}

\section{Random Variables}
In many contexts, we might wish to generalise a formula to compute the probability of the occurrence of a certain event. However, in cases where the events are abstract or unquantifiable (e.g. the event ``tomorrow is rainy''), it becomes hard to formulate a well-defined mapping from a sample space to $[0, 1]$. Thus, to model all events easily using functions and mappings, we introduce the notion of \textit{random variables}.
\begin{dfnbox}{Random Variable}{randVar}
    Let $\Omega$ be a sample space, the {\color{red} \textbf{random variable}}
    \begin{displaymath}
        X \colon \Omega \to \R
    \end{displaymath}
    is a real-valued function such that for any event $E \subseteq \Omega$, 
    \begin{equation*}
        P(E) = P(X[E]) = P(X \in X[E]),
    \end{equation*}
    where
    \begin{displaymath}
        X[E] \coloneqq \left\{X(\omega) \colon \omega \in \Omega\right\}
    \end{displaymath}
    is the image of the event $E$ under $X$.
\end{dfnbox}
\section{Discrete Random Variables}
Intuitively, there are certain events whose outcomes are finite or can be enumerated. In such cases, we may associate these events with a \textit{discrete random variable}.
\begin{dfnbox}{Discrete Random Variable (DRV)}{DRV}
    Let $X$ be a random variable over a sample space $S$, if $\mathrm{ran}(X)$ is countable, then $X$ is called a {\color{red} \textbf{discrete random variable}}.
\end{dfnbox}
\begin{dfnbox}{Probability Mass Function (PMF)}{PMF}
    Let $X$ be a discrete random variable over a sample space $S$, the function
    \begin{displaymath}
        p_X \colon X[S] \to [0, 1]
    \end{displaymath}
    where $p_X(a) = P(X = a)$ is known as the {\color{red} \textbf{probability mass function}} of $X$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        $\sum_{a \in X[S]} p_X(a) = 1$.
    \end{remark}
\end{notebox}
For any discrete random variable $X$, $p_X(a) = 0$ if and only if $\left\{s \in S \colon X(s) = a\right\} = \varnothing$. This essentially means that if $p_X$ evaluates to $0$, then the corresponding event is an impossible event.

Note that $p_X$ essentially gives the probability of \textbf{singleton} events in a sample space. Naturally, we can represent the probability of a union of singleton events as a linear combination of the values of $p_X$.
\begin{dfnbox}{Cumulative Distribution Function (CDF)}{CDF}
    Let $X$ be a discrete random variable over a sample space $S$ with PMF $p_X$, the function
    \begin{displaymath}
        F_X \colon X[S] \to [0, 1]
    \end{displaymath}
    where $F_X(a) = \sum_{x \leq a}p_X(x)$ is known as the {\color{red} \textbf{cumulative distribution function}} of $X$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Suppose that $X(s_i) = a_i$ for all $s_i \in S$ such that $a_i < a_j$ whenever $i < j$, then $F_X$ is a non-decreasing {\color{red} \textbf{step function}}, i.e., for all $a$ such that $a_i \leq a < a_{i + 1}$,
        \begin{equation*}
            F_X(a) = \sum_{x \leq a}p_X(x) = \sum_{k = 1}^{i}p_X(a_k).
        \end{equation*}
    \end{remark}
\end{notebox}
\subsection{Expectation of Discrete Random Variables}
Suppose $X$ is a discrete random variable with range $\left\{x_1, x_2, \cdots, x_m\right\}$ and PMF $p_X$. By repeating an experiment of $X$ for $n$ times, we can approximate the total number of occurrences of $X = x_i$ by $np_X(x_i)$. Therefore, the average value of $X$ can be approximated by
\begin{equation*}
    \frac{\sum_{i = 1}^{m}nx_ip_X(x_i)}{n}.
\end{equation*}
For large $n$, we have
\begin{equation*}
    \lim_{n \to \infty}\frac{\sum_{i = 1}^{m}nx_ip_X(x_i)x_i}{n} = \lim_{n \to \infty}\sum_{i = 1}^{m}x_ip_X(x_i) = \sum_{i = 1}^{m}x_ip_X(x_i).
\end{equation*}
Similarly, if the range of $X$ is countably infinite, replacing from the above $\sum_{i = 1}^{m}nx_ip_X(x_i)x_i$ with $\sum_{i = 1}^{\infty}nx_ip_X(x_i)x_i$ will yield the same limit.

Intuitively, the above limit represents the \textit{expected value} of $X$ when a large number of experiments are conducted, which leads to the following definition:
\begin{dfnbox}{Expectation of Discrete Random Variables}{DRVExp}
    Let $X$ be a discrete random variable. The {\color{red} \textbf{expectation}} (or {\color{red} \textbf{mean}}, {\color{red} \textbf{expected value}}) of~$X$ is defined to be
    \begin{equation*}
        E[X] = \sum_{i = 1}^{m}\left[x_ip_X(x_i)\right] = \sum_{i = 1}^{m}\left[x_iP(X = x_i)\right]
    \end{equation*}
    if $\abs{\mathrm{ran}(X)} = m$, and
    \begin{equation*}
        E[X] = \sum_{i = 1}^{\infty}\left[x_ip_X(x_i)\right] = \sum_{i = 1}^{\infty}\left[x_iP(X = x_i)\right]
    \end{equation*}
    if $\mathrm{ran}(X)$ is countably infinite.
\end{dfnbox}
By convention, we use $\mu$ to denote expectation, so $X[E]$ can be written as $\mu_X$.

For a discrete random variable $X$, we can define a function $g \colon \mathrm{ran}(X) \to \R$. It is easy to see that $g(X)$ is also a discrete random variable. Therefore, we may have the following result:
\begin{thmbox}{Expectation of Functions}{funcExp}
    Let $X$ be a discrete random variable and define $Y = g(X)$, then
    \begin{equation*}
        E[Y] = E[g(X)] = \sum_{x}\left[g(x)P(X = x)\right].
    \end{equation*}
    \tcblower
    \begin{proof}
        Note that for each $y \in \mathrm{ran}(Y)$, $g(x) = y$ for some $x \in \mathrm{ran}(X)$, and so
        \begin{equation*}
            P(Y = y) = \sum_{g(x) = y}P(X = x).
        \end{equation*}
        Therefore,
        \begin{align*}
            E[Y] & = \sum_{y}\left[yP(Y = y)\right] \\
            & = \sum_{y}\left[y\!\sum_{g(x) = y}\!P(X = x)\right] \\
            & = \sum_{y}\left[\sum_{g(x) = y}\!g(x)P(X = x)\right] \\
            & = \sum_{x}\left[g(x)P(X = x)\right].
        \end{align*}
    \end{proof}
\end{thmbox}
Two simple corollaries to the above theorem are:
\begin{align*}
    E[aX + b] & = aE[X] + b \\
    E[X + Y] & = E[X] + E[Y].
\end{align*}
In later sections, we will prove that the same rule applies to continuous random variables as well. The above theorem gives rise to the following notion of \textit{moments}:
\begin{dfnbox}{Moment}{moment}
    Let $X$ be a random variable. $E[X^n]$ is called the $n$-th {\color{red} \textbf{moment}} of $X$.
\end{dfnbox}
Following Theorem \ref{thm:funcExp}, it is easy to see that if $X$ is discrete, then
\begin{equation*}
    E[X^n] = \sum_{i = 1}^{\infty}x_i^np_X(x_i).
\end{equation*}

\subsection{Variance}
Note that given two different discrete random variables $X$ and $Y$, their probability mass functions can be different but they can still have identical expectations. For example, consider $p_X$ to be identically $0$ and $p_Y$ to be such that $p_Y(0) = 1$ and $p_Y(y) = 0$ for all $y \neq 0$.

This motivates us to find other properties to classify and characterise random variables. One of these properties is the \textbf{spread} of the possible values taken by a random variable with respect to its mean, i.e., consider the random variable $X$ with $E[X] = \mu$, we wish to determine $E[\abs{X - \mu}]$ or equivalently $E[(X - \mu)^2]$. This spread is known as the \textit{variance} of a random variable.
\begin{dfnbox}{Variance}{var}
    Let $X$ be a random variable with $E[X] = \mu$, the {\color{red} \textbf{variance}} of $X$ is defined to be
    \begin{equation*}
        \mathrm{Var}(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2.
    \end{equation*}
\end{dfnbox}
By convention, we use $\sigma^2$ to denote variance, so $\mathrm{Var}(X)$ can be written as $\sigma_X^2$.

The formula for $\mathrm{Var}(X)$ can be derived via Theorem \ref{thm:funcExp}:
\begin{align*}
    \mathrm{Var}(X) & = E[(X - \mu)^2] \\
    & = E[X^2 - 2\mu X + \mu^2] \\
    & = E[X^2] - 2\mu E[X] + \mu^2 \\
    & = E[X^2] - 2(E[X])^2 + (E[X])^2 \\
    & = E[X^2] - (E[X])^2.
\end{align*}
Another term we hear often is \textit{standard deviation}, which is defined as follows:
\begin{dfnbox}{Standard Deviation}{sd}
    Let $X$ be a random variable, the {\color{red} \textbf{standard deviation}} of $X$ is defined to be
    \begin{equation*}
        \mathrm{SD}(X) = \sqrt{\mathrm{Var}(X)} = \sigma_X.
    \end{equation*}
\end{dfnbox}
Note that we have computed the general formula for any linear combination of discrete random variables. We shall do the same for variance.
\begin{probox}{Variance of Linear Combinations of Random Variables}{linearCombiVar}
    Let $X$ be a random variable, then
    \begin{align*}
        \mathrm{Var}(aX + b) & = a^2\mathrm{Var}(X) \\
        \mathrm{SD(aX + b)} & =\abs{a}\mathrm{SD}(X)
    \end{align*}
    for all $a, b \in \R$.
    \tcblower
    \begin{proof}
        By using Theorem \ref{thm:funcExp}, we have
        \begin{align*}
            \mathrm{Var}(aX + b) & = E[(aX + b)^2] - (E[aX + b])^2 \\
            & = a^2 E[X^2] + 2ab E[X] + b^2 - \left[a(E[X])^2 + 2abE[X] + b^2\right] \\
            & = a^2\left[E[X^2] - (E[X])^2\right] \\
            & = a^2\mathrm{Var}(X).
        \end{align*}
        Therefore,
        \begin{equation*}
            \mathrm{SD}(aX + b) = \sqrt{\mathrm{Var}(aX + b)} = \abs{a}\mathrm{SD}(X).
        \end{equation*}
    \end{proof}
\end{probox}

\subsection{Bernoulli and Binomial Random Variables}
Suppose we conduct an experiment. In the most simplistic view, only two outcomes are considered, namely \textbf{success} and \textbf{failure}. We can model such experiments using a discrete random variable whose range has a cardinality of $2$.
\begin{dfnbox}{Bernoulli Random Variable}{bernoulliRV}
    A random variable $X$ is a {\color{red} \textbf{Bernoulli random variable}} if
    \begin{align*}
        p_X(x) = \begin{cases}
            p, \quad & \textrm{if } x = 1 \\
            1 - p, \quad & \textrm{if } x = 0 \\
            0, \quad & \textrm{otherwise}
        \end{cases}
    \end{align*}
    for some $p \in [0, 1]$.
\end{dfnbox}
Now, consider $n$ \textbf{independent} trials of an experiment with a probability for success of $p$. Let $X$ be the number of successes among these $n$ trials, then clearly,
\begin{equation*}
    p_X(x) = P(X = x) = \begin{pmatrix}
        n \\
        x
    \end{pmatrix}p^x(1 - p)^{n - x}.
\end{equation*}
\begin{dfnbox}{Binomial Random Variable}{binRV}
    A random variable $X$ is a {\color{red} \textbf{binomial random variable}} if
    \begin{equation*}
        p_X(x) = P(X = x) = \begin{pmatrix}
            n \\
            x
        \end{pmatrix}p^x(1 - p)^{n - x}
    \end{equation*}
    for some $p \in [0, 1]$. $X$ is said to have a {\color{red} \textbf{binomial distribution}} with parameters $(n, p)$, denoted by $X \sim \mathrm{B}(n, p)$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        In particular, if $X$ is a Bernoulli random variable, then $X \sim \mathrm{B}(1, p)$.
    \end{remark}
\end{notebox}
Suppose $X \sim \mathrm{B}(n, p)$. Let $N$ be the average number of successes in the $n$ trials, it is expected that $p \approx \frac{N}{n}$. Therefore, we may conjure that $N \approx np$.
\begin{thmbox}{Expectation and Variance of Binomial Distribution}{BExpVar}
    Let $X \sim B(n, p)$, then $E[X] = np$ and $\mathrm{Var}(X) = np(1 - p)$.
    \tcblower
    \begin{proof}
        Note that $iC^n_i = nC^{n - 1}_{i - 1}$, so
        \begin{align*}
            E[X] & = \sum_{i = 0}^{n}\left[i \begin{pmatrix}
                n \\
                i
            \end{pmatrix}p^i(1 - p)^{n - i}\right] \\
            & = \sum_{i = 1}^{n}\left[n \begin{pmatrix}
                n - 1 \\
                i - 1
            \end{pmatrix}p^i(1 - p)^{n - i}\right] \\
            & = n\sum_{j = 0}^{n - 1}\left[\begin{pmatrix}
                n - 1 \\
                j
            \end{pmatrix}p^{j + 1}(1 - p)^{n - 1 - j}\right] \\
            & = np\sum_{j = 0}^{n - 1}\left[\begin{pmatrix}
                n - 1 \\
                j
            \end{pmatrix}p^j(1 - p)^{n - 1 - j}\right] \\
            & = np, \\
            \mathrm{Var}(X) & = E[X^2] - (E[X])^2 \\
            & = \sum_{i = 0}^{n}\left[i^2\begin{pmatrix}
                n \\
                i
            \end{pmatrix}p^i(1 - p)^{n - i}\right] - n^2p^2 \\
            & = n\sum_{i = 1}^{n}\left[i\begin{pmatrix}
                n - 1 \\
                i - 1
            \end{pmatrix}p^i(1 - p)^{n - i}\right] - n^2p^2 \\
            & = n\sum_{j = 0}^{n - 1}\left[(j + 1)\begin{pmatrix}
                n - 1 \\
                j
            \end{pmatrix}p^{j + 1}(1 - p)^{n - 1 - j}\right] - n^2p^2 \\
            & = np\left\{\sum_{j = 0}^{n - 1}\left[j\begin{pmatrix}
                n - 1 \\
                j
            \end{pmatrix}p^j(1 - p)^{n - 1 - j}\right] + 1\right\} - n^2p^2 \\
            & = np\left[(n - 1)p + 1\right] - n^2p^2 \\
            & = np - np^2 \\
            & = np(1 - p).
        \end{align*}
    \end{proof}
\end{thmbox}
Let $X \sim \mathrm{B}(n, p)$, consider
\begin{align*}
    \frac{p_X(i + 1)}{p_X(i)} & = \frac{\frac{n!}{(i + 1)!(n - i - 1)!}p^{i + 1}(1 - p)^{n - i - 1}}{\frac{n!}{i!(n - i)!}p^i(1 - p)^{n - i}} \\
    & = \frac{\frac{1}{i + 1}p}{\frac{1}{n - i}(1 - p)} \\
    & = \frac{(n - i)p}{(i + 1)(1 - p)}.
\end{align*}
Suppose $p_X(i + 1) < p_X(i)$, then $(n - i)p < (i + 1)(1 - p)$. This implies that $i > (n + 1)p - 1$, which means that
\begin{itemize}
    \item $p_X(i)$ is monotonically increasing on $\left[0, (n + 1)p - 1\right]$.
    \item $p_X(i)$ maximises when $i = \left\lceil (n + 1)p - 1 \right\rceil = \left\lfloor (n + 1)p \right\rfloor$.
    \item $p_X(i)$ is monotincally decreasing on $\left((n + 1)p - 1, n\right]$.
\end{itemize}

\subsection{Poisson Random Variable}
Suppose that $X \sim \mathrm{B}(n, p)$ such that $n$ is large and $p$ is small. Let $\lambda = np$, then
\begin{align*}
    p_X(i) & = \begin{pmatrix}
        n \\
        i
    \end{pmatrix}p^i(1 - p)^{n - i} \\
    & = \frac{\prod_{j = 0}^{i - 1}(n - j)}{i!}\left(\frac{\lambda}{n}\right)^i\left(1 - \frac{\lambda}{n}\right)^{n - i} \\
    & = \frac{\prod_{j = 0}^{i - 1}(n - j)}{n^i}\cdot\frac{\lambda^i}{i!}\cdot\frac{\left(1 - \frac{\lambda}{n}\right)^n}{\left(1 - \frac{\lambda}{n}\right)^i}
\end{align*}
Therefore,
\begin{equation*}
    \lim_{n \to \infty}p_X(i) = e^{-\lambda}\frac{\lambda^i}{i!}.
\end{equation*}
Note that this means that we can use $e^{-\lambda}\frac{\lambda^i}{i!}$ as a good approximation for $p_X(i)$ when $n$ is large and $p$ is small! In this case, $\lambda$ is the expected frequency of occurrences of the event corresponding to $X = 1$ within a unit interval.
\begin{dfnbox}{Poisson Random Variable}{poisson}
    A random variable $X$ is a {\color{red} \textbf{Poisson random variable}} if 
    \begin{equation*}
        p_X(x) = P(X = x) = e^{-\lambda}\frac{\lambda^x}{x!}
    \end{equation*}
    for some $\lambda > 0$, denoted as $X \sim \mathrm{Po}(\lambda)$.
\end{dfnbox}
Note that for $X \sim \mathrm{Po}(\lambda)$, we can find some $Y \sim \mathrm{B}(n, p)$ where $n$ is large and $p$ is small such that $np = \lambda$. Therefore, it is expected that 
\begin{align*}
    E[X] & \approx E[Y] = \lambda, \\
    \mathrm{Var}(X) & \approx \mathrm{Var}(Y) = np(1 - p) \approx \lambda.
\end{align*}
\begin{thmbox}{Expectation and Variance of Poisson Random Variables}{poExpVar}
    If $X \sim \mathrm{Po}(\lambda)$ where $\lambda > 0$, then $E[X] = \mathrm{Var}(X) = \lambda$.
\end{thmbox}
\begin{dfnbox}{Weakly Dependent}{weakDepend}
    Let $E$ and $F$ be two events. If $P(E) \approx P(E\mid F)$, we say that $E$ and $F$ are {\color{red} \textbf{weakly dependent}}.
\end{dfnbox}
Let $i = 1, 2, 3, \cdots, n$ and $p_i$ be the probability of event $i$ occurring. If the $i$'s are independent or weakly dependent, then we can approximate for large $n$ that the rate of occurrences of these events is $\sum_{i = 1}^{n}p_i$. Let $X$ be the number of events which occur, then 
\begin{equation*}
    X \sim \mathrm{Po}\left(\sum_{i = 1}^{n}p_i\right).
\end{equation*}
\begin{thmbox}{Poisson Process}{poissonProcess}
    Let $E$ be an event which occurs randomly. Assume that
    \begin{enumerate}
        \item there are $\lambda$ occurrences per unit interval;
        \item no two occurrences happen at the same point;
        \item numbers of occurrences in disjoint intervals are independent.
    \end{enumerate}
    Let $N(t)$ be the number of occurrences of $E$ in an interval of length $t$, then $N(t) \sim \mathrm{Po}(\lambda t)$.
\end{thmbox}

\subsection{Geometric Random Variable}
Suppose we perform some experiment with a probability of success of $p$. Let $X$ be the number of failures before the first success occurs, then clearly,
\begin{equation*}
    P(X = x) = (1 - p)^xp.
\end{equation*}
Additionally, let $Y$ be the number of trials needed to reach the first success, then
\begin{equation*}
    P(Y = y) = (1 - p)^{y - 1}p.
\end{equation*}
Note that both $\bigl(P(X = x)\bigr)$ and $\bigl(P(Y = y)\bigr)$ form geometric sequences.
\begin{dfnbox}{Geometric Random Variable}{geoDRV}
    A random variable $X$ is called a {\color{red} \textbf{geometric random variable}} with parameter $p \in (0, 1)$, denoted by $X \sim \mathrm{Geo}(p)$, if
    \begin{equation*}
        p_X(n) = (1 - p)^{n - 1}p.
    \end{equation*}
\end{dfnbox}
\begin{thmbox}{Expectation and Variance of Geometric Random Variables}{expVarGeo}
    If $X \sim \mathrm{Geo}(p)$, then $E[X] = \frac{1}{p}$ and $\mathrm{Var}(X) = \frac{1 - p}{p^2}$.
\end{thmbox}
\subsection{Negative Binomial Random Variable}{negBinDRV}
Suppose we perform some experiment with a probability of success of $p$. Let $X$ be the number of trials needed to achieve the $r$-th success, then clearly, for $X = n$, we need $(r - 1)$ successes (i.e., $(n - r)$ failures) in the first $(n - 1)$ trials and the $r$-th trial to be a success. Therefore,
\begin{equation*}
    P(X = n) = C^{n - 1}_{r - 1}p^{r - 1}(1 - p)^{n - r}p = C^{n - 1}_{r - 1}p^r(1 - p)^{n - r}.
\end{equation*}
\begin{dfnbox}{Negative Binomial Random Variable}{negBinDRV}
    A random variable $X$ is called a {\color{red} \textbf{negative binomial random variable}} if
    \begin{equation*}
        p_X(n) = \begin{pmatrix}
            n - 1 \\
            r - 1
        \end{pmatrix}p^r(1 - p)^{n - r},
    \end{equation*}
    where $0 < p < 1$ and $n \geq r$, denoted as $X \sim \mathrm{NB}(r, p)$.
\end{dfnbox}
\begin{thmbox}{Expection and Variance of Negative Binomial Variables}{expVarNegBin}
    Let $X \sim \mathrm{NB}(r, p)$, then
    \begin{equation*}
        E[X] = \frac{r}{p}, \qquad \mathrm{Var}(X) = \frac{r(1 - p)}{p^2}.
    \end{equation*}
\end{thmbox}

\subsection{Hypergeometric Random Variable}
Suppose a collection contains $N$ objects, $m$ of which are of type A. If $n$ objects are selected randomly without replacement and let $X$ be the number of objects of type A selected, then 
\begin{equation*}
    P(X = x) = \frac{\begin{pmatrix}
        m \\
        x
    \end{pmatrix}\begin{pmatrix}
        N - m\\
        n - x
    \end{pmatrix}}{\begin{pmatrix}
        N \\
        n
    \end{pmatrix}}.
\end{equation*}
\begin{dfnbox}{Hypergeometric Random Variable}{hGeo}
    A random variable $X$ is called a {\color{red} \textbf{hypergeometric random variable}} with parameters $(n, N, m)$ if
    \begin{equation*}
        p_X(x) = \frac{\begin{pmatrix}
            m \\
            x
        \end{pmatrix}\begin{pmatrix}
            N - m\\
            n - x
        \end{pmatrix}}{\begin{pmatrix}
            N \\
            n
        \end{pmatrix}},
    \end{equation*}
    where $0 \leq m, n \leq N$.
\end{dfnbox}
\begin{thmbox}{\small Expection and Variance of Hypergeometric Random Variables}{expVarHGeo}
    Let $X$ be a hypergeometric random variable with parameters $(n, N, m)$, then
    \begin{equation*}
        E[X] = np, \qquad \mathrm{Var}(X) = np(1 - p)\left(1 - \frac{n - 1}{N - 1}\right).
    \end{equation*}
\end{thmbox}

\section{Continuous Random Variables}
In real life, the outcomes of certain events are infinitely many, and so they cannot be enumerated as discrete cases. Thus, we will need to use \textit{continuous random variables} to model these events.
\begin{dfnbox}{Continuous Random Variable}{CRV}
    A random variable $X$ is a {\color{red} \textbf{continuous random variable}} if there exists some non-negative function $f_X$ such that for all $B \subseteq \R$,
    \begin{equation*}
        P(X \in B) = \int_B\!f_X(x)\,\mathrm{d}x.
    \end{equation*}
    The function $f_X$ is known as the {\color{red} \textbf{probability density function}} of $X$. The function $F_X$ with $0 \leq F_X(x) \leq 1$ and $F_X'(x) = f_X(x)$ is known as the {\color{red} \textbf{cumulative distribution function}} of $X$.
\end{dfnbox}
An interesting property of a continuous random variable $X$ is that
\begin{equation*}
    P(X = x) = \int_{x}^{x}\!f_X(x) \,\mathrm{d}x = 0,
\end{equation*}
which means that the probability of any single outcome of an event is $0$, but this does not mean that it is impossible to occur! In particular, it is more meaningful to consider the probability of the occurrence of a range of outcomes. We have
\begin{align*}
    P(a \leq X \leq b) & = P(a < X < b) = P(a \leq X < b) = P(a < X \leq b) = \int_{a}^{b}\!f_X(x) \,\mathrm{d}x, \\
    P(X \leq a) & = P(X < a) = \int_{-\infty}^{a}\!f_X(x) \,\mathrm{d}x.
\end{align*}
It is not surprising that a function of a continuous random variable is still a continuous random variable.
\begin{thmbox}{Function of Continuous Random Variables}{funcCRV}
    Let $X$ be a continuous random variable and $Y = g(X)$. If $g$ is strictly monotonic and differentiable, then 
    \begin{equation*}
        f_Y(y) = f_X\left(g^{-1}(y)\right)\abs{\frac{\dif}{\dif y}g^{-1}(y)}.
    \end{equation*}
\end{thmbox}
Let $X$ be a continuous random variable with probability density function $f_X$ such that $f_X(x) = 0$ for all $x \in \R - [a, b]$. We divide $[a, b]$ into $n$ intervals $[x_{i - 1}, x_i]$ for $i = 1, 2, 3, \cdots, n$ with equal length $\symrm{\Delta} x = \frac{b - a}{n}$. Thus,
\begin{equation*}
    P(x_{i - 1} < X < x_i) \approx \symrm{\Delta}xf_X(x_i).
\end{equation*}
Let $Y$ be a discrete random variable with $P(Y = x_i) = \symrm{\Delta}xf_X(x_i)$, then 
\begin{equation*}
    E[X] \approx E[Y] = \sum_{i = 1}^{n}x_i\symrm{\Delta}xf_X(x_i).
\end{equation*}
When $n \to \infty$, i.e., $\symrm{\Delta}x \to 0$, we have
\begin{equation*}
    \lim_{\symrm{\Delta}x \to 0}E[Y] = \int_{a}^{b}\!xf_X(x)\,\mathrm{d}x.
\end{equation*}
By letting $a \to -\infty$ and $b \to \infty$, we have arrived at the following definition:
\begin{dfnbox}{Expectation of Continuous Random Variables}{expCRV}
    Let $X$ be a continuous random variable with probability density function $f_X$, then
    \begin{equation*}
        E[X] = \int_{-\infty}^{\infty}\!xf_X(x)\,\mathrm{d}x.
    \end{equation*}
\end{dfnbox}
Let $Y$ be a continuous random variable with probability density function $f$, consider
\begin{align*}
    \int_{0}^{\infty}\!P(Y > y)\,\mathrm{d}y & = \int_{0}^{\infty}\!\int_{y}^{\infty}\!f(x)\,\mathrm{d}x\,\mathrm{d}y \\
    & = \int_{0}^{\infty}\!\int_{0}^{x}\!f(x)\,\mathrm{d}y\,\mathrm{d}x \\ 
    & = \int_{0}^{\infty}\!xf(x)\,\mathrm{d}y\,\mathrm{d}x \\ 
    & = E[Y].
\end{align*}
Therefore, set $Y = g(X)$, then similar to discrete random variables, we have
\begin{equation*}
    E\left[g(X)\right] = \int_{-\infty}^{\infty}\!g(x)f_X(x)\,\mathrm{d}x.
\end{equation*}

\subsection{Uniform Random Variable}
Intuitively, we may call a random variable $X$ ``uniformly'' distributed in $(a, b)$ if $P(X = x)$ is a constant for all $x \in (a, b)$.
\begin{dfnbox}{Uniform Random Variable}{uniCRV}
    A continuous random variable $X$ is a {\color{red} \textbf{uniform random variable}} if
    \begin{equation*}
        f_X(x) = \begin{cases}
            \frac{1}{b - a}, & \quad\textrm{if } a < x < b \\
            0, & \quad\textrm{otherwise}
        \end{cases},
    \end{equation*}
    denoted by $X \sim \mathrm{U}(a, b)$.
\end{dfnbox}
Let $X \sim \mathrm{U}(a, b)$, then the cumulative density function is
\begin{equation*}
    F_X(x) = \begin{cases}
        0, & \quad\textrm{if } x \leq a \\
        \frac{x - a}{b - a}, & \quad\textrm{if } a < x < b \\
        1, & \quad\textrm{otherwise}
    \end{cases}.
\end{equation*}
\begin{thmbox}{Expectation and Variance of Uniform Random Variables}{expVarUni}
    Let $X \sim \mathrm{U}(a, b)$, then $E[X] = \frac{a + b}{2}$ and $\mathrm{Var}(X) = \frac{(b - a)^2}{12}$.
\end{thmbox}

\subsection{Normal Random Variable}
\begin{dfnbox}{Normal Random Variable}{normal}
    A continuous random variable $Z$ with probability density function $\phi$ is a {\color{red} \textbf{normal random variable}} if 
    \begin{equation*}
        \phi(z) = \frac{1}{\sqrt{2\symrm{\pi}}\sigma}e^{-\frac{(z - \mu)^2}{2\sigma^2}},
    \end{equation*}
    denoted as $Z \sim \mathcal{N}\left(\mu, \sigma^2\right)$.
\end{dfnbox}
In particular, $Z \sim \mathcal{N}(0, 1)$ is known as the \textit{standard normal random variable}. Let $\Phi$ be the cumulative density function for $Z$, then 
\begin{equation*}
    \Phi(z) = P(Z < z) = \frac{1}{\sqrt{2\symrm{\pi}}}\int_{-\infty}^{z}\!\mathrm{e}^{-\frac{t^2}{2}} \,\mathrm{d}t.
\end{equation*}
Let $X \sim \mathcal{N}(\mu, \sigma^2)$, then $Z = \frac{X - \mu}{\sigma}$, so
\begin{equation*}
    \Phi_X(x) = P\left(X < x\right) = P\left(\frac{X - \mu}{\sigma} < \frac{x - \mu}{\sigma}\right) = \Phi\left(\frac{x - \mu}{\sigma}\right)
\end{equation*}
\begin{thmbox}{Expection and Variance of Normal Random Variables}{expVarNormal}
    Let $Z \sim \mathcal{N}(\mu, \sigma^2)$, then $E[Z] = \mu$ and $\mathrm{Var}(Z) = \sigma^2$.
\end{thmbox}

\subsection{Exponential Random Variable}
Let $N(t) \sim \mathrm{Po}(t\lambda)$ be the number of occurrences of an event in an interval of length $t$. Suppose $X$ is the time before the first occurrence of the event, then
\begin{equation*}
    P(X > t) = P(N(t) = 0) = \mathrm{e}^{-\lambda t}.
\end{equation*}
In other words, if $F_X$ is the cumulative distribution function of $X$, then $F_X(x) = 1 - \mathrm{e}^{-\lambda t}$.
\begin{dfnbox}{Exponential Random Variable}{expCRV}
    A continuous random variable $X$ is an {\color{red} \textbf{exponential random variable}} if 
    \begin{equation*}
        f_X(x) = \begin{cases}
            \lambda\mathrm{e}^{-\lambda x}, & \quad\textrm{if } x \geq 0 \\
            0, & \quad\textrm{otherwise}
        \end{cases},
    \end{equation*}
    where $\lambda > 0$, denoted as $X \sim \mathrm{Exp}\left(\lambda\right)$.
\end{dfnbox}
\begin{thmbox}{Expection and Variance of Exponential Random Variables}{expVarExp}
    Let $X \sim \mathrm{Exp}(\lambda)$, then $E[X] = \frac{1}{\lambda}$ and $\mathrm{Var}(X) = \frac{1}{\lambda^2}$.
\end{thmbox}
Informally, an exponential random variable models the \textbf{waiting time} before an event occurs. Suppose we have already waited for $s$ unit of time for the occurrence, we may wish to know the probability of us having to wait for another $t$ unit of time. To solve such questions, we need to understand the \textit{memoryless} property.
\begin{dfnbox}{Memoryless Property}{memoryless}
    Let $X$ be a random variable, we say that $X$ is {\color{red} \textbf{memoryless}} if
    \begin{equation*}
        P(X > s + t \mid X > t) = P(X > s). 
    \end{equation*}
\end{dfnbox}
In particular, if $X \sim \mathrm{Exp}(\lambda)$, consider
\begin{align*}
    P(X > s + t \mid X > t) & = \frac{\mathrm{e}^{-\lambda(s + t)}}{\mathrm{e}^{-\lambda t}} \\
    & = \mathrm{e}^{-\lambda s} \\
    & = P(X > s).
\end{align*}
Therefore, exponential random variables are memoryless. One may also prove that geometric random variables are also memoryless.

Now we introduce another random variable which is closely related to the exponential random variable.
\begin{dfnbox}{Double Exponential Random Variable}{doubleExpCRV}
    A continuous random variable $X$ is a {\color{red} \textbf{double exponential variable}} if 
    \begin{equation*}
        f_X(x) = \frac{1}{2}\lambda x^{-\lambda\abs{x}}
    \end{equation*}
    for some $\lambda > 0$.
\end{dfnbox}
Consider $Y = \abs{X}$ where $X$ is a double exponential random variable. The double exponential random variable is so named because for any $y \geq 0$,
\begin{align*}
    P(Y > y) & = P(X > y) + P(X < -y) \\
    & = 2P(X > y) \\
    & = 2\int_{y}^{\infty}\!\frac{1}{2}\lambda \mathrm{e}^{-\lambda x}\,\mathrm{d}x \\
    & = \mathrm{e}^{-\lambda y}.
\end{align*}
Thus, $Y = \abs{X} \sim \mathrm{Exp}(\lambda)$.

A common application of exponential random variables is to determine the \textit{hazard rate}. Suppose $X$ is the survival time of some object and that the object has already survived for a time $t$. Consider $\epsilon$ to be a small interval, then the probability that the object cannot survive past this small interval is approximately
\begin{align*}
    P(X < t + \epsilon \mid X > t) & = \frac{P(t < X < t + \epsilon)}{P(X > t)} \\
    & \approx \frac{\epsilon f_X(t)}{1 - F_X(t)}.
\end{align*}
In general, we have the following definition:
\begin{dfnbox}{Hazard Rate Function}{hazardRatef}
    Let $X$ be a positive continuous random variable and define $\overline{F_X}(x) = 1 - F_X(x)$, then the function
    \begin{equation*}
        \lambda(x) = \frac{f_X(x)}{\overline{F_X}(x)}
    \end{equation*}
    is known as the {\color{red} \textbf{hazard rate function}} of $X$.
\end{dfnbox}
In particular, if $X \sim \mathrm{Exp}(\lambda)$, then its hazard rate funtion is just $\lambda(x) = \lambda$, which is also known as the \textit{rate} of $X$.

\subsection{Gamma Random Variable}
We have seen that the exponential random variable can be used to model the waiting time between two consecutive occurrences of an event modelled by a Poisson random variable. We would also like to know the waiting time till the $n$-th occurrence of an event.

Let $N(t)$ be a Poisson process with rate $\lambda$, so $P\bigl(N(t) = n\bigr) = \frac{\e^{-\lambda t\left(\lambda t\right)^n}}{n!}$. Let $T_n$ be the waiting time till the $n$-th event with $f_k$ being the probability density function, then
\begin{align*}
    f_k(t) & = \frac{\dif}{\dif t}\bigl(1 - P(T_n > t)\bigr) \\
    & = \frac{\dif}{\dif t}\Bigl(1 - P\bigl(N(t) < k\bigr)\Bigr) \\
    & = \frac{\dif}{\dif t}\left(1 - \e^{-\lambda t}\sum_{i = 0}^{k - 1}\frac{(\lambda t)^i}{i!}\right) \\
    & = \frac{\lambda\e^{-\lambda t}(\lambda t)^{k - 1}}{(k - 1)!}.
\end{align*}
\begin{dfnbox}{Gamma Random Variable}{gamma}
    A continuous random variable $X$ is called a {\color{red} \textbf{gamma}} random variable with parameters $(n, \lambda)$ where $\lambda > 0$ if 
    \begin{equation*}
        f(t) = \frac{\lambda\e^{-\lambda t}(\lambda t)^{n - 1}}{(n - 1)!}
    \end{equation*}
    for all $t \geq 0$.
\end{dfnbox}
\begin{dfnbox}{Gamma Function}{gammaFunc}
    Let $\alpha > 0$, the {\color{red} \textbf{gamma function}} is defined as
    \begin{align*}
        \Gamma(\alpha) & = \int_{0}^{\infty}\!\lambda\e^{-\lambda t}(\lambda t)^{\alpha - 1}\,\dif t \\
        & = \int_{0}^{\infty}\!\lambda\e^{-x}x^{\alpha - 1}\,\dif x.
    \end{align*}
    In particular, if $X$ is a continuous random variable with probability density function
    \begin{equation*}
        f(t) = \frac{\lambda\e^{-\lambda t}(\lambda t)^{\alpha - 1}}{\Gamma(\alpha)},
    \end{equation*}
    where $\lambda > 0$ and $t \geq 0$, then $X$ is a gamma random variable with parameters $(\alpha, \lambda)$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        One can prove that
        \begin{enumerate}
            \item $\Gamma(\alpha + 1) = \alpha\Gamma(\alpha)$ for all $\alpha > 0$.
            \item $\Gamma(n) = (n - 1)!$ for all $n \in \Z^+$.
        \end{enumerate}
    \end{remark}
\end{notebox}
\begin{thmbox}{Expectation and Variance of Gamma Random Variables}{expVarGamma}
    Let $X$ be a gamma random variable with parameters $(\alpha, \lambda)$, then 
    \begin{equation*}
        E[X] = \frac{\alpha}{\lambda}, \qquad \mathrm{Var}(X) = \frac{\alpha}{\lambda^2}.
    \end{equation*}
\end{thmbox}

\subsection{Beta Random Variable}
In real life, sometimes we may not know the exact probability distribution of a random variable and wish to deduce its probability distribution based on experiments. In other words, suppose there are $a$ successes and $b$ failures of an experiment, we wish to know what is the \textbf{most likely} probability of success.
\begin{dfnbox}{Beta Random Variable}{beta}
    A continuous random variable $X$ is a {\color{red} \textbf{beta}} random variable with parameters $(a, b)$ with $a, b > 0$ if 
    \begin{equation*}
        f_X(x) = \frac{1}{B(a, b)}x^{a - 1}(1 - x)^{b - 1},
    \end{equation*}
    where 
    \begin{equation*}
        B(a, b) = \int_{0}^{1}\!x^{a - 1}(1 - x)^{b - 1}\,\dif x = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a + b)}.
    \end{equation*}
\end{dfnbox}
If $a = b = 1$, we see that
\begin{equation*}
    f_X(x) = \frac{\Gamma(2)}{\bigl(\Gamma(1)\bigr)^2},
\end{equation*}
so $X$ is uniform on $(0, 1)$. In particular, we also see that
\begin{equation*}
    \frac{B(a + 1, b)}{B(a, b)} = \frac{a}{a + b}.
\end{equation*}
\begin{thmbox}{Expectation and Variance of Beta Random Variables}{expVarBeta}
    Let $X$ be a beta random variable with parameters $(a, b)$, then 
    \begin{equation*}
        E[X] = \frac{a}{a + b}, \qquad \mathrm{Var}(X) = \frac{ab}{(a + b)^2(a + b + 1)}.
    \end{equation*}
\end{thmbox}

\section{Jointly Distributed Random Variables}
Sometimes, the outcomes of the events we wish to study cannot be expressed with a single random variable. In general, if $X_1, X_2, \cdots, X_n$ are random variables, we may be interested to know
\begin{equation*}
    P\bigl((X_1, X_2, \cdots, X_n) \in C\bigr), \qquad C \subseteq \R^n.
\end{equation*}
Take $C = \prod_{i = 1}^{n}(-\infty, x_i]$, then $(X_1, X_2, \cdots, X_n) \in C$ if and only if $X_i \leq x_i$ for $i = 1, 2, \cdots, n$. 
\begin{dfnbox}{Joint Cumulative Distribution Function}{jointCDF}
    Let $X_1, X_2, \cdots, X_n$ be random variables, then their {\color{red} \textbf{joint cumulative distribution function}} is defined as
    \begin{equation*}
        F_{\mathbfit{X}}(x_1, x_2, \cdots, x_n) = P(X_1 \leq x_1, X_2 \leq x_2, \cdots, X_n \leq x_n).
    \end{equation*}
\end{dfnbox}
One may be tempted to think that $P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y)$, but in general this is not true!
\begin{notebox}
    \begin{remark}
        $P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y)$ if and only if $X$ and $Y$ are independent random variables (Definition \ref{dfn:indRV}).
    \end{remark}
\end{notebox}
Consider two random variables $X$ and $Y$ jointly distributed, observe that
\begin{align*}
    P(x_1 \leq X \leq x_2, Y \leq y) & = P(X \leq x_2, Y \leq y) - P(X \leq x_1, Y \leq y) \\
    & = F_{X, Y}(x_2, y) - F_{X, Y}(x_1, y).
\end{align*}
Similarly,
\begin{equation*}
    P(X \leq x, y_1 \leq Y \leq y_2) = F_{X, Y}(x, y_2) - F_{X, Y}(x, y_1).
\end{equation*}
Combining the two equations we have
\begin{align*}
    P(x_1 \leq X \leq x_2, y_1 \leq Y \leq y_2) & = P(x \leq x_2, y_1 \leq Y \leq y_2) - P(x \leq x_1, y_1 \leq Y \leq y_2) \\
    & = F_{X, Y}(x_2, y_2) - F_{X, Y}(x_2, y_1) - F_{X, Y}(x_1, y_2) + F_{X, Y}(x_1, y_1).
\end{align*}
Note that with this identity we can compute $P\bigl((X, Y) \in C\bigr)$ for all Borel sets $C$. A \textit{Borel set} is a set generated by countable unions and countable intersections of intervals.

Consider jointly distributed discrete random variables. We first introduce the following intuitive definition:
\begin{dfnbox}{Joint Probability Mass Function}{jointPMF}
    Let $X_1, X_2, \cdots, X_n$ be discrete random variables. Their {\color{red} \textbf{joint probability mass function}} is defined to be
    \begin{equation*}
        p_{\mathbfit{X}}(x_1, x_2, \cdots, x_n) = P(X_1 = x_1, X_2 = x_2, \cdots, X_n = x_n).
    \end{equation*}
\end{dfnbox} 
In the continuous case, we have a similar definition:
\begin{dfnbox}{Joint Continuity}{jointPDF}
    Let $X_1, X_2, \cdots, X_n$ be continuous random variables. They are said to be {\color{red} \textbf{jointly continuous}} if there exists a {\color{red} \textbf{joint probability density function}} $f$ such that
    \begin{equation*}
        P\bigl((X_1, X_2, \cdots, X_n) \in C\bigr) = \int\cdots\int_{C}\!f_{\mathbfit{X}}(x_1, x_2, \cdots, x_n)\,\mathrm{d}x_1\cdots\mathrm{d}x_n.
    \end{equation*}
\end{dfnbox}
Let $X, Y$ be discrete random variables. It is easy to see that
\begin{align*}
    P(X = x) & = \sum_{i = 1}^{\infty}P(X = x, Y = y_i), \\
    P(Y = y) & = \sum_{i = 1}^{\infty}P(X = x_i, Y = y).
\end{align*}
On the other hand, if $X, Y$ are continuous random variables, we have
\begin{align*}
    P(X \in A) & = P(X \in A, Y \in \R) \\
    & = \int_{A}\int_{-\infty}^{\infty}\!f_{X, Y}(x, y)\,\mathrm{d}y\,\mathrm{d}x.
\end{align*}
These are known as \textit{marginal probability density functions}.
\begin{dfnbox}{Marginal Probability Density Function}{marginalPDF}
    Let $X, Y$ be discrete random variables. Their {\color{red} \textbf{marginal probability density functions}} are defined as
    \begin{align*}
        p_X(x) & = \sum_{i = 1}^{\infty}p_{X, Y}(x, y_i), \\
        p_Y(y) & = \sum_{i = 1}^{\infty}p_{X, Y}(x_i, y).
    \end{align*}
    If $X, Y$ are continuous random variables, then
    \begin{align*}
        f_X(x) & = \int_{-\infty}^{\infty}\!f_{X, Y}(x, y)\,\mathrm{d}y, \\
        f_Y(y) & = \int_{-\infty}^{\infty}\!f_{X, Y}(x, y)\,\mathrm{d}x.
    \end{align*}
\end{dfnbox}
Lastly, we state the notion of \textit{joint distribution of functions}.
\begin{thmbox}{Joint Distribution of Functions}{jointDistrFunc}
    Let $X_1$ and $X_2$ be jointly continuous random variables. Let $Y_1 = g_1(X_1, X_2)$ and $Y_2 = g_2(X_1, X_2)$. If for all $y_1, y_2$, the linear system
    \begin{equation*}
        \begin{bmatrix}
            y_1 \\
            y_2
        \end{bmatrix} = \begin{bmatrix}
            g_1(x_1, x_2) \\
            g_2(x_1, x_2)
        \end{bmatrix}
    \end{equation*}
    has a unique solution, and the Jacobian
    \begin{equation*}
        J(x_1, x_2) = \frac{\partial y_1}{\partial x_1}\cdot\frac{\partial y_2}{\partial x_2} - \frac{\partial y_1}{\partial x_2}\cdot \frac{\partial y_2}{\partial x_1}
    \end{equation*}
    is continuous and non-zero, then
    \begin{equation*}
        f_{Y_1, Y_2}(y_1, y_2) = f_{X_1, X_2}(x_1, x_2)\abs{J(x_1, x_2)}^{-1}.
    \end{equation*}
\end{thmbox}
\subsection{Independent Random Variables}
Recall that we have previously defined the notion of independent events (Definition \ref{dfn:indEvents}). Observe that an event can be precisely expressed as $X \in A$ for some random variable $X$ and set $A$, which motivates us to define independence of random variables.
\begin{dfnbox}{Independent Random Variables}{indRV}
    Two random variables $X$ and $Y$ are {\color{red} \textbf{independent}} if for any sets $A$ and $B$,
    \begin{equation*}
        P(X \in A, Y \in B) = P(X \in A)P(Y \in B).
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Less rigorously, notice that for Borel sets $A$ and $B$, since they can be expressed as countable unions and intersections of intervals, we can say that $X$ and $Y$ are independent if and only if $F_{X, Y}(x, y) = F_X(x)F_Y(y)$.
    \end{remark}
\end{notebox}
Since the cumulative distribution functions are closely related to the probability mass and probability density, we can then check for independence using them instead. We first consider the discrete case.
\begin{thmbox}{Independence of Discrete Random Variables}{indDRV}
    Let $X$ and $Y$ be discrete random variables with probability mass functions $p_X$ and $p_Y$. $X$ and $Y$ are independent if and only if $p_{X, Y}(x, y) = p_X(x)p_Y(y)$.
\end{thmbox}
Based on Theorem \ref{thm:indDRV}, one may check that if $X$ and $Y$ are independent discrete random variables, then 
\begin{align*}
    E[XY] & = E[X]E[Y], \qquad \mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y).
\end{align*}
In the continuous case, we have a similar conclusion.
\begin{thmbox}{Independence of Continuous Random Variables}{indCRV}
    Let $X$ and $Y$ be continuous random variables with probability density functions $f_X$ and $f_Y$. $X$ and $Y$ are independent if and only if $f_{X, Y}(x, y) = f_X(x)f_Y(y)$.
\end{thmbox}
The above theorems and definitions can be easily extended to a countable number of independent random variables.

\subsection{Sums of Independent Random Variables}
Suppose $X$ and $Y$ are independent \textbf{integer-valued} discrete random variables, then clearly
\begin{align*}
    p_{X + Y}(n) & = P(X + Y = n) \\
    & = \sum_{i}P(X = i, Y = n - i) \\
    & = \sum_{i}p_X(i)p_Y(n - i) \\
    & = \sum_{i + j = n}p_X(i)p_Y(j).
\end{align*}
On the other hand, if $X$ and $Y$ are independent continuous random variables, we have
\begin{align*}
    F_{X + Y}(n) & = \iint_{x + y \leq n}\!f_{X + Y}(x + y)\,\mathrm{d}x\,\mathrm{d}y \\
    & = \int_{-\infty}^{\infty}\int_{-\infty}^{n - y}\!f_X(x)f_Y(y)\,\mathrm{d}x\,\mathrm{d}y \\
    & = \int_{-\infty}^{\infty}\!F_X(n - y)f_Y(y)\,\mathrm{d}y \\
    & = \int_{-\infty}^{\infty}\!F_X(n - y)\,\mathrm{d}F_Y(y).
\end{align*}
We say that $F_{X + Y}$ is the \textit{convolution} $F_X * F_Y$.

Now, we proceed to discussing some special cases.

Let $X$ and $Y$ be independent uniform random variables on $(a, b)$. Note that $f_Y(y) = \frac{1}{b - a}$ for all $y \in (a, b)$, so
\begin{align*}
    f_{X + Y}(n) & = \int_{a}^{b}\!f_X(n - y)f_Y(y)\,\mathrm{d}y \\
    & = \frac{1}{b - a}\int_{a}^{b}\!f_X(n - y)\,\mathrm{d}y.
\end{align*}
Note that $f_X(n - y) = \frac{1}{b - a}$ if and only if $a < n - y < b$, i.e., $n - b < y < n - a$. Otherwise, $f_X(n - y) = 0$. If $2a < n < a + b$, we have $(a, b) \cap (n - b, n - a) = (a, n - a)$, so
\begin{align*}
    \frac{1}{b - a}\int_{a}^{b}\!f_X(n - y)\,\mathrm{d}y & = \frac{1}{b - a}\int_{a}^{n - a}\!\frac{1}{b - a}\,\mathrm{d}y \\
    & = \frac{n - 2a}{(b - a)^2}.
\end{align*}
If $a + b < n < 2b$, then $(a, b) \cap (n - b, n - a) = (n - b, b)$, so
\begin{align*}
    \frac{1}{b - a}\int_{a}^{b}\!f_X(n - y)\,\mathrm{d}y & = \frac{1}{b - a}\int_{n - b}^{b}\!\frac{1}{b - a}\,\mathrm{d}y \\
    & = \frac{2b - n}{(b - a)^2}.
\end{align*}
Therefore,
\begin{equation*}
    f_{X + Y}(n) = \begin{cases}
        \frac{n - 2a}{(b - a)^2} & \quad\textrm{if } 2a < n \leq a + b \\
        \frac{2b - n}{(b - a)^2} & \quad\textrm{if } a + b < n < 2b \\
        0 & \quad\textrm{otherwise}
    \end{cases}.
\end{equation*}
This is known as a \textit{triangular distribution}.

Let $X$ and $Y$ be independent gamma random variables with parameters $(\alpha, \lambda)$ and $(\beta, \lambda)$. One may check with some computation that
\begin{align*}
    f_{X + Y}(n) & = \frac{B(\alpha, \beta)}{\Gamma(\alpha)\Gamma(\beta)}\lambda\e^{-\lambda n}(\lambda n)^{\alpha + \beta - 1} \\
    & = \frac{1}{\Gamma(\alpha + \beta)}\lambda\e^{-\lambda n}(\lambda n)^{\alpha + \beta - 1}.
\end{align*}
Therefore, $X + Y$ is a gamma random variable with parameters $(\alpha + \beta, \lambda)$.

Lastly, we shall state the following theorem without proof:
\begin{thmbox}{Sum of Normal Random Variables Is Normal}{sumOfNormal}
    Let $X \sim N\left(\mu_X, \sigma_X^2\right)$ and $Y \sim N\left(\mu_Y, \sigma_Y^2\right)$, then 
    \begin{equation*}
        X + Y \sim N\left(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2\right).
    \end{equation*}
\end{thmbox}

\section{Conditional Distribution}
Recall that previously, we have defined the conditional probability
\begin{equation*}
    P(E \mid F) = \frac{P(EF)}{P(F)}.
\end{equation*}
Let $X$ and $Y$ be discrete random variables, we can similarly see that
\begin{align*}
    P(X = x \mid Y = y) & = \frac{P(X = x, Y = y)}{P(Y = y)} \\
    & = \frac{p_{X, Y}(x, y)}{p_Y(y)}.
\end{align*}
\begin{dfnbox}{Conditional Probability Mass Function}{condPMF}
    Let $X$ and $Y$ be discrete random variables with probability mass functions $p_X$ and $p_Y$ respectively, the {\color{red} \textbf{conditional probability mass function}} of $X$ given $Y = y$ is defined as
    \begin{equation*}
        p_{X \mid Y}(x \mid y) = \frac{p_{X, Y}(x, y)}{p_Y(y)}.
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        In particular, we have
        \begin{equation*}
            P(X = x \mid X \in A) = \begin{cases}
                \frac{P(X = x)}{P(X \in A)}, & \quad\textrm{if } x \in A \\
                0, & \quad\textrm{otherwise}
            \end{cases}.
        \end{equation*}
    \end{remark}
\end{notebox}
We can similar define for the continuous case:
\begin{dfnbox}{Conditional Probability Density Function}{condPDF}
    Let $X$ and $Y$ be jointly continuous random variables with probability density functions $f_X$ and $f_Y$ respectively, the {\color{red} \textbf{conditional probability density function}} of $X$ given $Y = y$ is defined as
    \begin{equation*}
        f_{X \mid Y}(x \mid y) = \frac{f_{X, Y}(x, y)}{f_Y(y)}.
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        In particular, we have
        \begin{equation*}
            f_{X \mid X \in A}(x) = \frac{f_X(x)}{\int_A\!f_X(x)\,\mathrm{d}x}.
        \end{equation*}
    \end{remark}
\end{notebox}
Now, we can compute the conditional probability for jointly distributed continuous random variables with
\begin{equation*}
    P(X \in A \mid Y = y) = \int_{A}\!f_{X \mid Y}(x \mid y)\,\mathrm{d}x.
\end{equation*}
In particular, we could define the \textit{conditional cumulative distribution function} as
\begin{align*}
    F_{X \mid Y}(x \mid y) & = P(X \leq x \mid Y = y) \\
    & = \int_{-\infty}^x\!f_{X \mid Y}(x \mid y)\,\mathrm{d}x
\end{align*}
What if $X$ is continuous but $Y$ is discrete? In this case, we have
\begin{align*}
    f_{X \mid Y}(x \mid y) = \frac{P(Y = y \mid X = x)}{P(Y = y)}f_X(x).
\end{align*}

\chapter{Expectation}
\section{Sums of Random Variables}
In general, we have:
\begin{thmbox}{\small Expectation of Functions of Jointly Distributed Random Variables}{expFuncJoint}
    Let $X$ and $Y$ be jointly distributed random variables. If $X$ and $Y$ are discrete, then 
    \begin{equation*}
        E[g(X, Y)] = \sum_{x}\sum_{y}g(x, y)p_{X, Y}(x, y).
    \end{equation*}
    If $X$ and $Y$ are continuous, then 
    \begin{equation*}
        E[g(X, Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\!g(x, y)f_{X, Y}(x, y)\,\mathrm{d}x\,\mathrm{d}y.
    \end{equation*}
\end{thmbox}
Based on the above, it is easy to see that if $X_1, X_2, \cdots, X_n$ are random variables,
\begin{equation*}
    E\left[\sum_{i = 1}^{n}X_i\right] = \sum_{i = 1}^{n}E[X_i].
\end{equation*}
\begin{dfnbox}{Sample Mean}{mean}
    Let $X_1, X_2, \cdots, X_n$ be independent random variables. The {\color{red} \textbf{sample mean}} is defined as
    \begin{equation*}
        \overline{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i.
    \end{equation*}
\end{dfnbox}
Clearly, if $X_i = \mu$ for all $i = 1, 2, \cdots, n$,
\begin{equation*}
    E\left[\overline{X}\right] = \frac{1}{n}\sum_{i = 1}^{n}E[X_i] = \mu.
\end{equation*}
This means that given a random sample, its sample mean is an \textit{unbiased estimate} of its expectation.

Using the expectation of the sum of random variables, we can also prove the following inequality:
\begin{thmbox}{Boole's Inequality}{boole}
    Let $A_1, A_2, \cdots, A_n$ be events, then 
    \begin{equation*}
        \sum_{i = 1}^{n}P(A_i) \geq P\left(\bigcup_{i = 1}^{n}A_i\right).
    \end{equation*}
    \tcblower       
    \begin{proof}
        Define
        \begin{equation*}
            X_i = \begin{cases}
                1, & \quad\textrm{if } A_i \textrm{ occurs} \\
                0, & \quad\textrm{otherwise}
            \end{cases}.
        \end{equation*}
        Let $X = \sum_{i = 1}^{n}X_i$ and $Y = \max\{X_1, X_2, \cdots, X_n\}$. Note that $Y = 1$ if and only if at least one of the $A_i$'s occurs, so 
        \begin{equation*}
            E[Y] = P\left(\bigcup_{i = 1}^{n}A_i\right).
        \end{equation*}
        Note that $E[X] = \sum_{i = 1}^{n}P(A_i)$ and that $X \geq Y$, so
        \begin{equation*}
            \sum_{i = 1}^{n}P(A_i) = E[X] \geq E[Y] = P\left(\bigcup_{i = 1}^{n}A_i\right).
        \end{equation*}
    \end{proof}
\end{thmbox}
So far we have been discussing the sum of finitely many random variables. It turns out that in the infinite case, the following applies:
\begin{thmbox}{Expectation of Infinite Sum of Random Variables}{expInfSum}
    Let $X_1, X_2, \cdots$ be infinitely many random variables, if 
    \begin{itemize}
        \item $X_i \geq 0$ for all $i \in \N^+$, or
        \item the series $\sum_{i = 1}^{\infty}E\left[\abs{X_i}\right]$ converges,
    \end{itemize}
    then 
    \begin{equation*}
        E\left[\sum_{i = 1}^{\infty}X_i\right] = \sum_{i = 1}^{\infty}E[X_i].
    \end{equation*}
\end{thmbox}

\section{Moments}
Consider $n$ events $A_1, A_2, \cdots, A_n$. Define
\begin{equation*}
    X_i = \begin{cases}
        1, & \quad\textrm{if } A_i \textrm{ occurs} \\
        0, & \quad\textrm{otherwise}
    \end{cases},
\end{equation*}
then $X = \sum_{i = 1}^{n}X_i$ is the number of events which have occurred. Observe that $E[X] = \sum_{i = 1}^{n}P(A_i)$. Define the event
\begin{equation*}
    E_{m, k} = \bigcap_{i = 1}^{k}A_{m_i}
\end{equation*}
where $1 \leq m_i \leq n$ for any $i = 1, 2, \cdots, k$, then the number of such $E_{m, k}$'s which have occurred is given by $\left(\begin{smallmatrix}
    X \\
    k
\end{smallmatrix}\right)$. Note that the event $\bigcap_{i = 1}^{k}A_{m_i}$ occurs if and only if $\prod_{i = 1}^{k}X_{m_i} = 1$, so
\begin{equation*}
    \begin{pmatrix}
        X \\
        k
    \end{pmatrix} = \sum_{m_1 < m_2 < \cdots < m_k}\left(\prod_{i = 1}^{k}X_{m_i}\right).
\end{equation*}
Therefore, 
\begin{align*}
    E\left[\begin{pmatrix}
        X \\
        k
    \end{pmatrix}\right] & = E\left[\sum_{m_1 < m_2 < \cdots < m_k}\left(\prod_{i = 1}^{k}X_{m_i}\right)\right] \\
    & = \sum_{m_1 < m_2 < \cdots < m_k}P\left(E_{m, k}\right).
\end{align*}
Notice that $E\left[\left(\begin{smallmatrix}
    X \\
    k
\end{smallmatrix}\right)\right]$ is a linear combination of the first $k$-th momenets of $X$. We would like to find a way to quickly compute the $n$-th moment of a random variable.

Let $X$ be a discrete random variable, then 
\begin{equation*}
    E\left[X^n\right] = \sum_{x}x^np_X(x).
\end{equation*}
Note that by Maclaurin Series, we have
\begin{equation*}
    g(t) = \sum_{n = 0}^{\infty}\frac{g^{(n)(0)}}{n!}t^n
\end{equation*}
for any function $g$. A motivation is to construct $g$ such that $g^{(n)}(0) = E\left[X^n\right]$. Therefore,
\begin{align*}
    g(t) & = \sum_{n = 0}^{\infty}\frac{E\left[X^n\right]}{n!}t^n \\
    & = \sum_{n = 0}^{\infty}\frac{\sum_{x}x^np_X(x)}{n!}t^n \\
    & = \sum_{x}\left(p_X(x)\sum_{n = 0}^{\infty}\frac{(tx)^n}{n!}\right) \\
    & = \sum_{x}\e^{tx}p_X(x) \\
    & = E\left[\e^{tX}\right].
\end{align*}
\begin{dfnbox}{Moment Generating Function}{momentGenFunc}
    Let $X$ be a random variable, the {\color{red} \textbf{moment generating function}} of $X$ is defined as 
    \begin{equation*}
        M_X(t) = E\left[\e^{tX}\right]
    \end{equation*}
    such that 
    \begin{equation*}
        E\left[X^n\right] = M_X^{(n)}(0) \quad\textrm{for all } n \in \N.
    \end{equation*}
\end{dfnbox}
It can be proven that for any random variable $X$, $M_X$ is unique. In other words, if two random variables have the same moment generating function, they must be the same random variable.
\begin{thmbox}{Uniqueness of Moment Generating Function}{uniqueMomentGenFunc}
    Let $X$ and $Y$ be random variables. If
    \begin{equation*}
        \lim_{t \to 0}M_X(t) = \lim_{t \to 0}M_Y(t),
    \end{equation*}
    then $X$ and $Y$ have the same distribution.
\end{thmbox}
By properties of exponential functions, the following theorem can also be easily proven:
\begin{probox}{\small Moment Generating Function of Sum of Independent Random Variables}{momentSumIndRV}
    Let $X_1, X_2, \cdots, X_n$ be independent random variables, then 
    \begin{equation*}
        M_{\sum_{i = 1}^{n}X_i}(t) = \prod_{i = 1}^{n}M_{X_i}(t).
    \end{equation*}
\end{probox}
However, note that the converse of Proposition \ref{pro:momentSumIndRV} is not true in general.
\begin{dfnbox}{Joint Moment Generating Function}{jointMomentGenFunc}
    Let $X$ and $Y$ be random variables. The {\color{red} \textbf{joint moment generating function}} of $X$ and $Y$ is defined as
    \begin{equation*}
        M_{X, Y}(s, t) = E\left[\e^{sX + tY}\right].
    \end{equation*}
\end{dfnbox}
Observe that in the joint case, $M_X(s) = M_{X, Y}(s, 0)$ and $M_Y(t) = M_{X, Y}(0, t)$. $X$ and $Y$ are independent if and only if $M_{X, Y}(s, t) = M_X(s)M_Y(t)$.
\section{Covariance and Correlations}
\subsection{Covariance}
Suppose that $X$ and $Y$ are independent random variables. We have seen that
\begin{equation*}
    E[XY] = E[X]E[Y].
\end{equation*}
This motivates us to consider the quantity $E[XY] - E[X]E[Y]$.
\begin{dfnbox}{Covariance}{covar}
    Let $X$ and $Y$ be random variables. The {\color{red} \textbf{covariance}} of $X$ and $Y$ is defined as
    \begin{equation*}
        \mathrm{Cov}(X, Y) = E[XY] - E[X]E[Y].
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        We have seen that if $X$ and $Y$ are independent, then $\mathrm{Cov}(X, Y) = 0$. But the converse is not true in general!
    \end{remark}
\end{notebox}
Note that covariance has the following properties:
\begin{enumerate}
    \item $\mathrm{Cov}(X, X) = \mathrm{Var}(X)$.
    \item $\mathrm{Cov}(X, Y) = \mathrm{Cov}(Y, X)$.
    \item $\mathrm{Cov}(aX, Y) = a\mathrm{Cov}(X, Y)$.
    \item $\mathrm{Cov}(X_1 + X_2, Y) = \mathrm{Cov}(X_1, Y) + \mathrm{Cov}(X_2, Y)$.
\end{enumerate}
\begin{notebox}
    \begin{remark}
        Let $V$ be the set of all random variables, then $\mathrm{Cov}(\cdot, \cdot) \colon V \times V \to \R$ is an inner product over $V$.
    \end{remark}
\end{notebox}
Let $X$ and $Y$ be random variables, then by the above properties, we have
\begin{align*}
    \mathrm{Var}(X + Y) & = \mathrm{Cov}(X + Y, X + Y) \\
    & = \mathrm{Cov}(X, X + Y) + \mathrm{Cov}(Y, X + Y) \\
    & = \mathrm{Cov}(X + Y, X) + \mathrm{Cov}(X + Y, Y) \\
    & = \mathrm{Cov}(X, X) + \mathrm{Cov}(Y, X) + \mathrm{Cov}(X, Y) + \mathrm{Cov}(Y, Y) \\
    & = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2\mathrm{Cov}(X, Y).
\end{align*}
In general, we have the following formula:
\begin{probox}{}{}
    Let $X_1, X_2, \cdots, X_n$ be random variables, then
    \begin{equation*}
        \mathrm{Var}\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\mathrm{Var}(X_i) + 2\sum_{i < j}\mathrm{Cov}(X_i, X_j).
    \end{equation*}
    In particular, if all of the $X_i$'s are independent, then
    \begin{equation*}
        \mathrm{Var}\left(\sum_{i = 1}^{n}X_i\right) = \sum_{i = 1}^{n}\mathrm{Var}(X_i).
    \end{equation*}
\end{probox}
\begin{dfnbox}{Deviation}{deviation}
    Let $X_1, X_2, \cdots, X_n$ be independent random variables with mean $\mu$ and variance $\sigma^2$. The {\color{red} \textbf{deviation}} is defined as
    \begin{equation*}
        X_i - \overline{X} = X_i - \frac{1}{n}\sum_{i = 1}^{n}X_i.
    \end{equation*}
\end{dfnbox}
Similar to the sample mean, we would wish the deviation gives an unbiased estimate for the variance. However, 
\begin{equation*}
    E\left[X_i - \overline{X}\right] = \frac{n - 1}{n}\sigma^2.
\end{equation*}
Therefore, we need to eliminate the bias from the deviation.
\begin{dfnbox}{Sample Variance}{sampleVar}
    Let $X_1, X_2, \cdots, X_n$ be independent random variables with mean $\mu$ and variance $\sigma^2$. The {\color{red} \textbf{sample variance}} is defined as
    \begin{equation*}
        S^2 = \sum_{i = 1}^{n}\frac{X_i - \overline{X}}{n - 1}.
    \end{equation*}
\end{dfnbox}
\subsection{Correlation}
Intuitively, we view covariance $\mathrm{Cov}(X, Y)$ as a measure of the degree of spread of the joint distribution of $X$ and $Y$. Naturally, we can make use of it to measure how related are $X$ and~$Y$.
\begin{dfnbox}{Correlation}{correlation}
    Let $X$ and $Y$ be random variables with positive variances. The {\color{red} \textbf{correlation}} of $X$ and $Y$ is defined as
    \begin{equation*}
        \rho(X, Y) = \frac{\mathrm{Cov}(X, Y)}{\sqrt{\mathrm{Var}(X)\mathrm{Var}(Y)}}.
    \end{equation*}
    In particular, $X$ and $Y$ are said to be {\color{red} \textbf{uncorrelated}} if $\rho(X, Y) = 0$.
\end{dfnbox}
It is easy to see that for $c > 0$, $\rho(\pm cX, Y) = \pm\rho(X, Y)$. Suppose $X$ and $Y$ have variances $\sigma_X^2$ and $\sigma_Y^2$ respectively, we have
\begin{equation*}
    \rho(X, Y) = \rho\left(\frac{X}{\sigma_X}, \frac{Y}{\sigma_Y}\right) = \mathrm{Cov}\left(\frac{X}{\sigma_X}, \frac{Y}{\sigma_Y}\right).
\end{equation*}
The above property leads to the following result:
\begin{thmbox}{Boundedness of Correlation}{boundedCorrelation}
    Let $X$ and $Y$ be random variables, then 
    \begin{equation*}
        -1 \leq \rho(X, Y) \leq 1.
    \end{equation*}
\end{thmbox}
Note that $\mathrm{Var}(X) = 0$ if and only if $X$ is a constant, i.e., $P(X = c) = 1$ and $P(X = x) = 0$ for all $x \neq c$. Therefore, $\rho(X, Y) = \pm 1$ if and only if $Y = \pm aX + b$ for some constants $a$, $b$ with $a > 0$.
\section{Conditional Expectation and Variance}
\subsection{Conditional Expectation}
Recall that in Definitions \ref{dfn:condPMF} and \ref{dfn:condPDF}, we have defined conditional distribution of a random variable $X$ given $Y = y$. We will study the notion of \textit{conditional expectation} now.
\begin{dfnbox}{Conditional Expectation}{condExp}
    Let $X$ and $Y$ be random variables. The {\color{red} \textbf{conditional expectation}} of $X$ given $Y = y$ is defined as
    \begin{equation*}
        E\left[X \mid Y = y\right] = \sum_{x}xp_{X \mid Y}(x \mid y)
    \end{equation*}
    if $X$ and $Y$ are discrete, and
    \begin{equation*}
        E\left[X \mid Y = y\right] = \int_{-\infty}^{\infty}\!\frac{f_{X, Y}(x, y)}{f_Y(y)}\,\mathrm{d}x
    \end{equation*}
    if $X$ and $Y$ are jointly continuous.
\end{dfnbox}
All properties of expectation are still satisfied by conditional expectation. In particular, notice that $E\left[X \mid Y\right]$ is a function in $Y$, so it is a random variable by itself. Let $Z = E[X \mid Y]$, what is the expectation of $Z$?
\begin{probox}{}{}
    Let $X$ and $Y$ are random variables, then
    \begin{equation*}
        E\bigl[E[X \mid Y]\bigr] = E[X].
    \end{equation*}
\end{probox}
\subsection{Conditional Variance}
Similarly, we can define \textit{conditional variance}.
\begin{dfnbox}{Conditional Variance}{condVar}
    Let $X$ and $Y$ be random variables. The {\color{red} \textbf{condtional variance}} of $X$ given $Y$ is defined as
    \begin{equation*}
        \mathrm{Var}(X \mid Y) = E\bigl[(X - E[X \mid Y])^2 \mid Y\bigr].
    \end{equation*}
\end{dfnbox}
Notice that $\mathrm{Var}(X \mid Y)$ is also a random variable as it is a function of $Y$. Since $\mathrm{Var}(X \mid Y) = E\left[X^2 \mid Y\right] - \bigl(E[X \mid Y]\bigr)^2$, we have
\begin{align*}
    E\left[\mathrm{Var}(X \mid Y)\right] & = E\Bigl[E\left[X^2 \mid Y\right]\Bigr] - E\left[\bigl(E[X \mid Y]\bigr)^2\right] \\
    & = E\left[X^2\right] - E\left[\bigl(E[X \mid Y]\bigr)^2\right], \\
    \mathrm{Var}\bigl(\mathrm{Var}(X \mid Y)\bigr) & = E\left[(E[X \mid Y])^2\right] - \left(E\bigl[E[X \mid Y]\bigr]\right)^2.
\end{align*}
With some manipulations we can prove the following proposition:
\begin{probox}{}{}
    Let $X$ and $Y$ be random variables, then
    \begin{equation*}
        \mathrm{Var}(X) = E[\mathrm{Var}(X \mid Y)] + \mathrm{Var}(E[X \mid Y]).
    \end{equation*}
\end{probox}
\subsection{Prediction}
In many situations, we seek to \textbf{predict} the outcome of an event. Specifically, given $X = x$, we wish to find a function $g \colon \mathrm{Range}(X) \to \mathrm{Range}(Y)$ such that $g(x) = \hat{y}$ is the prediction for $Y$. In an ideal case, we would want $g(X)$ to be the \textbf{closest} to $Y$. In other words, we would minimise $g(X) - Y$. To eliminate the inconvenience of a negative difference, we would choose a $g$ such that $E\left[\bigl(Y - g(X)\bigr)^2\right]$ is minimised.

Let us consider a simpler case. When $g(x) = c$, to minimise $E\left[(Y - c)^2\right]$, consider
\begin{align*}
    E\left[(Y - c)^2\right] & = c^2 - 2cE[Y] + E\left[Y^2\right] \\
    & = \left(c - E[Y]\right)^2 + \mathrm{Var}(Y).
\end{align*}
Therefore, we need $c = E[Y]$. Since $c = g(x)$, this implies that $g(x) = E[Y \mid X = x]$. This is summarised into the following theorem:
\begin{thmbox}{Best Predictor}{bestPredict}
    Let $X$ and $Y$ be random variables. Given $X$, the best predictor of $Y$ is the function 
    \begin{equation*}
        g(X) = E[Y \mid X].
    \end{equation*}
\end{thmbox}
Essentially, this implies that for all function $f$ of $X$, $E\left[\bigl(Y - f(X)\bigr)^2\right] \geq E\left[\bigl(Y - E[Y \mid X]\bigr)^2\right]$.

However, in some cases we do not know the exact joint distribution of $X$ and $Y$ and so we cannot find $E[Y \mid X]$. Therefore, we may attempt to predict $Y$ with a linear function of $X$ which can be easily formulated. Hence, we will find constants $a$ and $b$ such that $E\left[(Y - a - bX)^2\right]$ is minimised.

Let $E[X] = \mu_X$, $E[Y] = \mu_Y$, $\mathrm{Var}(X) = \sigma_X^2$, $\mathrm{Var}(Y) = \sigma_Y^2$. Define
\begin{equation*}
    X' = \frac{X - \mu_X}{\sigma_X}, \qquad Y' = \frac{Y - \mu_Y}{\sigma_Y}.
\end{equation*}
Then, $E\left[X'\right] = E\left[Y'\right] = 0$ and $\mathrm{Var}(X') = \mathrm{Var}(Y') = 1$. Note that
\begin{equation*}
    Y - a - bX = \sigma_Y\left(Y' - \frac{a + b\mu_X - \mu_Y}{\sigma_Y} - \frac{b\sigma_X}{\sigma_Y}X'\right).    
\end{equation*}
Therefore, setting $\frac{a + b\mu_X - \mu_Y}{\sigma_Y} = a'$ and $\frac{b\sigma_X}{\sigma_Y} = b'$, we have 
\begin{align*}
    E\left[(Y - a - bX)^2\right] & = \sigma_Y^2E\left[\left(Y' - a' - b'X'\right)^2\right] \\
    & = \sigma_Y^2E\left[Y'^2 + a'^2 + b'^2X'^2 - 2a'Y' + 2a'b'X' - 2b'X'Y'\right] \\
    & = \sigma_Y^2\left(1 + a'^2 + b'^2 - 2b'\rho(X, Y)\right) \\
    & = \sigma_Y^2\left(a'^2 + \bigl(b' - \rho(X, Y)\bigr)^2 + 1 - \rho(X, Y)^2\right).
\end{align*}
Therefore, we need $a' = 0$ and $b' = \rho(X, Y)$, i.e.,
\begin{align*}
    a = \mu_Y - b\mu_X, \qquad b = \rho(X, Y)\frac{\sigma_Y}{\sigma_X}.
\end{align*}
Therefore, 
\begin{align*}
    a + bX & = \mu_Y - b\mu_X + bX \\
    & = \mu_Y + b(X - \mu_X) \\
    & = \mu_Y + \rho(X, Y)\frac{\sigma_Y}{\sigma_X}(X - \mu_X)
\end{align*}
and the minimum of $E\left[(Y - a - bX)^2\right]$ is $\sigma_Y^2\left(1 - \rho(X, Y)^2\right)$.
\begin{thmbox}{Best Linear Predictor}{bestLinPredict}
    Let $X$ and $Y$ be random variables with means $\mu_X$ and $\mu_Y$, and variances $\sigma_X^2$ and $\sigma_Y^2$ respectively. Given $X$, the best linear predictor of $Y$ is
    \begin{equation*}
        g(X) = E\left[\left(Y - \mu_Y - \rho(X, Y)\frac{\sigma_Y}{\sigma_X}(X - \mu_X)\right)\right].
    \end{equation*}
\end{thmbox}
\end{document}