\documentclass[math]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{yhmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}
\usepackage{diagbox}
\DeclareSymbolFont{yhlargesymbols}{OMX}{yhex}{m}{n} \DeclareMathAccent{\yhwidehat}{\mathord}{yhlargesymbols}{"62}

\usepackage{scalerel}[2014/03/10]
\usepackage{stackengine}

\renewcommand\widetilde[1]{\ThisStyle{%
  \setbox0=\hbox{$\SavedStyle#1$}%
  \stackengine{1pt-\LMpt}{$\SavedStyle#1$}{%
    \stretchto{\scaleto{\SavedStyle\mkern.2mu\sim}{.5467\wd0}}{.5\ht0}%
%    .2mu is the kern imbalance when clipping white space
%    .5467++++ is \ht/[kerned \wd] aspect ratio for \sim glyph
  }{O}{c}{F}{T}{S}%
}}
\makeatletter
\let\save@mathaccent\mathaccent
\newcommand*\if@single[3]{%
  \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
  \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
  \ifdim\ht0=\ht2 #3\else #2\fi
  }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
  \begingroup
  \def\mathaccent##1##2{%
%Enable nesting of accents:
    \let\mathaccent\save@mathaccent
%If there's more than a single symbol, use the first character instead \left(see below\right):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = \left(italic correction / 3\right) - \left(Breite / 10\right)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
      \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
      \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern \left(-\dimen@\right) if it is >0 or if a superscript follows:
      \let\final@kern#2%
      \ifdim\dimen@<\z@ \let\final@kern1\fi
      \if\final@kern1 \kern-\dimen@\fi
    \else
      \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
  \if#31
    \macc@nested@a\relax111{#1}%
  \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
      \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
  \fi
  \endgroup
}
\makeatother

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\I}{\mathbfit{I}}
\newcommand{\e}{\mathrm{e}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\im}{\mathrm{i}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\begin{document}
\fancyhead[L]{
    Information and Coding Theory
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Probability}
\section{Probability Spaces}
In an elementary level, we have been viewing probability as the quotient between the number of desired outcomes and the number of all possible outcomes. This definition, though intuitive, is not very solid when it comes to an infinite sample space. In this introductory chapter, we would establish the theories of probability using a more modern and rigorous structure.
\begin{dfnbox}{Set Algebra}{setAlgebra}
    Let $X$ be a set. A {\color{red} \textbf{set algebra}} over $X$ is a family $\mathcal{F} \subseteq \mathcal{P}\left(X\right)$ such that 
    \begin{itemize}
        \item $X \backslash F \in \mathcal{F}$ for all $F \in \mathcal{F}$ (closed under complementation);
        \item $X \in \mathcal{F}$;
        \item $X_1 \cup X_2 \in \mathcal{F}$ for any $X_1, X_2 \in \mathcal{F}$ (closed under binary union).
    \end{itemize}
\end{dfnbox}
There are several immediate implications from the above definition. 

First, by closure under complementation, we know that an algebra over any set $X$ must contain the empty set. 

Second, by De Morgan's Law, one can easily check that if the first $2$ axioms hold, the closure under binary union is equivalent to 
\begin{itemize}
    \item $X_1 \cap X_2 \in \mathcal{F}$ for any $X_1, X_2 \in \mathcal{F}$;
    \item $\bigcup_{i = 1}^{n}X_i \in \mathcal{F}$ for any $X_1, X_2, \cdots, X_n \in \mathcal{F}$ for all $n \in \N$;
    \item $\bigcap_{i = 1}^{n}X_i \in \mathcal{F}$ for any $X_1, X_2, \cdots, X_n \in \mathcal{F}$ for all $n \in \N$.
\end{itemize}
$\left(X, \mathcal{F}\right)$ is known as a \textit{field of sets}, where the elements of $X$ are called \textit{points} and those of $\mathcal{F}$, \textit{complexes} or \textit{admissible sets} of $X$.

In probability theory, what we are interested in is a special type of set algebras known as $\sigma$-\textit{algebras}.
\begin{dfnbox}{$\sigma$-Algebra}{sigmaAlgebra}
    A {\color{red} \textbf{$\sigma$-Algebra}} over a set $A$ is a non-empty set algebra over $A$ that is closed under countable union.
\end{dfnbox}
Of course, by the same argument as above, we known that any $\sigma$-algebra is closed under countable intersection as well.

Now, as we all know, we can take some set $\Omega$ as a \textit{sample space} and denote an \textit{event} by some subset of $\Omega$. Roughly speaking, we could now define the probability of an event $E \subseteq \Omega$ as the ratio between the sets' volumes. The remaining question now is: how do we define the volume of a set properly?
\begin{dfnbox}{Measure}{measure}
    Let $X$ be a set and $\Sigma$ be a $\sigma$-algebra over $X$. A {\color{red} \textbf{measure}} over $\Sigma$ is a function 
    \begin{equation*}
        \mu \colon \Sigma \to \R \cup \{-\infty, +\infty\}
    \end{equation*}
    such that 
    \begin{itemize}
        \item $\mu\left(E\right) \geq 0$ for all $E \in \Sigma$ (non-negativity);
        \item $\mu\left(\varnothing\right) = 0$;
        \item $\mu\left(\bigcup_{i = 1}^{\infty}E_i\right) = \sum_{i = 1}^{\infty}\mu\left(E_i\right)$ for any countable collection of pairwise disjoint elements of $\Sigma$ (countable additivity or $\sigma$-additivity).
    \end{itemize}
    The triple $\left(X, \Sigma, \mu\right)$ is known as a {\color{red} \textbf{measure space}} and the pair $\left(X, \Sigma\right)$, a {\color{red} \textbf{measurable space}}.
\end{dfnbox}
One thing to note here is that if at least one $E \in \Sigma$ has a finite measure, then $\mu\left(\varnothing\right) = 0$ is automatically guaranteed for obvious reasons.
\begin{dfnbox}{Probability Space}{probSpace}
    Let $\Omega$ be a sample space and $\mathcal{F}$ be a $\sigma$-algebra over $\Omega$. A {\color{red} \textbf{probability space}} is a measure space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ where $\mathbb{P} \colon \mathcal{F} \to [0, 1]$, known as a {\color{red} \textbf{probability measure}}, is such that $\mathbb{P}\left(\Omega\right) = 1$.
\end{dfnbox}
Obviously, the above definition immediately guarantees that 
\begin{enumerate}
    \item $\mathbb{P}\left(A^c\right) = 1 - \mathbb{P}\left(A\right)$;
    \item $\mathbb{P}\left(A\right) \leq \mathbb{P}\left(B\right)$ if $\mathbb{P}\left(A\right) \subseteq \mathbb{P}\left(A\right)$;
    \item $\mathbb{P}\left(A \cup B\right) \leq \mathbb{P}\left(A\right) + \mathbb{P}\left(B\right)$.
\end{enumerate}
The third result follows from a direct application of the principle of inclusion and exclusion. By induction, one can easily check that 
\begin{equation*}
    \mathbb{P}\left(\bigcup_{i = 1}^{n}E_i\right) \leq \sum_{i = 1}^{n}\mathbb{P}\left(E_i\right)
\end{equation*}
for any finitely many events. The following proposition extends this result to countable collections of events:
\begin{probox}{Union Bound of Countable Collections of Events}{unionBound}
    Let $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ be a probability space and $E_1, E_2, \cdots, E_n, \cdots \in \mathcal{F}$ is any countable sequence of events, then 
    \begin{equation*}
        \mathbb{P}\left(\bigcup_{i = 1}^{\infty}E_i\right) \leq \sum_{i = 1}^{\infty}\mathbb{P}\left(E_i\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        Define $F_1 \coloneqq E_1$ and $F_k \coloneqq E_k \backslash \bigcup_{i = 1}^{k - 1}E_i$ for $k \geq 2$. Clearly, the $F_i$'s are pairwise disjoint. By Definition \ref{dfn:sigmaAlgebra}, the $F_i$'s are elements of $\mathcal{F}$. Note that $\mathbb{P}\left(F_i\right) \leq \mathbb{E_i}$ for all $i \in \N^+$, so 
        \begin{align*}
            \mathbb{P}\left(\bigcup_{i = 1}^{\infty}E_i\right) & = \mathbb{P}\left(\bigcup_{i = 1}^{\infty}F_i\right) \\
            & = \sum_{i = 1}^{\infty}\mathbb{P}\left(F_i\right) \\
            & \leq \sum_{i = 1}^{\infty}\mathbb{P}\left(E_i\right).
        \end{align*}
    \end{proof}
\end{probox}
Next, we will introduce the notion of \textit{random variables} formally. For this purpose, we first establish the notion of a \textit{Borel algebra}.
\begin{dfnbox}{Borel Algebra}{borelAlgebra}
    Let $X$ be a topological space. A {\color{red} \textbf{Borel set}} on $X$ is a set which can be formed via countable union, countable intersection and relative complementation of open sets in $X$. The smallest $\sigma$-algebra over $X$ containing all Borel sets on $X$ is known as the {\color{red} \textbf{Borel algebra}} over $X$.
\end{dfnbox}
Clearly, the Borel algebra over $X$ contains all open sets in $X$ according to the above axioms from Definition \ref{dfn:sigmaAlgebra}. This helps us define the following:
\begin{dfnbox}{Random Variable}{RV}
    Let $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ be a probability space and $\left(\mathcal{X}, \mathcal{B}\right)$ be a measurable space where $\mathcal{B}$ is the Borel algebra over $\mathcal{X}$. A {\color{red} \textbf{random variable}} is a function $X \colon \Omega \to \mathcal{X}$ such that 
    \begin{equation*}
        \left\{\omega \in \Omega \colon X\left(\omega\right) \in B\right\} \in \mathcal{F} 
    \end{equation*}
    for all $B \in \mathcal{B}$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Rigorously, such a random variable $X$ is a \textit{measurable function} or \textit{measurable mapping} from $\left(\Omega, \mathcal{F}\right)$ to $\left(\mathcal{X}, \mathcal{B}\right)$.
    \end{remark}
\end{notebox}
The probability measure $\mathbb{P}$ thus induces a probability measure $P_X$ over $\left(\mathcal{X}, \mathcal{B}\right)$.
\begin{dfnbox}{Distribution}{distribution}
    Let $X \colon \Omega \to \mathcal{X}$ be a random variable over the probability space $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ and $\mathcal{B}$ be the Borel algebra over $\mathcal{X}$, the {\color{red} \textbf{distribution}} of $X$ is the probability measure $P_X$ on $\left(\mathcal{X}, \mathcal{B}\right)$ given by 
    \begin{equation*}
        P_X\left(B\right) = \mathbb{P}\left(\left\{\omega \in \Omega \colon X\left(\omega\right) \in B\right\}\right).
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Often times, we write $\mathrm{Pr}\left(X \in B\right) = P_X\left(B\right)$.
    \end{remark}
\end{notebox}
In the context of information theory, we mostly are concerned with real-valued random variables only.
\begin{dfnbox}{Real-Valued Random Variable}{RRV}
    Let $\left(\Omega, \mathcal{F}, \mathbb{P}\right)$ be a probability space, a {\color{red} \textbf{real-valued random variable}} over the space is a mapping $X \colon \Omega \to \R$ such that 
    \begin{equation*}
        \left\{\omega \in \Omega \colon X\left(\omega\right) \leq x\right\} \in \mathcal{F}
    \end{equation*}
    for all $x \in \R$.
\end{dfnbox}
Note that the Borel set over $\R$ is just the family of all open intervals. 

Clearly, if $X$ is a real-valued random variable, we have $\left\{\omega \in \Omega \colon X\left(\omega\right) > x\right\} \in \mathcal{F}$. Moreover, we claim that 
\begin{equation*}
    \left\{\omega \in \Omega \colon X\left(\omega\right) < x\right\} = \bigcup_{y < x}\left\{\omega \in \Omega \colon X\left(\omega\right) \leq y\right\}.
\end{equation*}
The proof is quite straightforward and is left to the reader as an exercise. By Definition \ref{dfn:sigmaAlgebra}, this means that 
\begin{equation*}
    \left\{\omega \in \Omega \colon X\left(\omega\right) < x\right\} \cup \left\{\omega \in \Omega \colon X\left(\omega\right) > x\right\} \in \mathcal{F}.
\end{equation*}
Therefore, $\left\{\omega \in \Omega \colon X\left(\omega\right) = x\right\} \in \mathcal{F}$. This argument justifies the probabilities $\mathrm{Pr}\left(X < x\right)$ and $\mathrm{Pr}\left(X = x\right)$. We give a special name to the range of a random variable in computer science.
\begin{dfnbox}{Alphabet}{alphabet}
    Let $X$ be a random variable, the range of $X$ is called an {\color{red} \textbf{alphabet}}, denoted as $\mathcal{X}$.
\end{dfnbox}
Recall that we have defined expectations for discrete and continuous random variables in elementary probability theory. In terms of measure theory, the two formulae can be unified as the Lebesgue integral
\begin{equation*}
    \mathbb{E}[X] = \int_{\Omega}\!X\left(\omega\right)\,\d\mathbb{P}\left(\omega\right).
\end{equation*}
Note that $\mathbb{E}[X]$ is a real number while $\mathbb{E}[X \mid Y]$ is a \textbf{random variable} formed as a function of $Y$. In a way, $Y$ partitions the sample space into regions where $\mathbb{E}[X \mid Y = y_i]$ gives the expectation of $X$ in the region induced by $Y = y_i$ for each $y_i \in \mathcal{Y}$. In general, the following result holds:
\begin{thmbox}{Law of Iterated Expectations}{iterExpectations}
    Let $X$ and $Y$ be random variables, then $\mathbb{E}\bigl[\mathbb{E}[X \mid Y]\bigr] = \mathbb{E}[X]$.
\end{thmbox}
The above formula can be interpreted as the fact that $\mathbb{E}[X \mid Y]$ is a best estimator for $X$.
\section{Markov Chains}
Recall that $2$ random variables $X$ and $Z$ are \textit{independent} if and only if $P_{X, Z}\left(x, z\right) = P_X\left(x\right)P_Z\left(z\right)$ or $P_{X \mid Z}\left(x \mid z\right) = P_X\left(x\right)$ for all $\left(x, z\right) \in \mathcal{X} \times \mathcal{Z}$. We will extend this definition with a third random variable.
\begin{dfnbox}{Conditional Independence}{condInd}
    Let $X, Y, Z$ be random variables. If 
    \begin{equation*}
        P_{X, Y, Z}\left(x, y, z\right) = P_X\left(x\right)P_{Y \mid X}\left(y \mid x\right)P_{Z \mid Y}\left(z \mid y\right)
    \end{equation*}
    for all $\left(x, y, z\right) \in \mathcal{X} \times \mathcal{Y} \times \mathcal{Z}$, then we say that $X, Y, Z$ forms a {\color{red} \textbf{Markov chain}} in this order, or that $X$ and $Z$ are {\color{red} \textbf{conditionally independent}} on $Y$.
\end{dfnbox}
Recall also that the \textit{Bayes's Rule} states the following:
\begin{thmbox}{Bayes's Rule}{BayesRule}
    For any random variables $X$ and $Y$, 
    \begin{equation*}
        P_{X \mid Y}\left(x \mid y\right) = \frac{P_{Y \mid X}\left(y \mid x\right)P_X\left(x\right)}{\sum_{x' \in \mathcal{X}}P_{Y \mid X}\left(y \mid x'\right)P_X\left(x'\right)}.
    \end{equation*}
\end{thmbox}
Based on Theorem \ref{thm:BayesRule}, we have 
\begin{equation*}
    P_{X, Y}\left(x, y\right) = P_{X \mid Y}\left(x \mid y\right)P_Y\left(y\right) = P_X\left(x\right)P_{Y \mid X}\left(y \mid x\right).
\end{equation*}
By applying the formula repeatedly, we have 
\begin{align*}
    P_{X, Y, Z}\left(x, y, z\right) & = P_{X, Y}\left(x, y\right)P_{Z \mid X, Y}\left(z \mid x, y\right) \\
    & = P_X\left(x\right)P_{Y \mid X}\left(y \mid x\right)P_{Z \mid X, Y}\left(z \mid x, y\right).
\end{align*}
Therefore, a Markov chain simply states that the distribution of $Z$ is no longer dependent on $X$, but depends on $Y$ solely. Therefore, this allows us to remove one condition when applying Theorem \ref{thm:BayesRule}. Thus, it actually suffices to prove $P_{Z \mid X, Y} = P_{Z \mid Y}$ when proving that $X$-$Y$-$Z$ forms a Markov chain.  
 
We can denote a Markov chain by $X$-$Y$-$Z$. Intuitively, such a relationship should be symmetric.
\begin{probox}{Symmetricity of Markov Chains}{symmetricMarkovChains}
    If $X$-$Y$-$Z$ is a Markov chain, then $Z$-$Y$-$X$ is also a Markov chain.
    \tcblower
    \begin{proof}
        By Definition \ref{dfn:condInd}, 
        \begin{align*}
            P_{X, Y, Z}\left(x, y, z\right) & = P_X\left(x\right)P_{Y \mid X}\left(y \mid x\right)P_{Z \mid Y}\left(z \mid y\right).
        \end{align*}
        By Theorem \ref{thm:BayesRule}, we have 
        \begin{align*}
            P_{X \mid Y}\left(x \mid y\right) & = \frac{P_X\left(x\right)P_{Y \mid X}\left(y \mid x\right)}{P_Y\left(y\right)} \\
            & = \frac{P_{X, Y, Z}\left(x, y, z\right)}{P_Y\left(y\right)P_{Z \mid Y}\left(z \mid y\right)} \\
            & = \frac{P_{X, Y, Z}\left(x, y, z\right)}{P_{Z, Y}\left(z, y\right)} \\
            & = P_{X \mid Z, Y}\left(x \mid z, y\right).
        \end{align*}
        Therefore, $Z$-$Y$-$X$ is a Markov chain.
    \end{proof}
\end{probox}
One obvious case where dependence exists between the random variables in a Markov chain is that one of the random variables is a function of another one.
\begin{probox}{Markov Chain Involving Functions of a Random Variable}{funcMarkovChain}
    Let $X$ and $Y$ be any random variables and $Z \coloneqq f\left(Y\right)$ for some function $f$, then $X$-$Y$-$Z$ is a Markov chain.
    \tcblower
    \begin{proof}
        Notice that 
        \begin{align*}
            P_{Z \mid X, Y}\left(z \mid x, y\right) & = P_{f\left(Y\right) \mid X, Y}\left(z \mid x, y\right) = \begin{cases}
                1 &\quad \textrm{if } z = f\left(y\right) \\
                0 &\quad \textrm{otherwise} 
            \end{cases}, \\
            P_{Z \mid Y}\left(z \mid y\right) & = P_{f\left(Y\right) \mid Y}\left(z \mid y\right) = \begin{cases}
                1 &\quad \textrm{if } z = f\left(y\right) \\
                0 &\quad \textrm{otherwise} 
            \end{cases}
        \end{align*}
        for all $\left(x, y, z\right) \in \mathcal{X} \times \mathcal{Y} \times \mathcal{Z}$. Therefore, $P_{Z \mid X, Y} = P_{Z \mid Y}$ and so $X$-$Y$-$Z$ forms a Markov chain.
    \end{proof}
\end{probox}
Note that if $X$ and $Z$ are independent, they are naturally conditionally independent given any $Y$. However, the inverse may not be true.
\begin{probox}{Conditional Independence Does Not Imply Independence}{condIndNotInd}
    There exists random variables $X, Y, Z$ such that $X$ and $Z$ are dependent but conditionally independent given $Y$.
    \tcblower
    \begin{proof}
        Let $N_1, N_2, N_3$ be pairwise independent random variables such that 
        \begin{equation*}
            \mathcal{N}_1 = \mathcal{N}_2 = \mathcal{N}_3 = \{0, 1\}.
        \end{equation*}
        Take $X = N_1 + N_2$, $Y = N_2$ and $Z = N_2 + N_3$. Clearly, $X$ and $Z$ are dependent, but 
        \begin{align*}
            P_{Z \mid X}\left(z \mid x\right) & = P_{N_2 + N_3 \mid N_1 + N_2}\left(z \mid x\right) \\
            & = P_{N_3 \mid N_1, N_2}\left(z - y \mid x - y, y\right) \\
            & = P_{N_2 + N_3 \mid N_1 + N_2, N_2}\left(z \mid x, y\right) \\
            & = P_{Z \mid X, Y}\left(z \mid x, y\right),
        \end{align*}
        which implies that $X$ and $Z$ are conditionally independent given $Y$.
    \end{proof}
\end{probox}
\section{Probability Bounds}
We use various bounds to make estimates and approximations for probability distributions. The first commonly used bound is \textit{Markov's Inequality}.
\begin{thmbox}{Markov's Inequality}{MarkovIneq}
    If $X$ is a non-negative random variable, then $\mathrm{Pr}\left(X \geq a\right) \leq \frac{\mathbb{E}[X]}{a}$ for all $a > 0$.
    \tcblower
    \begin{proof}
        It suffices to prove for the continuous case. Notice that 
        \begin{align*}
            \mathbb{E}[X] & = \int_{0}^{\infty}\!xf_X\left(x\right)\,\d x \\
            & \geq \int_{a}^{\infty}\!xf_X\left(x\right)\,\d x \\
            & \geq a\int_{0}^{\infty}\!f_X\left(x\right)\,\d x \\
            & = \mathrm{Pr}\left(X \geq a\right).
        \end{align*}
        Therefore, $\mathrm{Pr}\left(X \geq a\right) \leq \frac{\mathbb{E}[X]}{a}$.
    \end{proof}
\end{thmbox}
Note that the bound given by Markov's inequality is a rather loose bound. The following inequality proposes a better bound:
\begin{thmbox}{Chebyshev's Inequality}{ChebyshevIneq}
    For any real-valued random variable $X$ with finite variance, 
    \begin{equation*}
        \mathrm{Pr}\left(\abs{X - \mathbb{E}[X]} > a\sqrt{\mathrm{Var}\left(X\right)}\right) \leq \frac{1}{a^2}
    \end{equation*}
    for all $a > 0$.
    \tcblower
    \begin{proof}
        Define $g\left(X\right) \colon \left(X - \mathbb{E}[X]\right)^2$, which is clearly non-negative. By Theorem \ref{thm:MarkovIneq}, we have 
        \begin{equation*}
            \mathrm{Pr}\bigl(g\left(X\right) > a^2\mathrm{Var}\left(X\right)\bigr) \leq \frac{\mathbb{E}[g\left(X\right)]}{a^2\mathrm{Var}\left(X\right)}.
        \end{equation*}
        Note that $\mathbb{E}[g\left(X\right)] = \mathrm{Var}\left(X\right)$, so 
        \begin{equation*}
            \mathrm{Pr}\left(\abs{X - \mathbb{E}[X]} > a\sqrt{\mathrm{Var}\left(X\right)}\right) = \mathrm{Pr}\bigl(g\left(X\right) > a^2\mathrm{Var}\left(X\right)\bigr) \leq \frac{1}{a^2}.
        \end{equation*}
    \end{proof}
\end{thmbox}
Finally, we state the following law of large numbers:
\begin{thmbox}{Weak Law of Large Numbers}{weakLawLargeNum}
    Let $X_1, X_2, \cdots, X_n$ be pairwise independent and identically distributed random variables with $\mathbb{E}[X_i] = \mu$ and $\mathrm{Var}\left(X_i\right) = \sigma^2 \in \R$ for every $i \in \N^+$. For every $\epsilon > 0$, we have 
    \begin{equation*}
        \lim_{n \to \infty}\mathrm{Pr}\left(\abs{\frac{1}{n}\sum_{i = 1}^nX_i - \mu} > \epsilon\right) = 0.
    \end{equation*}
    \tcblower
    \begin{proof}
        Note that $\mathbb{E}\left[\frac{1}{n}\sum_{i = 1}^nX_i\right] = \mu$ and that 
        \begin{equation*}
            \mathrm{Var}\left(\frac{1}{n}\sum_{i = 1}^nX_i\right) = \frac{\sum_{i = 1}^{n}\mathrm{Var}\left(X_i\right)}{n^2} = \frac{\sigma^2}{n}.
        \end{equation*}
        By Theorem \ref{thm:ChebyshevIneq}, we have 
        \begin{equation*}
            0 \leq \mathrm{Pr}\left(\abs{\frac{1}{n}\sum_{i = 1}^nX_i - \mu} > \epsilon\right) \leq \frac{\sigma^2}{n\epsilon^2}.
        \end{equation*}
        By Squeeze Theorem, this clearly implies that 
        \begin{equation*}
            \lim_{n \to \infty}\mathrm{Pr}\left(\abs{\frac{1}{n}\sum_{i = 1}^nX_i - \mu} > \epsilon\right) = 0.
        \end{equation*}
    \end{proof}
\end{thmbox}
Alternatively, we may phrase Theorem \ref{thm:weakLawLargeNum} as ``$\frac{1}{n}\sum_{i = 1}^nX_i$ converges to $\mu$ in probability''. When a sequence $\{S_n\}_{n = 1}^{\infty}$ converges to $b$ in probability, we write $S_n \xrightarrow{\mathrm{p}} b$. 
\begin{notebox}
    \begin{remark}
        Essentially, what Theorem \ref{thm:weakLawLargeNum} says is that when $n$ is large, the sample mean from $n$ measurements of the same data converges to the expectation of the distribution.
    \end{remark}
\end{notebox}
Under some mild conditions, this convergence occurs exponentially fast, i.e., the probability $\mathrm{Pr}\left(\abs{\frac{1}{n}\sum_{i = 1}^nX_i - \mu} > \epsilon\right)$ decreases at least as fast as $\exp\bigl(-ng\left(\epsilon\right)\bigr)$ for some real-valued function $g \colon \R^+ \to \R^+$. In terms of asymptotic analysis, we write this as 
\begin{equation*}
    \mathrm{Pr}\left(\abs{\frac{1}{n}\sum_{i = 1}^nX_i - \mu} > \epsilon\right) \leq \exp\bigl(-ng\left(\epsilon\right) + o\left(n\right)\bigr).
\end{equation*}
Equivalently, this means that there exists a function $g \colon \R \to \R$ with $g\left(\epsilon\right) > 0$ for every $\epsilon > 0$ such that 
\begin{equation*}
    \liminf_{n \to \infty}-\frac{1}{n}\log\mathrm{Pr}\left(\abs{\frac{1}{n}\sum_{i = 1}^nX_i - \mu} > \epsilon\right) \geq g\left(\epsilon\right) + o\left(1\right).
\end{equation*}
There is a strong version for the law, which shall be stated without proof:
\begin{thmbox}{Strong Law of Large Numbers}{strongLawLargeNum}
    Let $X_1, X_2, \cdots, X_n$ be pairwise independent and identically distributed random variables with $\mathbb{E}[X_i] = \mu$ and $\mathrm{Var}\left(X_i\right) = \sigma^2 \in \R$ for every $i \in \N^+$, then 
    \begin{equation*}
        \mathrm{Pr}\left(\lim_{n \to \infty}\frac{1}{n}\sum_{i = 1}^{n}X_i = \mu\right) = 1.
    \end{equation*}
\end{thmbox}
\section{Convexity}
Recall the following definition:
\begin{dfnbox}{Convex Function}{convexFunc}
    A function $f \colon \R^n \to \R$ is {\color{red} \textbf{convex}} if for any $\lambda \in [0, 1]$ and any $\mathbfit{x}, \mathbfit{y} \in \R^n$,
    \begin{equation*}
        f\bigl(\lambda\mathbfit{x} + \left(1 - \lambda\right)\mathbfit{y}\bigr) \leq \lambda f\left(\mathbfit{x}\right) + \left(1 - \lambda\right)f\left(\mathbfit{y}\right).
    \end{equation*}
\end{dfnbox}
From a graphical perspective, a convex function is an overestimate of all linear functions whose values are bounded above by it. The following proposition set this result in a rigorous context:
\begin{probox}{Convex Functions as Overestimates for Linear Functions}{convexFuncEstimate}
    Let $f \colon \R^n \to \R$ be a convex function and define 
    \begin{equation*}
        \mathcal{L} \coloneqq \left\{\ell \in \mathrm{Maps}\left(\R^n, \R\right) \colon \ell\left(\mathbfit{u}\right) = \mathbfit{a}^{\mathrm{T}} \cdot \mathbfit{u} + b \leq f\left(\mathbfit{u}\right) \textrm{ for all } \mathbfit{u} \in \R^n, \mathbfit{a} \in \R^n, b \in \R\right\}
    \end{equation*}
    to be the set of all linear functions bounded above by $f$, then for each $\mathbfit{x} \in \R^n$, 
    \begin{equation*}
        f\left(\mathbfit{x}\right) = \sup_{\ell \in \mathcal{L}}\ell\left(\mathbfit{x}\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        It suffices to prove that for all $\mathbfit{x} \in \R^n$, there exists some linear function~$\ell \in \mathcal{L}$ such that $\ell\left(\mathbfit{x}\right) = f\left(\mathbfit{x}\right)$. Take any $\mathbfit{h} \in \R^n$. Since $f$ is convex, we have 
        \begin{align*}
            2f\left(\mathbfit{x}\right) & = 2f\left(\frac{1}{2}\left(\mathbfit{x + h}\right) + \frac{1}{2}\left(\mathbfit{x - h}\right)\right) \\
            & \leq f\left(\mathbfit{x + h}\right) + f\left(\mathbfit{x - h}\right).
        \end{align*}
        Therefore, we have
        \begin{equation*}
            L_1 = \lim_{\norm{\mathbfit{h}} \to 0}\frac{f\left(\mathbfit{x}\right) - f\left(\mathbfit{x - h}\right)}{\norm{\mathbfit{h}}} \leq \lim_{\norm{\mathbfit{h}} \to 0}\frac{f\left(\mathbfit{x + h}\right) - f\left(\mathbfit{x}\right)}{\norm{\mathbfit{h}}} = L_2.
        \end{equation*}
        Take some $a \in [L_1, L_2]$ and let $\ell\left(\mathbfit{y}\right) = a\norm{\mathbfit{y - x}} + f\left(\mathbfit{x}\right)$. Observe that $\ell\left(\mathbfit{x}\right) = f\left(\mathbfit{x}\right)$. Take $\mathbfit{h} = \mathbfit{y - x}$, then 
        \begin{align*}
            \ell\left(\mathbfit{y}\right) & = a\norm{\mathbfit{y - x}} + f\left(\mathbfit{x}\right) \\
            & \leq \frac{f\left(\mathbfit{x + h}\right) - f\left(\mathbfit{x}\right)}{\norm{\mathbfit{h}}}\norm{\mathbfit{y - x}} + f\left(\mathbfit{x}\right) \\
            & = f\left(\mathbfit{x + h}\right) \\
            & = f\left(\mathbfit{y}\right).
        \end{align*} 
        Therefore, $\ell \in \mathcal{L}$ as desired.
    \end{proof}
\end{probox}
The following proposition gives a simple test for convexity in one-dimensional case, which is a special case of the Hessian matrix test:
\begin{probox}{Second Derivative Test for Convexity}{2ndDiffTest}
    If a real-valued function $f$ is twice-differentiable on $[a, b]$, then it is convex if and only if $f''\left(x\right) \geq 0$ for all $x \in \left(a, b\right)$.
\end{probox}
Convex functions produce the following interesting result regarding expectation:
\begin{thmbox}{Jensen's Inequality}{JensenIneq}
    Let $f$ be a convex function and $X$ be a random variable, then $\mathbb{E}[f\left(X\right)] \geq f\left(\mathbb{E}[X]\right)$.
    \tcblower
    \begin{proof}
        Let $\mathcal{L}$ be the set of all linear functions bounded above by $f$. By Proposition \ref{pro:convexFuncEstimate}, we have 
        \begin{align*}
            \mathbb{E}[f\left(X\right)] & = \mathbb{E}\left[\sup_{\ell \in \mathcal{L}}\ell\left(X\right)\right] \\
            & \geq \sup_{\ell \in \mathcal{L}}\mathbb{E}[\ell\left(X\right)] \\
            & = \sup_{\ell \in \mathcal{L}}\ell\left(\mathbb{E}[X]\right) \\
            & = f\left(\mathbb{E}[X]\right).
        \end{align*}
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        If $f$ is strictly convex, the equality holds if and only if $X$ is constant.
    \end{remark}
\end{notebox}
\chapter{Information Theory}
\section{Entropy}
In information theory, the very first question to ask is how we can measure the quantity of information contained in communication. Colloquially, we say that communication gives more information if more knowledge which has remained unknown previously is revealed.

We describe such revelation of new knowledge as the ``surprise'' of an event. Using probability theory, we use a random variable $X$ to represent an event by $X = x$. Intuitively, an event is surprising if the probability of its occurrence is low. This is formally stated as follows:
\begin{dfnbox}{Surprise}{surprise}
    Let $X$ be a random variable. The {\color{red} \textbf{surprise}} of an event $X = x$ is defined as 
    \begin{equation*}
        \log_2\frac{1}{P_X\left(x\right)} = \log_2\frac{1}{\mathrm{Pr}\left(X = x\right)}.
    \end{equation*}
\end{dfnbox}
Now, suppose we are \textbf{uncertain} about some event $X = x$. We may wish to measure how much uncertainty we have towards the outcome of the event, or equivalently, what is the \textbf{expected surprise} for the event. It is easy to see that if we define a random variable for surprise as a function of $X$, we can make use of the expectation formula to compute this quantity.
\begin{dfnbox}{Entropy of Discrete Random Variables}{entropy}
    Let $X$ be a discrete random variable supported on a finite alphabet $\mathcal{X}$ with probability mass function $p_X$, then {\color{red} \textbf{entropy}} of $X$ is defined as
    \begin{equation*}
        H\left(X\right) \coloneqq -\sum_{x \in \mathcal{X}}p_X\left(x\right)\log_{2}p_X\left(x\right).
    \end{equation*}
\end{dfnbox}
It is clear that this definition can be manipulated into 
\begin{equation*}
    H\left(X\right) = \mathbb{E}\left[\log_2\frac{1}{p_X\left(X\right)}\right],
\end{equation*}
i.e., the entropy of $X$ is exactly the expected surprise of $X$. There is a small problem, though, which is that $\log_2n$ is undefined when $n \leq 0$. Since $p_X\left(x\right) \geq 0$ for all $x \in \mathcal{X}$, we only need to take care of $0$ as a special case. Notice that 
\begin{align*}
    \lim_{x \to 0^+}x\log_2x & = \lim_{x \to 0^+}\frac{x\ln x}{\ln 2} \\
    & = -\frac{1}{\ln 2}\lim_{x \to 0^+}\frac{-\ln x}{x^{-1}} \\
    & = -\frac{1}{\ln 2}\lim_{x \to 0^+}\frac{-x^{-1}}{-x^{-2}} \\
    & = 0.
\end{align*}
Therefore, it makes sense to set $x\log_2x = 0$ when $x = 0$.
\begin{notebox}
    \begin{remark}
        By convention, we set $0\log_20 = 0$.
    \end{remark}
\end{notebox}
We will later prove that $0 \leq H\left(X\right) \leq \log_2\abs{\mathcal{X}}$. Moreover, $H\left(X\right)$ is closely related with the minimal number of bits to encode $X$ in binary number unambiguously. In particular, if we let $b\left(X\right)$ be the minimal number of bits to encode $X$ in binary strings unambiguously, we have 
\begin{equation*}
    H\left(X\right) \leq \mathbb{E}[b\left(X\right)] < H\left(X\right) + 1.
\end{equation*}
Moreover, if we let $q\left(X\right)$ to be the number of attempts to guess the value of $X$ correctly, we might be surprised by the fact that 
\begin{equation*}
    H\left(X\right) \leq \mathbb{E}[q\left(X\right)] < H\left(X\right) + 1,
\end{equation*}
i.e., it is expected to attempt at least $H\left(X\right)$ times to guess the value of $X$, but there is always a strategy to expect success before the $\bigl(H\left(X\right) + 1\bigr)$-th attempt.

Those with prior knowledge in machine learning and decision trees might find the following special form of entropy familiar:
\begin{dfnbox}{Binary Entropy}{binEntropy}
    Let $X$ be a Bernoulli random variable with parameter $p$. The {\color{red} \textbf{binary entropy}} of $p$ is defined as 
    \begin{equation*}
        H_b\left(p\right) = -p\log_2p - \left(1 - p\right)\log_2\left(1 - p\right).
    \end{equation*}
\end{dfnbox}
With some simple computation, it is easy to check that $H_b\left(p\right)$ is maximised when $p = \frac{1}{2}$ and is zero if and only if $p = 1$ or $p = 0$.

Entropy can be defined over multiple random variables just like probability distributions. In fact, we denote the tuple of $n$ random variables as 
\begin{equation*}
    X_{1}^{n} \coloneqq \left(X_1, X_2, \cdots, X_n\right)
\end{equation*}
Clearly, we may view $X_1^n$ as nothing else than a single random variable whose alphabet is just $\mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_n$.
\begin{dfnbox}{Joint Entropy}{jointEntropy}
    Let $X_1^n$ be a tuple of discrete random variable supported on a finite alphabet 
    \begin{equation*}
        \mathcal{X} \coloneqq \mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_n
    \end{equation*}
    with joint probability mass function $p_{X_1^n}$. The {\color{red} \textbf{joint entropy}} of $X_1, X_2, \cdots, X_n$ is defined as 
    \begin{equation*}
        H\left(X_1^n\right) \coloneqq -\sum_{\mathbfit{x} \in \mathcal{X}}p_{X_1^n}\left(\mathbfit{x}\right)\log_{2}p_{X_1^n}\left(\mathbfit{x}\right).
    \end{equation*}
\end{dfnbox}
Additionally, we can of course define the conditional entropy to measure the uncertainty of one event given the information on another event.
\begin{dfnbox}{Conditional Entropy}{condEntropy}
    Let $\left(X, Y\right)$ be a pair of discrete random variables supported on an alphabet~$\mathcal{X} \times \mathcal{Y}$ which is finite. Let $p_X$ and $p_Y$ be the probability mass functions for $X$ and $Y$ respectively. The {\color{red} \textbf{conditional entropy}} of $X$ given $Y$ is defined as 
    \begin{equation*}
        H\left(X \mid Y\right) \coloneqq \sum_{y \in \mathcal{Y}}p_Y\left(y\right)H\left(X \mid Y = y\right).
    \end{equation*}
\end{dfnbox}
Note that here $H\left(X \mid Y = y\right)$ is also known as the \textit{conditional entropy}, but it is different in meaning with $H\left(X \mid Y\right)$. In particular:
\begin{equation*}
    H\left(X \mid Y = y\right) = -\sum_{x \in \mathcal{X}}p_{X \mid Y}\left(x \mid y\right)\log_2p_{X \mid Y}\left(x \mid y\right).
\end{equation*}
Therefore, we can expand the expression in Definition \ref{dfn:condEntropy} into
\begin{align*}
    H\left(X \mid Y\right) & = -\sum_{y \in \mathcal{Y}}p_Y\left(y\right)\sum_{x \in \mathcal{X}}p_{X \mid Y}\left(x \mid y\right)\log_2p_{X \mid Y}\left(x \mid y\right) \\
    & = -\sum_{\left(x, y\right) \in \mathcal{X \times Y}}p_{X, Y}\left(x, y\right)\log_2p_{X \mid Y}\left(x \mid y\right) \\
    & = \mathbb{E}\left[\log_2\frac{1}{p_{X \mid Y}\left(X \mid Y\right)}\right].
\end{align*}
One thing to note is that conditional entropy is \textbf{not symmetric}. We can interpret $H\left(X \mid Y\right)$ as ``the remaining uncertainty of $X$ given information on $Y$''. Hence, it is not surprising that the following identity is true:
\begin{equation*}
    H\left(X, Y\right) = H\left(X\right) + H\left(Y \mid X\right)
\end{equation*}
This is generalised as follows:
\begin{probox}{Chain Rule of Entropy}{chain}
    Let $X_1^n$ be a tuple of any $n$ discrete random variables supported on finite alphabets, then 
    \begin{equation*}
        H\left(X_1^n\right) = \sum_{i = 1}^{n}H\left(X_i \mid X_1^{i - 1}\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        The case where $n = 2$ follows directly from the result that 
        \begin{equation*}
            H\left(X_1, X_2\right) = H\left(X_1\right) + H\left(X_2 \mid X_1\right).
        \end{equation*}
        Suppose that there exists some integer $k \geq 2$ such that $H\left(X_1^k\right) = \sum_{i = 1}^{k}H\left(X_i \mid X_1^{i - 1}\right)$, consider
        \begin{align*}
            H\left(X_1^{k + 1}\right) & = H\left(X_1^k, X_{k + 1}\right) \\
            & = H\left(X_1^k\right) + H\left(X_{k + 1} \mid X_1^k\right) \\
            & = \sum_{i = 1}^{k}H\left(X_i \mid X_1^{i - 1}\right) + H\left(X_{k + 1} \mid X_1^k\right) \\
            & = \sum_{i = 1}^{k + 1}H\left(X_i \mid X_1^{i - 1}\right).
        \end{align*}
    \end{proof}
\end{probox}
A direct application of Proposition \ref{pro:chain} yields the following result:
\begin{corbox}{Chain Rule of Entropy for Conditional Joint Distributions}{chain3}
    Let $X, Y, Z$ be discrete random variables supported on finite alphabets, then 
    \begin{equation*}
        H\left(X, Y \mid Z\right) = H\left(X \mid Z\right) + H\left(Y \mid X, Z\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $X_1 \coloneqq X \mid Z$ and $X_2 \coloneqq Y \mid Z$, then 
        \begin{align*}
            H\left(X, Y \mid Z\right) & = H\left(X_1, X_2\right) \\
            & = H\left(X_1\right) + H\left(X_2 \mid X_1\right) \\
            & = H\left(X \mid Z\right) + H\bigl(\left(Y \mid Z\right) \mid \left(X \mid Z\right)\bigr) \\
            & = H\left(X \mid Z\right) + H\left(Y \mid X, Z\right).
        \end{align*}
    \end{proof}
\end{corbox}
Given different distributions for the same random variable, we may be interested to know how much the distributions differ from one another. In other words, we wish to measure how much one distribution is different from another in terms of uncertainty.
\begin{dfnbox}{Relative Entropy}{relativeEntropy}
    Let $p$ and $q$ be probability mass functions for some discrete random variable $X$ supported over an alphabet $\mathcal{X}$. The {\color{red} \textbf{relative entropy}}, or alternatively, {\color{red} \textbf{Kullback-Leibler (KL) divergence}}, between $p$ and $q$ is defined as 
    \begin{equation*}
        D\left(p \parallel q\right) \coloneqq \sum_{x \in \mathcal{X}}p\left(x\right)\log_2\frac{p\left(x\right)}{q\left(x\right)}.
    \end{equation*}
\end{dfnbox}
The above definition essentially describes the ``difference'' between two distributions as their expected ratio because 
\begin{equation*}
    \sum_{x \in \mathcal{X}}p\left(x\right)\log_2\frac{p\left(x\right)}{q\left(x\right)} = \mathbb{E}\left[\log_2\frac{p(X)}{q(X)}\right].
\end{equation*}
\begin{notebox}
    \begin{remark}
        Using a similar argument to Definition \ref{dfn:entropy}, we set the following conventions:
        \begin{enumerate}
            \item $0\log_2\frac{0}{q} = 0$ for all $q \in \R$;
            \item $p\log_2\frac{p}{0} = +\infty$ for all $p \in \R$.
        \end{enumerate}
    \end{remark}
\end{notebox}
Relative entropy can be defined in a conditional context as well.
\begin{dfnbox}{Conditional Relative Entropy}{condRelativeEntropy}
    Let $p_{X, Y}$ and $q_{X, Y}$ be joint probability mass functions for some pair of discrete random variables $\left(X, Y\right)$ supported over an alphabet $\mathcal{X} \times \mathcal{Y}$. The {\color{red} \textbf{conditional relative entropy}} between $p_{Y \mid X}$ and $q_{Y \mid X}$ averaged over $X$ is 
    \begin{equation*}
        D\left(p_{Y \mid X} \parallel q_{Y \mid X} \mid p_X\right) \coloneqq \sum_{x \in \mathcal{X}}p_X\left(x\right)D\left(p_{Y \mid X}\left(\cdot \parallel x\right) \parallel q_{Y \mid X}\left(\cdot \parallel x\right)\right).
    \end{equation*}
\end{dfnbox}
Notice that by applying Proposition \ref{pro:chain}, we have 
\begin{equation*}
    H\left(X\right) + H\left(Y \mid X\right) = H\left(Y\right) + H\left(X \mid Y\right)
\end{equation*}
due to $H\left(X, Y\right)$ being symmetric. This implies that 
\begin{equation*}
    H\left(X\right) - H\left(X \mid Y\right) = H\left(Y\right) - H\left(Y \mid X\right).
\end{equation*}
Informally speaking, the left-hand side of the above identity is the remaining uncertainty of $X$ after knowing $Y$, while the right-hand side is that of $Y$ after knowing $X$. Note that this quantity can be interpreted as ``the uncertain part of $X$ and $Y$ which cannot be reduced by knowing one of them''. In other words, this remaining uncertainty is shared by both $X$ and $Y$. The following notion formalises this observation:
\begin{dfnbox}{Mutual Information}{mutualInfo}
    Let $\left(X, Y\right)$ be a pair of discrete random variables with joint probability mass function~$p_{X, Y}$. The {\color{red} \textbf{mutual information}} between $X$ and $Y$ is defined as 
    \begin{equation*}
        I\left(X ; Y\right) \coloneqq D\left(p_{X, Y} \parallel p_X \cdot p_Y\right).
    \end{equation*}
\end{dfnbox}
It turns out that mutual information is symmetric, because 
\begin{align*}
    I\left(X ; Y\right) & = D\left(p_{X, Y} \parallel p_X \cdot p_Y\right) \\
    & = \sum_{(x, y) \in \mathcal{X} \times \mathcal{Y}}p_{X, Y}(x, y)\log_2\frac{p_{X, Y}(x, y)}{p_X(x)p_Y(y)} \\
    & = \mathbb{E}_{p_{X, Y}}\left[\log_2\frac{p_{X, Y}(x, y)}{p_X(x)p_Y(y)}\right].
\end{align*}
One may check that 
\begin{equation*}
    I\left(X ; Y\right) = H\left(X\right) - H\left(X \mid Y\right) = H\left(Y\right) - H\left(Y \mid X\right).
\end{equation*}
In this way, by using Proposition \ref{pro:chain}, we can also see that 
\begin{align*}
    I\left(X ; Y\right) & = H\left(X\right) - H\left(X \mid Y\right) \\
    & = H\left(X\right) + H\left(Y\right) - H\left(X, Y\right),
\end{align*}
and hence the symmetric property of mutual information.

Naturally, the mutual information between $X$ and $Y$ cannot exceed the entropy of either of them. Therefore, it is intuitive that 
\begin{equation*}
    0 \leq I(X ; Y) \leq \min\left\{\log_2\abs{\mathcal{X}}, \log_2\abs{\mathcal{Y}}\right\}.
\end{equation*}
Mutual information can be conditional as well.
\begin{probox}{Chain Rule of Mutual Information}{IChain}
    Let $\left(X_1^n, Y\right)$ be a tuple of discrete random variables with joint probability mass function~$p$, then 
    \begin{equation*}
        I\left(X_1^n ; Y\right) = \sum_{i = 1}^{n}I\left(X_i ; Y \mid X_1^{i - 1}\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        By using Proposition \ref{pro:chain},
        \begin{align*}
            I\left(X_1^n ; Y\right) & = H\left(X_1^n\right) - H\left(X_1^n \mid Y\right) \\
            & = \sum_{i = 1}^{n}H\left(X_i \mid X_1^{i - 1}\right) - \sum_{i = 1}^{n}H\left(X_i \mid Y, X_i^{i - 1}\right) \\
            & = \sum_{i = 1}^{n}\Bigl(H\left(X_i \mid X_1^{i - 1}\right) - H\left(X_i \mid Y, X_i^{i - 1}\right)\Bigr) \\
            & = \sum_{i = 1}^{n}I\left(X_i ; Y \mid X_1^{i - 1}\right).
        \end{align*}
    \end{proof}
\end{probox}
We could make some analogy between entropy and set theory. Suppose we have two random variables $X$ and $Y$, we could let some sets $\mathcal{H}_X$ and $\mathcal{H}_Y$ represent $H\left(X\right)$ and $H\left(Y\right)$. It is intuitive to see that $H\left(X \mid Y\right)$ corresponds to $\mathcal{H}_X \backslash \mathcal{H}_Y$, $H\left(X, Y\right)$ corresponds to $\mathcal{H}_X \cup \mathcal{H}_Y$, and that $I\left(X ; Y\right)$ corresponds to $\mathcal{H}_X \cap \mathcal{H}_Y$.

This inspires us to study mutual information between more than $2$ random variables via the principle of inclusion and exclusion. However, the situation becomes problematic when we consider more random variables. It can be shown that there exist random variables $X, Y, Z$ such that $I\left(X ; Y ; Z\right) < 0$, which does not make much sense in information theory.
\section{Information Inequality}
A lot of theorems in information theory are developed from inequalities. Among them, the core inequality result is known as the \textit{information inequality} which can be used to prove a wide range of corollaries.
\begin{thmbox}{Information Inequality}{infoIneq}
    For any probability mass functions $p$ and $q$ for some random variable $X$, $D\left(p \parallel q\right) \geq 0$. The equality is attained if and only if $p = q$.
    \tcblower
    \begin{proof}
        Let $A \coloneqq \left\{x \in \mathcal{X} \colon p\left(x\right) > 0\right\}$. Take $Y \coloneqq \frac{q\left(X\right)}{p\left(X\right)}$ with support 
        \begin{equation*}
            \mathcal{Y} \coloneqq \left\{\frac{q\left(x\right)}{p\left(x\right)} \colon x \in A\right\}.
        \end{equation*}
        By Theorem \ref{thm:JensenIneq}, we have 
        \begin{equation*}
            \sum_{x \in A}p\left(x\right)\log_2\frac{q\left(x\right)}{p\left(x\right)} = \mathbb{E}_p\left[\log_2Y\right] \leq \log_2\mathbb{E}_p\left[Y\right] = \log_2\sum_{x \in A}p\left(x\right)\frac{q\left(x\right)}{p\left(x\right)}.
        \end{equation*}
        Note that $q\left(x\right) \geq 0$ for all $x \in \mathcal{X}$ and $p\left(x\right) > 0$ for all $x \in A$. Therefore, 
        \begin{align*}
            -D\left(p \parallel q\right) & = -\sum_{x \in \mathcal{X}}p\left(x\right)\log_2\frac{p\left(x\right)}{q\left(x\right)} \\
            & = \sum_{x \in A}p\left(x\right)\log_2\frac{q\left(x\right)}{p\left(x\right)} \\
            & \leq \log_2\sum_{x \in A}p\left(x\right)\frac{q\left(x\right)}{p\left(x\right)} \\
            & = \log_2\sum_{x \in A}q\left(x\right) \\
            & \leq \log_2\sum_{x \in \mathcal{X}}q\left(x\right) \\
            & = \log_21 \\
            & = 0.
        \end{align*}
        Therefore, $D\left(p \parallel q\right) \geq 0$. Clearly, the equality holds if and only if 
        \begin{equation*}
            \mathbb{E}_p\left[\log_2Y\right] = \log_2\mathbb{E}_p\left[Y\right] \quad\textrm{and}\quad \sum_{x \in A}q\left(x\right) = \sum_{x \in \mathcal{X}}q\left(x\right).
        \end{equation*}
        Note that $f\left(x\right) = \log_2x$ is strictly convex, so $\mathbb{E}_p\left[\log_2Y\right] = \log_2\mathbb{E}_p\left[Y\right]$ if and only if~$Y$ is constant, i.e., $\frac{q\left(x\right)}{p\left(x\right)} = c$ for some fixed $c \in \R$ for all $x \in A$. Notice that this is equivalent to 
        \begin{equation*}
            1 = \sum_{x \in A}q\left(x\right) = c\sum_{x \in A}p\left(x\right) = c,
        \end{equation*}
        i.e., $p\left(x\right) = q\left(x\right)$ for all $x \in A$. Note that in the same time, $q\left(x\right) = 0$ for all $x \in \mathcal{X} \backslash A$, i.e., $q\left(x\right) = 0$ if and only if $p\left(x\right) = 0$. Therefore, $p = q$ as desired.
    \end{proof}
\end{thmbox}
The information inequality leads to many bounding conditions to the common quantities we have discussed so far.
\begin{corbox}{Mutual Information Is Non-negative}{nonnegativeI}
    For any jointly distributed discrete random variables $X$ and $Y$, $I\left(X ; Y\right) \geq 0$ with equality attained if and only if $X$ and $Y$ are independent.
    \tcblower
    \begin{proof}
        Notice that 
        \begin{align*}
            \sum_{\left(x, y\right) \in \mathcal{X \times Y}}p_X\left(x\right)p_Y\left(y\right) = \left(\sum_{x \in \mathcal{X}}p_X\left(x\right)\right)\left(\sum_{y \in \mathcal{Y}}p_Y\left(y\right)\right) = 1,
        \end{align*}
        so $p_X \cdot p_Y$ is a probability mass function for $\left(X, Y\right)$. Therefore, by Theorem \ref{thm:infoIneq}, 
        \begin{align*}
            I\left(X ; Y\right) = D\left(p_{X, Y} \parallel p_X \cdot p_Y\right) \geq 0,
        \end{align*}
        where the equality is attained if and only if $p_{X, Y} = p_X \cdot p_Y$, i.e., $X$ and $Y$ are independent.
    \end{proof}
\end{corbox}
Naturally, the conditional relative entropy should also be non-negative.
\begin{corbox}{Conditional Relative Entropy Is Non-negative}{nonnegativeCondKLDiv}
    For any pair of discrete random variables $\left(X, Y\right)$, $D\left(p_{Y \mid X} \parallel q_{Y \mid X} \mid p_X\right) \geq 0$ with equality attained if and only if $p_{Y \mid X}\left(\cdot \mid x\right) = q_{Y \mid X}\left(\cdot \mid x\right)$ for all $x \in \mathcal{X} \backslash p_X^{-1}\left[\left\{0\right\}\right]$. 
    \tcblower
    \begin{proof}
        By Theorem \ref{thm:infoIneq}, 
        \begin{equation*}
            D\left(p_{Y \mid X}\left(\cdot \parallel x\right) \parallel q_{Y \mid X}\left(\cdot \parallel x\right)\right) \geq 0
        \end{equation*}
        for all $x \in \mathcal{X}$. Since $p_X\left(x\right) \geq 0$ for all $x \in \mathcal{X}$, clearly 
        \begin{equation*}
            D\left(p_{Y \mid X} \parallel q_{Y \mid X} \mid p_X\right) \geq 0,
        \end{equation*}
        where the equality is attained if and only if 
        \begin{equation*}
            D\left(p_{Y \mid X}\left(\cdot \parallel x\right) \parallel q_{Y \mid X}\left(\cdot \parallel x\right)\right) = 0
        \end{equation*}
        for all $x \in \mathcal{X} \backslash p_X^{-1}\left[\left\{0\right\}\right]$. This is equivalent to $p_{Y \mid X}\left(\cdot \parallel x\right) = q_{Y \mid X}\left(\cdot \parallel x\right)$ for all $x \in \mathcal{X} \backslash p_X^{-1}\left[\left\{0\right\}\right]$.
    \end{proof}
\end{corbox}
We can do a similar argument for conditional mutual information as well.
\begin{corbox}{Conditional Mutual Information Is Non-negative}{nonnegativeCondI}
    For any discrete random variables $X, Y, Z$, we have $I\left(X ; Y \mid Z\right) \geq 0$ with equality attained if and only if $X$-$Z$-$Y$ is a Markov chain.
    \tcblower
    \begin{proof}
        Notice that by Theorem \ref{thm:infoIneq},
        \begin{equation*}
            I\left(X ; Y \mid Z\right) = D\left(p_{X, Y \mid Z} \parallel p_{X \mid Z} \cdot p_{Y \mid Z}\right) \geq 0,
        \end{equation*}
        where the equality is attained if and only if $p_{X, Y \mid Z} = p_{X \mid Z} \cdot p_{Y \mid Z}$. 
    \end{proof}
\end{corbox}
Recall that we previously mentioned that $0 \leq H\left(X\right) \leq \log_2\abs{\mathcal{X}}$. The upper bound can be derived using the information inequality as well.
\begin{probox}{Upper Bound of Entropy}{maxEntropy}
    For any discrete random variable $X$ supported on a finite alphabet $\mathcal{X}$, we have $H\left(X\right) \leq \log_2\abs{\mathcal{X}}$ with equality attained if and only if $p_X$ is uniform on $\mathcal{X}$.
    \tcblower
    \begin{proof}
        Define $u\left(x\right) \coloneqq \frac{1}{\abs{\mathcal{X}}}$ to be the uniform distribution over $\mathcal{X}$. Consider 
        \begin{align*}
            D\left(p_X \parallel u\right) & = \sum_{x \in \mathcal{X}}p_X\left(x\right)\log_2\frac{p_X\left(x\right)}{u\left(x\right)} \\
            & = \sum_{x \in \mathcal{X}}p_X\left(x\right)\log_2\abs{\mathcal{X}}p_X\left(x\right) \\
            & = \sum_{x \in \mathcal{X}}p_X\left(x\right)\log_2\abs{\mathcal{X}} + \sum_{x \in \mathcal{X}}p_X\left(x\right)\log_2p_X\left(x\right) \\
            & = \log_2\abs{X} - H\left(X\right).
        \end{align*}
        By Theorem \ref{thm:infoIneq}, we have $D\left(p_X \parallel u\right) \geq 0$ and so $H\left(X\right) \leq \log_2\abs{X}$. The equality is attained if and only if $p_X = u$ is uniform over $\mathcal{X}$.
    \end{proof}
\end{probox}
One important result derived from this upper bound is as follows:
\begin{corbox}{Conditioning Does Not Increase Entropy}{condDoesNotIncrEntropy}
    For any pair of discrete random variables $\left(X, Y\right)$, we have $H\left(X \mid Y\right) \leq H\left(X\right)$ with equality attained if and only if $X$ and $Y$ are independent.
    \tcblower
    \begin{proof}
        Notice that 
        \begin{equation*}
            H\left(X\right) - H\left(X \mid Y\right) = I\left(X ; Y\right) \geq 0
        \end{equation*}
        by Corollary \ref{cor:nonnegativeI}, so $H\left(X \mid Y\right) \leq H\left(X\right)$ as desired.
    \end{proof}
\end{corbox}
However, do note that there could exist some $y \in \mathcal{Y}$ such that $H\left(X \mid Y = y\right) > H\left(X\right)$. For example, consider the following joint distribution:
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        \diagbox{$Y$}{$X$} & $1$ & $2$ \\
        \hline 
        $1$ & $0$ & $0.75$ \\
        \hline 
        $2$ & $0.125$ & $0.125$ \\
        \hline
    \end{tabular}
\end{center}
We compute the conditional entropy values:
\begin{align*}
    H\left(X \mid Y = 1\right) & = -p_{X \mid Y}\left(2 \mid 1\right)\log_2p_{X \mid Y}\left(2 \mid 1\right) = 0, \\
    H\left(X \mid Y = 2\right) & = -p_{X \mid Y}\left(1 \mid 2\right)\log_2p_{X \mid Y}\left(1 \mid 2\right) - p_{X \mid Y}\left(2 \mid 2\right)\log_2p_{X \mid Y}\left(2 \mid 2\right) = 1.
\end{align*}
However, $H\left(X \mid Y\right) = p_Y(2)H\left(X \mid Y = 2\right) = 0.25 < H\left(X \mid Y = 2\right)$. 

An important result which can be proven using the above facts is the \textit{Hans's inequality}, which states that 
\begin{equation*}
    H\left(X_1, X_2, X_3\right) \leq \frac{1}{2}\bigl(H\left(X_1, X_2\right) + H\left(X_1, X_3\right) + H\left(X_2, X_3\right)\bigr).
\end{equation*}
This inequality can be easily proven by considering the fact that 
\begin{equation*}
    H\left(X_1, X_2, X_3\right) = H\left(X_1 \mid X_2, X_3\right) + H\left(X_2, X_3\right).
\end{equation*}
However, the inequality can in fact be seen as a special case for a more powerful result known as \textit{Shearer's inequality}.
\begin{thmbox}{Shearer's Inequality}{Shearer}
    For any $n \in \N$ and any random subset $\mathcal{S} \subseteq \left[n\right] \backslash \left\{0\right\}$ such that $\Pr\left(i \in \mathcal{S}\right) \geq \mu$ for all~$i \in \left[n\right]$, 
    \begin{equation*}
        \mathbb{E}\left[H\left(X_{\mathcal{S}}\right)\right] \geq H\left(X_1^n\right).
    \end{equation*} 
    \tcblower
    \begin{proof}
        For any $\mathcal{S} \subseteq \left[n\right] \backslash \left\{0\right\}$, we can write 
        \begin{equation*}
            \mathcal{S} = \left\{i_1, i_2, \cdots, i_k\right\}
        \end{equation*}
        for some $k \in \left[n\right]$ such that $i_p < i_q$ whenever $p < q$. Notice that 
        \begin{equation*}
            H\left(X_{\mathcal{S}}\right) = \sum_{j = 1}^{k}H\left(X_{i_j} \mid X_{i_1}, X_{i_2}, \cdots, X_{i_{j - 1}}\right) \geq \sum_{j = 1}^{k}H\left(X_{i_j} \mid X_1^{i_j - 1}\right).
        \end{equation*}
        Therefore,
        \begin{align*}
            \mathbb{E}\left[H\left(X_{\mathcal{S}}\right)\right] & \geq \mathbb{E}\left[\sum_{i \in \mathcal{S}}H\left(X_{i} \mid X_1^{i - 1}\right)\right] \\
            & = \mathbb{E}\left[\sum_{i = 1}^{n}\mathbf{1}\left\{i \in \mathcal{S}\right\}H\left(X_{i} \mid X_1^{i - 1}\right)\right] \\
            & = \sum_{i = 1}^{n}\mathbb{E}\left[\mathbf{1}\left\{i \in \mathcal{S}\right\}\right]H\left(X_{i} \mid X_1^{i - 1}\right) \\
            & = \sum_{i = 1}^{n}\Pr\left(i \in \mathcal{S}\right)H\left(X_{i} \mid X_1^{i - 1}\right) \\
            & \geq \mu\sum_{i = 1}^{n}H\left(X_{i} \mid X_1^{i - 1}\right) \\
            & = \mu H\left(X_1^n\right).
        \end{align*}
    \end{proof}
\end{thmbox}
A different way to state Shearer's inequality uses a combinatorial construction:
\begin{thmbox}{Equivalent Statement for Shearer's Inequality}{ShearerEquiv}
    Let $X_1, X_2, \cdots, X_n$ be $n$ random variables for some $n \in \N^+$. For every positive integer~$m \leq n$, define $\mathcal{S}_m \subseteq \mathcal{P}\left(\left[n\right] \backslash \left\{0\right\}\right)$ to be a collection of subsets of $\left[n\right] \backslash \left\{0\right\}$ such that for each~$i \in \left[n\right] \backslash \left\{0\right\}$, there exist at least $m$ sets containing $i$ in $\mathcal{S}_m$, then
    \begin{equation*}
        H\left(X_1, X_2, \cdots, X_n\right) \leq \frac{1}{m}\sum_{S \in \mathcal{S}_m}H\left(X_{S}\right),
    \end{equation*}
    where $X_{S} = \left\{X_i \colon i \in S \subseteq \left[n\right] \backslash \left\{0\right\}\right\}$.
\end{thmbox}
Using either version, we can prove Hans's inequality by simply taking $n = 3$ and $\mathcal{S}$ to be uniform over all subsets of $\left\{1, 2, 3\right\}$ with cardinality $2$.
\begin{corbox}{Hans's Inequality}{Hans}
    For any random variables $X_1, X_2, X_3$,
    \begin{equation*}
        H\left(X_1, X_2, X_3\right) \leq \frac{1}{2}\bigl(H\left(X_1, X_2\right) + H\left(X_1, X_3\right) + H\left(X_2, X_3\right)\bigr).
    \end{equation*}
\end{corbox}
Theorem \ref{thm:Shearer} has many seemingly surprising applications. For example, we can apply it to prove some results in graph theory by probabilistic methods.
\begin{probox}{The Number of Triangles in Simple Undirected Graphs}{NumTri}
    Let $G$ be a simple undirected graph containing $t$ triangles, then 
    \begin{equation*}
        t \leq \frac{1}{6}\bigl(2e\left(G\right)\bigr)^{\frac{3}{2}}.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $\mathcal{V}$ be the set of all ordered $3$-tuples inducing some~$C_3 \subseteq G$. Notice that for every $H \subseteq G$ such that $H \cong C_3$, there are $6$ different permutations of $V\left(H\right)$. Therefore,~$\abs{\mathcal{V}} = 6t$. Let $\left(X_1, X_2, X_3\right)$ be uniformly distributed over $\mathcal{V}$, then clearly 
        \begin{equation*}
            \Pr\left(X_1 = v_i, X_2 = v_j, X_3 = v_k\right) = \frac{1}{6t}
        \end{equation*}
        for all $\left(v_i, v_j, v_k\right) \in \mathcal{V}$. By Proposition \ref{pro:maxEntropy}, it is clear that $H\left(X_1, X_2, X_3\right) = \log_2\left(6t\right)$. Let $\mathcal{S}$ be uniformly distributed over $\mathcal{P}\left(\left\{1, 2, 3\right\}\right)$, then by Theorem \ref{thm:Shearer}, 
        \begin{equation*}
            \mathbb{E}\left[H\left(X_{\mathcal{S}}\right)\right] \geq \frac{2}{3}H\left(X_1, X_2, X_3\right) = \frac{2}{3}\log_2\left(6t\right).
        \end{equation*}
        Notice that this implies that there exists some $\mathcal{T} \subseteq \left\{1, 2, 3\right\}$ with $\abs{\mathcal{T}} = 2$ such that~$H\left(X_{\mathcal{T}}\right) \geq \frac{2}{3}\log_2\left(6t\right)$. However, observe that $X_{\mathcal{T}}$ is supported over the set of all ordered pairs induced by $E\left(G\right)$, which means that $H\left(X_{\mathcal{T}}\right) \leq \log_2\bigl(2e\left(G\right)\bigr)$. Therefore, 
        \begin{equation*}
            \frac{2}{3}\log_2\left(6t\right) \leq \log_2\bigl(2e\left(G\right)\bigr).
        \end{equation*}
        One may check that this inequality is equivalent to 
        \begin{equation*}
            t \leq \frac{1}{6}\bigl(2e\left(G\right)\bigr)^{\frac{3}{2}}.
        \end{equation*}
    \end{proof}
\end{probox}
We have seen how useful Corollary \ref{cor:condDoesNotIncrEntropy} is. Actually, the result can be generalised further. We first introduce a preliminary definition.
\begin{dfnbox}{Mutual Independence}{mutualInd}
    Let $X_1, X_2, \cdots, X_n$ be any $n$ random variables. They are said to be {\color{red} \textbf{mutually independent}} if for any $S \subseteq \left[n\right] \backslash \left\{0\right\}$, 
    \begin{equation*}
        p_{X_S} = \prod_{i \in S}p_{X_i}.
    \end{equation*}
\end{dfnbox}
Now, we propose the following inequality for conditional entropy:
\begin{corbox}{Sub-additivity of Conditional Entropy}{subadd}
    For any random variables $X_1, X_2, \cdots, X_n$,
    \begin{equation*}
        H\left(X_1^n\right) \leq \sum_{i = 1}^{n}H\left(X_i\right),
    \end{equation*}
    where the equality attained if and only if the $X_i$'s are mutually independent.
    \tcblower
    \begin{proof}
        By Corollary \ref{cor:condDoesNotIncrEntropy}, $H\left(X_i \mid X_1^{i - 1}\right) \leq H\left(X_i\right)$ for all $i = 1, 2, \cdots, n$, so
        \begin{align*}
            H\left(X_1^n\right) & = \sum_{i = 1}^nH\left(X_i \mid X_1^{i - 1}\right)  \leq \sum_{i = 1}^{n}H\left(X_i\right).
        \end{align*}
        The equality is attained if and only if $X_i$ and $X_j$ are independent whenever $i \neq j$, i.e., the $X_i$'s are mutually independent.
    \end{proof}
\end{corbox}
The next inequality is very useful tool which can be used to prove the many results derived from the information inequality so far.
\begin{thmbox}{Log-sum Inequality}{logsum}
    Let $\left\{a_i\right\}_{i = 1}^n$ and $\left\{b_i\right\}_{i = 1}^n$ be non-negative real-valued sequences, then 
    \begin{equation*}
        \sum_{i = 1}^{n}a_i\log_2\frac{a_i}{b_i} \geq \left(\sum_{i = 1}^{n}a_i\right)\log_2\frac{\sum_{i = 1}^{n}a_i}{\sum_{i = 1}^{n}b_i}
    \end{equation*}
    \tcblower
    \begin{proof}
        If there exists some $i \in \N^+$ such that $b_i = 0$, then the left-hand side is $+\infty$. If there exists some $i \in \N^+$ such that $a_i = 0$, then $a_i$ contribute $0$ to both side of the inequality. Therefore, without loss of generality, we can assume that $a_i, b_i > 0$ for all~$i = 1, 2, \cdots, n$. Let $a = \sum_{i = 1}^{n}a_i$ and $b = \sum_{i = 1}^nb_i$. One may check that the function~$f\left(x\right) = x\log_2x$ is strictly convex, so
        \begin{align*}
            \sum_{i = 1}^{n}\frac{b_ia_i}{bb_i}\log_2\frac{a_i}{b_i} & \geq \left(\sum_{i = 1}^{n}\frac{b_ia_i}{bb_i}\right)\log_2\sum_{i = 1}^{n}\frac{b_ia_i}{bb_i}.
        \end{align*}
        Simplifying the inequality yields 
        \begin{align*}
            \frac{1}{b}\sum_{i = 1}^{n}a_i\log_2\frac{a_i}{b_i} & \geq \frac{a}{b}\log_2\frac{a}{b},
        \end{align*}
        and so 
        \begin{equation*}
            \sum_{i = 1}^{n}a_i\log_2\frac{a_i}{b_i} \geq a\log_2\frac{a}{b}.
        \end{equation*}
    \end{proof}
\end{thmbox}
One result which can be proven with the aid of the log-sum inequality is the joint convexity of relative entropy.
\begin{probox}{Convexity of Relative Entropy}{convextRelativeEntropy}
    $D\left(p \parallel q\right)$ is jointly convex.
    \tcblower
    \begin{proof}
        Let $p_1, p_2, q_1, q_2$ be probability mass functions for the same random variable $X$. It suffices to prove that for any $\lambda \in \left[0, 1\right]$,
        \begin{equation*}
            D\bigl(\lambda p_1 + \left(1 - \lambda\right)p_2 \parallel \lambda q_1 + \left(1 - \lambda\right)q_2\bigr) \leq \lambda D\left(p_1 \parallel q_1\right) + \left(1 - \lambda\right)D\left(p_2 \parallel q_2\right).
        \end{equation*}
        For any $x \in \mathcal{X}$, let 
        \begin{align*}
            x_{p_1} = \lambda p_1\left(x\right), & \qquad x_{p_2} = \left(1- \lambda\right)p_2\left(x\right); \\
            x_{q_1} = \lambda q_1\left(x\right), & \qquad x_{q_2} = \left(1- \lambda\right)q_2\left(x\right).
        \end{align*}
        By Theorem \ref{thm:logsum},
        \begin{equation*}
            \left(x_{p_1} + x_{p_2}\right)\log_2\frac{x_{p_1} + x_{p_2}}{x_{q_1} + x_{q_2}} \leq x_{p_1}\log_2\frac{x_{p_1}}{x_{q_1}} + x_{p_2}\log_2\frac{x_{p_2}}{x_{q_2}}.
        \end{equation*}
    \end{proof}
\end{probox}
We can use a similar approach to analyse the convexity of entropy
\begin{probox}{Concavity of Entropy}{concaveEntropy}
    For any discrete random variable $X$ with a finite alphabet and distribution $p$, $H\left(p\right)$ is concave in $p$.
    \tcblower
    \begin{proof}
        Let $u$ be the uniform distribution for $X$, then $u\left(x\right) = \frac{1}{\abs{\mathcal{X}}}$ for all $x \in \mathcal{X}$. Notice that 
        \begin{align*}
            D\left(p \parallel u\right) & = \sum_{x \in \mathcal{X}}p\left(x\right)\log_2\frac{p\left(x\right)}{u\left(x\right)} \\
            & = \sum_{x \in \mathcal{X}}p\left(x\right)\log_2\frac{1}{u\left(x\right)} - \sum_{x \in \mathcal{X}}p\left(x\right)\log_2\frac{1}{p\left(x\right)} \\
            & = \log_2\abs{\mathcal{X}} - H\left(p\right).
        \end{align*}
        By Proposition \ref{pro:convextRelativeEntropy}, $D\left(p \parallel u\right)$ is convex, so $H\left(p\right)$ must be concave.
    \end{proof}
\end{probox}
Alternatively, let $T \sim \mathrm{Bernoulli}\left(\lambda\right)$ and define $Z \coloneqq X \mid T$, where $X \mid T = 1$ and $X \mid T = 0$ have distributions $p_1$ and $p_2$ respectively, then clearly $p_Z = \lambda p_1 + \left(1 - \lambda\right)p_2$. Therefore,
\begin{align*}
    H\bigl(\lambda p_1 + \left(1 - \lambda\right)p_2\bigr) & = H\left(Z\right) \\
    & \geq H\left(Z \mid T\right) \\
    & = p_T\left(1\right)H\left(Z \mid T = 1\right) + p_T\left(0\right)H\left(Z \mid T = 0\right) \\
    & = \lambda H\left(X \mid T = 1\right) + \left(1 - \lambda\right)H\left(X \mid T = 0\right) \\
    & = \lambda H\left(p_1\right) + \left(1 - \lambda\right)H\left(p_2\right).
\end{align*}
This gives a more classical approach to proving Proposition \ref{pro:concaveEntropy}.

Note that we can view $I\left(X ; Y\right)$ as a function of $p_{X, Y}$, which can be further unpacked as a function of $p_X$ and $p_{Y \mid X}$. Here, $p_X$ is known as the \textit{input distribution} and $p_{Y \mid X}$ is known as the \textit{channel}.
\begin{thmbox}{Convexity of Mutual Information}{convexI}
    $I\left(X ; Y\right)$ is concave in $p_X$ and convex in $p_{Y \mid X}$.
    \tcblower
    \begin{proof}
        Fix $p_{Y \mid X}$, consider
        \begin{align*}
            I\left(X ; Y\right) & = H\left(Y\right) - H\left(X \mid Y\right) \\
            & = H\left(p_Y\right) - \sum_{x \in \mathcal{X}}p_X\left(x\right)H\left(Y \mid X = x\right).
        \end{align*}
        Note that $H\left(Y \mid X = x\right)$ only depends on $p_{Y \mid X}$ and so is a constant and for any $y \in \mathcal{Y}$,
        \begin{equation*}
            p_Y\left(y\right) = \sum_{x \in \mathcal{X}}p_X\left(x\right)p_{Y \mid X}\left(y \mid x\right)
        \end{equation*}
        is linear in $p_X\left(x\right)$. Therefore, $I\left(X ; Y\right)$ is linear in $p_X$ and so concave in $p_X$. Now, fix $p_X$ and define $T \sim \mathrm{Bernoulli}\left(\lambda\right)$ to be independent of $X$, then 
        \begin{equation*}
            p_{Y \mid X} = \lambda p_{Y \mid X, T = 1} + \left(1 - \lambda\right)p_{Y \mid X, T = 0}.
        \end{equation*}
        Notice that 
        \begin{equation*}
            I\left(X ; T\right) + I\left(X ; Y \mid T\right) = I\left(X ; Y\right) = I\left(X ; Y\right) + I\left(X ; T \mid Y\right).
        \end{equation*}
        Since $I\left(X ; T\right) = 0$, this implies that $I\left(X ; Y \mid T\right) = I\left(X ; Y\right) + I\left(X ; T \mid Y\right)$. This means that 
        \begin{align*}
            I\left(X ; Y\right) & \leq I\left(X ; Y \mid T\right) \\
            & = \lambda I\left(X ; Y \mid T = 1\right) + \left(1 - \lambda\right)I\left(X ; Y \mid T = 0\right),
        \end{align*}
        which implies that $I\left(X ; Y\right)$ is convex in $p_{Y \mid X}$. 
    \end{proof}
\end{thmbox}
The next inequality is concerning data processing.
\begin{thmbox}{Data Processing Inequality (DPI)}{DPI}
    If $X$-$Y$-$Z$ forms a Markov chain, then $I\left(X ; Y\right) \geq I\left(X ; Z\right)$.
    \tcblower
    \begin{proof}
        Notice that 
        \begin{equation*}
            I\left(X ; Z\right) + I\left(X ; Y \mid Z\right) = I\left(X ; Y, Z\right) = I\left(X ; Y\right) + I\left(X; Z \mid Y\right).
        \end{equation*}
        Since $X$-$Y$-$Z$ forms a Markov chain, $I\left(X ; Z \mid Y\right) = 0$. Therefore, 
        \begin{equation*}
            I\left(X ; Y\right) = I\left(X ; Z\right) + I\left(X ; Y \mid Z\right) \geq I\left(X ; Z\right).
        \end{equation*}
    \end{proof}
\end{thmbox}
Mathematically, data processing can be described as mapping a random variable $Y$ to some transformed image via a function $g$. Recall that in Proposition \ref{pro:funcMarkovChain}, we have shown that for any random variables $X$ and $Y$, $X$-$Y$-$f\left(Y\right)$ is always a Markov chain, which motivates the following application of the DPI.
\begin{corbox}{Mutual Information Does Not Increase After Processing}{nonIncrI}
    For any random variables $X$ and $Y$, we have $I\left(X ; Y\right) \geq I\bigl(X ; g\left(Y\right)\bigr)$ for all function $g$.
\end{corbox}
One implication of this is that no matter what method is used to process the information~$Y$, the shared knowledge between $Y$ and some other data set $X$ can be at best retained at the same level as before.
\section{Sufficient Statistics}
Consider a parametric family of probability distributions $\left\{f_{\theta}\left(x\right) \colon \theta \in \Theta\right\}$ for some index set $\Theta$. Let $T\left(X\right)$ be any statistic. One may check that $\Theta$-$X$-$T\left(X\right)$ is a Markov chain. By Theorem \ref{thm:DPI} we know that $I\left(\Theta ; T\left(X\right)\right) \leq I\left(\Theta ; X\right)$. If the equality is attained, no information is lost in this statistic.
\begin{dfnbox}{Sufficient Statistic}{sufficientStats}
    A function $T \colon \mathcal{X} \to \R$ is said to be a {\color{red} \textbf{sufficient statistic}} relative to a parametric family $\mathcal{F}_{\theta} \coloneqq \left\{f_{\theta}\left(x\right) \colon \theta \in \Theta\right\}$ of probability distributions if $\Theta$-$T\left(X\right)$-$X$ forms a Markov chain.
\end{dfnbox}
Let $X$ be any random variable and $Y$ be another random variable correlated to $Y$. Suppose now we wish to estimate $X$ via observations about $Y$. Let $\yhwidehat{X}\left(Y\right)$ be an estimator obtained this way about $X$.

If $H\left(X \mid Y\right) = 0$, one can expect that a perfect estimation is possible. On the other hand, if $H\left(X \mid Y\right) = \log_2\abs{\mathcal{X}}$, the the estimation is bad. Notice that this happens if and only if $X$ is uniform and independent of $Y$. In reality, we may wish $H\left(X \mid Y\right)$ to be small, where the error of estimation can be small.
\begin{thmbox}{Fano's Inequality}{Fano}
    For any estimator $\yhwidehat{X}$ obtained from $Y$, let $p_e \coloneqq \Pr\left(\yhwidehat{X} \neq X\right)$ be the probability of error, then 
    \begin{equation*}
        H_b\left(p_e\right) + p_e\log_2\abs{\mathcal{X}} \geq H\left(X \mid \yhwidehat{X}\right) \geq H\left(X \mid Y\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        Define $E \coloneqq \mathbf{1}\left\{\yhwidehat{X} \neq X\right\}$ to be the error random variable, then $p_e = \Pr\left(E = 1\right)$. Consider 
        \begin{equation*}
            H\left(E \mid \yhwidehat{X}\right) + H\left(X \mid E, \yhwidehat{X}\right) = H\left(E, X \mid \yhwidehat{X}\right) = H\left(X \mid \yhwidehat{X}\right) + H\left(E \mid X, \yhwidehat{X}\right).
        \end{equation*}
        It is clear that $H\left(E \mid X, \yhwidehat{X}\right) = 0$. By Corollary \ref{cor:condDoesNotIncrEntropy}, since $E$ is a Bernoulli random variable, we have 
        \begin{equation*}
            H\left(E \mid \yhwidehat{X}\right) \leq H\left(E\right) = H_b\left(p_e\right).
        \end{equation*}
        Note that 
        \begin{equation*}
            H\left(X \mid E, \yhwidehat{X}\right) = \Pr\left(E = 1\right)H\left(X \mid E = 1, \yhwidehat{X}\right) + \Pr\left(E = 0\right)H\left(X \mid E = 0, \yhwidehat{X}\right).
        \end{equation*}
        Clearly, $H\left(X \mid E = 0, \yhwidehat{X}\right) = 0$ and $H\left(X \mid E = 1, \yhwidehat{X}\right) \leq \log_2\abs{\mathcal{X}}$, so 
        \begin{equation*}
            H\left(X \mid E, \yhwidehat{X}\right) \leq p_e\log_2\abs{\mathcal{X}}.
        \end{equation*}
        Therefore, 
        \begin{equation*}
            H\left(X \mid \yhwidehat{X}\right) = H\left(E \mid \yhwidehat{X}\right) + H\left(X \mid E, \yhwidehat{X}\right) \leq H_b\left(p_e\right) + p_e\log_2\abs{\mathcal{X}}.
        \end{equation*}
        Note that $\yhwidehat{X}$ is a function of $Y$, so by Corollary \ref{pro:funcMarkovChain}, $X$-$Y$-$\yhwidehat{X}$ forms a Markov chain. By Theorem \ref{thm:DPI}, 
        \begin{align*}
            H\left(X\right) - H\left(X \mid Y\right) & = I\left(X ; Y\right) \\
            & \geq I\left(X ; \yhwidehat{X}\right) \\
            & = H\left(X\right) - H\left(X \mid \yhwidehat{X}\right),
        \end{align*}
        which reduces to $H\left(X \mid \yhwidehat{X}\right) \geq H\left(X \mid Y\right)$.
    \end{proof}
\end{thmbox}
With some algebraic manipulations, it can be obtained from Theorem \ref{thm:Fano} that 
\begin{equation*}
    p_e \geq \frac{H\left(X \mid Y\right) - 1}{\log_2\abs{\mathcal{X}}}.
\end{equation*}
In other words, this means that 
\begin{equation*}
    \inf \Pr\left(\yhwidehat{X}\left(Y\right) \neq X\right) \geq \frac{H\left(X \mid Y\right) - 1}{\log_2\abs{\mathcal{X}}}.
\end{equation*}
This is one of the few results which offer a \textbf{lower bound estimate} for probabilities. In particular, this result shows that in a non-trivial scenario ($\abs{\mathcal{X}} > 1$), perfect estimator is attainable only when $H\left(X \mid Y\right) \leq 1$. 

\chapter{Data Compression}
\section{Asymptotic Equipartition Property}
In this section, we try to investigate some bounding conditions on the rate of data compression. We focus on a special type of sources called the \textit{discrete memoryless source} (DMS), which can be viewed as a sequence of \textbf{independent and identically distributed} discrete random variables such that the distribution of $X_n$ is independent of the distributions of all~$X_i$'s with $i < n$. 
\begin{dfnbox}{Discrete Memoryless Source}{DMS}
    A {\color{red} \textbf{discrete memoryless source}} is a sequence of identically distributed random variables $X_1^n$ such that $X_i$ is independent of all $X_j$ whenever $j < i$.
\end{dfnbox}
One may check that a DMS is mutually independent.

We introduce the following notion:
\begin{dfnbox}{$\epsilon$-Weakly Typical Set}{weaklyTypical}
    Let $X$ be a discrete memoryless random variable with distribution $p_X$. An {\color{red} \textbf{$\epsilon$-Weakly Typical Set}} of $X$ is defined as 
    \begin{equation*}
        A_{\epsilon}^{\left(n\right)}\left(X\right) \coloneqq \left\{\mathbfit{x} \in \mathcal{X}^n \colon \abs{\frac{1}{n}\log_2\frac{1}{\prod_{i = 1}^np_X\left(x_i\right)} - H\left(X\right)} \leq \epsilon\right\}
    \end{equation*}
    where $n \in \N^+$.
\end{dfnbox}
While this definition could seem obscure, we can unpack the inequality into 
\begin{equation*}
    2^{-n\bigl(H\left(X\right) + \epsilon\bigr)} \leq \prod_{i = 1}^np_X\left(x_i\right) \leq 2^{-n\bigl(H\left(X\right) - \epsilon\bigr)}.
\end{equation*}
As $\epsilon \to 0$, we see that $A_{\epsilon}^{\left(n\right)}\left(X\right)$ defines a set of sequences which have an asymptotically equal probability of occurring as the output from a discrete memoryless source.
\begin{thmbox}{Asymptotic Equipartition Property}{AEP}
    If $X_1^n$ is a discrete memoryless source, then for all $\epsilon > 0$, there exists some $N \in \N^+$ such that for all $n > N$, 
    \begin{equation*}
        \Pr\left(X_1^n \in A_{\epsilon}^{\left(n\right)}\left(X\right)\right) \geq 1 - \epsilon,
    \end{equation*}
    where 
    \begin{equation*}
        \left(1 - \epsilon\right)2^{n\bigl(H\left(X\right) - \epsilon\bigr)} \leq \abs{A_{\epsilon}^{\left(n\right)}\left(X\right)} \leq 2^{n\bigl(H\left(X\right) + \epsilon\bigr)}.
    \end{equation*}
    \tcblower
    \begin{proof}
        Consider 
        \begin{align*}
            \Pr\left(X_1^n \notin A_{\epsilon}^{\left(n\right)}\left(X\right)\right) & = \Pr\left(\abs{\frac{1}{n}\log_2\frac{1}{\prod_{i = 1}^np_X\left(Z_i\right)} - H\left(X\right)} > \epsilon\right) \\
            & = \Pr\left(\abs{\frac{1}{n}\sum_{i = 1}^{n}\frac{1}{p_X\left(X_i\right)} - H\left(X\right)} > \epsilon\right) \\
            & = \Pr\left(\abs{\frac{1}{n}\sum_{i = 1}^{n}\frac{1}{p_X\left(X_i\right)} - \mathbb{E}\left[\frac{1}{p_X\left(X_1\right)}\right]} > \epsilon\right).
        \end{align*}
        Since the $X_i$'s are identical and independent, by Theorem \ref{thm:weakLawLargeNum}, for all $\epsilon > 0$, there exists some $N \in \N^+$ such that for all $n > N$, we have
        \begin{equation*}
            \Pr\left(X_1^n \notin A_{\epsilon}^{\left(n\right)}\left(X\right)\right) < \epsilon,
        \end{equation*}
        which implies that 
        \begin{equation*}
            \Pr\left(X_1^n \in A_{\epsilon}^{\left(n\right)}\left(X\right)\right) = 1 - \Pr\left(X_1^n \notin A_{\epsilon}^{\left(n\right)}\left(X\right)\right) \geq 1 - \epsilon.
        \end{equation*}
        Notice that for each $\mathbfit{x} \in A_{\epsilon}^{\left(n\right)}\left(X\right)$, we have 
        \begin{equation*}
            2^{-n\bigl(H\left(X\right) + \epsilon\bigr)} \leq \prod_{i = 1}^np_X\left(x_i\right) \leq 2^{-n\bigl(H\left(X\right) - \epsilon\bigr)}.
        \end{equation*}
        Since $A_{\epsilon}^{\left(n\right)}\left(X\right) \subseteq \mathcal{X}^n$, we have 
        \begin{align*}
            1 & \geq \sum_{\mathbfit{x} \in A_{\epsilon}^{\left(n\right)}\left(X\right)}\prod_{i = 1}^{n}p_X\left(x_i\right) \\
            & \geq \sum_{\mathbfit{x} \in A_{\epsilon}^{\left(n\right)}\left(X\right)}2^{-n\bigl(H\left(X\right) + \epsilon\bigr)} \\
            & = 2^{-n\bigl(H\left(X\right) + \epsilon\bigr)}\abs{A_{\epsilon}^{\left(n\right)}\left(X\right)}.
        \end{align*}
        Therefore, 
        \begin{equation*}
            \abs{A_{\epsilon}^{\left(n\right)}\left(X\right)} \leq 2^{n\bigl(H\left(X\right) + \epsilon\bigr)}.
        \end{equation*}
        For all $n > N$, consider 
        \begin{align*}
            1 - \epsilon & \leq \Pr\left(X_1^n \in A_{\epsilon}^{\left(n\right)}\left(X\right)\right) \\
            & = \sum_{\mathbfit{x} \in A_{\epsilon}^{\left(n\right)}\left(X\right)}\prod_{i = 1}^{n}p_X\left(x_i\right) \\
            & \leq \sum_{\mathbfit{x} \in A_{\epsilon}^{\left(n\right)}\left(X\right)}2^{-n\bigl(H\left(X\right) - \epsilon\bigr)} \\
            & = 2^{-n\bigl(H\left(X\right) - \epsilon\bigr)}\abs{A_{\epsilon}^{\left(n\right)}\left(X\right)}.
        \end{align*}
        Therefore, 
        \begin{equation*}
            \left(1 - \epsilon\right)2^{n\bigl(H\left(X\right) - \epsilon\bigr)} \leq \abs{A_{\epsilon}^{\left(n\right)}\left(X\right)}.
        \end{equation*}
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        The upper bound for $\abs{A_{\epsilon}^{\left(n\right)}\left(X\right)}$ holds for all $n \in \N^+$, where the lower bound only holds when $n$ is sufficiently large.
    \end{remark}
\end{notebox}
One important implication of the AEP is as follows: in computers, we code information into binary strings. Now, suppose $X$ is a binary random variable, then clearly $\mathcal{X}^n = \left\{0, 1\right\}^n$. By Theorem \ref{thm:AEP}, we know that when an input binary string $X_1^n$ is very long, there exists a very small subset $A_{\epsilon}^{\left(n\right)}\left(X\right) \subseteq \mathcal{X}^n$ of size approximately $2^{nH\left(X\right)}$. This is because if $X$ is not uniform, then $0 \leq H\left(X\right) < 1$ and so $\abs{A_{\epsilon}^{\left(n\right)}\left(X\right)}$ is much smaller than $\abs{\mathcal{X}^n} = 2^n$. However, the probability that $X_1^n$ falls in $A_{\epsilon}^{\left(n\right)}\left(X\right)$ is very high according to Theorem \ref{thm:AEP}.
\section{Fixed-to-Fixed-Length Data Compression}
Consider a sequence of random variables $X_1^n$, we wish to construct an \textit{encoder} 
\begin{equation*}
    \mathrm{Enc} \colon \mathcal{X}^n \to \left\{0, 1\right\}^{nR}
\end{equation*}
for some $R \in \R$ to map any possible input to a binary string. Moreover, we also wish to construct a \textit{decoder}
\begin{equation*}
    \mathrm{Dec} \colon \left\{0, 1\right\}^{nR} \to \yhwidehat{\mathcal{X}}_1^n
\end{equation*}
to restore the original input as a sequence of estimators. This process is known as \textit{compression}. This can be formalised as the following:
\begin{dfnbox}{Fixed-To-Fixed-Length Source Code}{ftf}
    An {\color{red} \textbf{$\left(n, 2^{nR}\right)$-fixed-to-fixed-length source code}} for a random variable $X$ is an {\color{red} \textbf{encoder}}
    \begin{equation*}
        f \colon \mathcal{X}^n \to \left\{1, 2, \cdots, 2^{nR}\right\}
    \end{equation*}
    plus a {\color{red} \textbf{decoder}}
    \begin{equation*}
        \varphi \colon \left\{1, 2, \cdots, 2^{nR}\right\} \to \mathcal{X}^n,
    \end{equation*}
    where $n$ is known as the {\color{red} \textbf{block length}} and $R$, {\color{red} \textbf{code rate}}. We define $M \coloneqq f\left(X_1^n\right)$ as the {\color{red} \textbf{compression index}} and $\yhwidehat{X}_1^n \coloneqq \varphi\left(M\right)$ as the {\color{red} \textbf{reconstructed source}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Technically, $nR$ might not be an integer, but for the sake of simplicity, we take $2^{nR}$ to be the same as $\left\lceil 2^{nR}\right\rceil$.
    \end{remark}
\end{notebox}
Ideally, we wish to achieve perfect compression with $\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) = 0$, but that means that we need $R = \log_2\abs{X}$, which is as if we are not compressing any data contained in $X$. Therefore, we can relax this constraint a bit and now aim at 
\begin{equation*}
    \lim_{n \to \infty}\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) = 0.
\end{equation*} 
\begin{dfnbox}{Achievable Rate}{achievable}
    Let $X$ be a random variable. A code rate $R \geq 0$ is said to be {\color{red} \textbf{achievable}} if there exists a sequence of $\left(n, 2^{nR}\right)$-codes such that 
    \begin{equation*}
        \lim_{n \to \infty}\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) = 0.
    \end{equation*} 
\end{dfnbox}
In other words, as $n \to \infty$, we have $\yhwidehat{X}_1^n = \varphi\bigl(f\left(X_1^n\right)\bigr) \approx X_1^n$. This is known as \textit{vanishing probability of error}. The question is: how large should we take $R$ to be in order to achieve this?

First, observe that if $R$ is achievable, then any $R' > R$ is also achievable. Therefore, it suffices to focus on the minimal achievable rate.
\begin{dfnbox}{Optimal Source Coding Rate}{optRate}
    The {\color{red} \textbf{optimal source coding rate}} for a discrete memoryless source $X$ is defined as 
    \begin{equation*}
        R^*\left(X\right) \coloneqq \inf \left\{R \geq 0 \colon R \textrm{ is achievable}\right\}.
    \end{equation*}
\end{dfnbox}
The following theorem states the main result for fixed-to-fixed-length data compression.
\begin{thmbox}{Fixed-To-Fixed-Length Data Compression}{ftfDataCompression}
    For any discrete memoryless source $X$, we have $R^*\left(X\right) = H\left(X\right)$.
    \tcblower
    \begin{proof}
        By Theorem \ref{thm:AEP}, $A_{\epsilon}^{\left(n\right)}\left(X\right)$ is finite for all $n \in \N^+$ and all $\epsilon > 0$, so we can always fix a bijection $m_n \colon A_{\epsilon}^{\left(n\right)}\left(X\right) \to \left\{1, 2, \cdots, \abs{A_{\epsilon}^{\left(n\right)}\left(X\right)}\right\}$. For any $n \in \N^+$ and any~$\epsilon > 0$, define an encoder $f_n \colon \mathcal{X}^n \to \left\{1, 2, \cdots, 2^{nR}\right\}$ by 
        \begin{equation*}
            f_n\left(X_1^n\right) = \begin{cases}
                m_n\left(X_1^n\right) & \quad\textrm{if } X_1^n \in A_{\epsilon}^{\left(n\right)}\left(X\right) \\
                1 & \quad\textrm{otherwise}
            \end{cases}
        \end{equation*}
        and a decoder $\varphi_n \colon \left\{1, 2, \cdots, 2^{nR}\right\} \to \mathcal{X}^n$ by $\varphi_n\left(M\right) = m_n^{-1}\left(M\right)$. Notice that by Theorem \ref{thm:AEP}, $\abs{A_{\epsilon}^{\left(n\right)}\left(X\right)} \leq 2^{n\bigl(H\left(X\right) + \epsilon\bigr)}$, so
        \begin{equation*}
            R \leq H\left(X\right) + \epsilon
        \end{equation*}
        for all $\epsilon > 0$. Observe that for any $X_1^n$, there could be an error if $X_1^n \notin A_{\epsilon}^{\left(n\right)}\left(X\right)$, so 
        \begin{equation*}
            \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) \leq \Pr\left(X_1^n \notin A_{\epsilon}^{\left(n\right)}\left(X\right)\right).
        \end{equation*}
        By Theorem \ref{thm:AEP}, $\lim_{n \to \infty}\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) = 0$ as desired. This means that $R$ is achievable and so $R^*\left(X\right) \leq H\left(X\right) + \epsilon$ for all $\epsilon > 0$.
        \\\\
        Let $M = f_n\left(X_1^n\right)$. Since $\yhwidehat{\mathcal{X}}_1^n$ is an estimator for $\mathcal{X}_1^n$ obtained from $M$, by Theorem \ref{thm:Fano} we have 
        \begin{equation*}
            \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) \geq \frac{H\left(X_1^n \mid M\right) - 1}{\log_2\left(\abs{\mathcal{X}}^n\right)}.
        \end{equation*}
        Therefore,
        \begin{align*}
            H\left(X_1^n \mid M\right) & \leq n\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right)\log_2\abs{\mathcal{X}} + 1.
        \end{align*}
        Notice that 
        \begin{align*}
            nR & = \log_2\abs{M} \\
            & \geq H\left(M\right) \\
            & = H\left(M\right) - H\left(M \mid X_1^n\right) + H\left(M \mid X_1^n\right) \\
            & = I\left(M ; X_1^n\right) \\
            & = nH\left(X\right) - H\left(X_1^n \mid M\right).
        \end{align*}
        By Corollary \ref{cor:subadd}, $H\left(X_1^n\right) \leq nH\left(X\right)$, so we have 
        \begin{equation*}
            nR \geq nH\left(X\right) - n\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right)\log_2\abs{\mathcal{X}} - 1.
        \end{equation*}
        Therefore, 
        \begin{equation*}
            R \geq H\left(X\right) - \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right)\log_2\abs{\mathcal{X}} - \frac{1}{n}.
        \end{equation*}
        Since 
        \begin{equation*}
            \lim_{n \to \infty}\left(\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right)\log_2\abs{\mathcal{X}} + \frac{1}{n}\right) = 0,
        \end{equation*}
        we have $R \geq H\left(X\right)$. Therefore, $R^*\left(X\right) \geq H\left(X\right)$. Combining the two directions we have $R^*\left(X\right) = H\left(X\right)$.
    \end{proof}
\end{thmbox}
We can attempt to relax the condition on the error probability to 
\begin{equation*}
    \limsup_{n \to \infty}\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) \leq \epsilon
\end{equation*}
for some $\epsilon \in \left[0, 1\right)$ instead. This kind of behaviour is called $\epsilon$-achievability. If we define the optimal $\epsilon$-achievable rate to be 
\begin{equation*}
    R_{\epsilon}^*\left(X\right) \coloneqq \left\{R \geq 0 \colon R \textrm{ is } \epsilon\textrm{-achievable}\right\}.
\end{equation*}
It is clear that $R^*\left(X\right) = R^*_0\left(X\right) \geq R^*_{\epsilon}\left(X\right)$ for all $\epsilon \in \left[0, 1\right)$. Surprisingly, this does not improve $R^*\left(X\right)$. We first consider a lemma which helps us prove this:
\begin{lembox}{Information Spectrum Technique}{infoSpec}
    For any $\left(n, 2^{nR}\right)$-code, 
    \begin{equation*}
        \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) \geq \inf_{\gamma > 0}\left\{\Pr\left(\frac{1}{n}\log_2\frac{1}{\prod_{i = 1}^np_X\left(x_i\right)} \geq R + \gamma\right) - \e^{-n\gamma}\right\}.
    \end{equation*}
    \tcblower
    \begin{proof}
        For every $\gamma > 0$ and $n \in \N^+$, define 
        \begin{align*}
            T_n & \coloneqq \left\{\mathbfit{x} \in \mathcal{X}^n \colon \frac{1}{n}\log_2\frac{1}{\prod_{i = 1}^np_X\left(x_i\right)} \geq R + \gamma\right\}, \\
            S_n & \coloneqq \left\{\mathbfit{x} \in \mathcal{X}^n \colon \phi_n\bigl(f_n\left(\mathbfit{x}\right)\bigr) = \mathbfit{x}\right\}.
        \end{align*}
        Consider 
        \begin{align*}
            \Pr\left(X_1^n \in T_n\right) & = \Pr\left(X_1^n \in T_n \cap S_n\right) + \Pr\left(X_1^n \in T_n \cap S_n^c\right) \\
            & \leq \Pr\left(X_1^n \in T_n \cap S_n\right) + \Pr\left(X_1^n \in S_n^c\right) \\
            & = \Pr\left(X_1^n \in T_n \cap S_n\right) + \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right).
        \end{align*}
        For each $\mathbfit{x} \in T_n$, we have 
        \begin{equation*}
            \prod_{i = 1}^{n}p_X\left(x_i\right) \leq 2^{-n\left(R + \gamma\right)},
        \end{equation*}
        and so 
        \begin{align*}
            \Pr\left(X_1^n \in T_n \cap S_n\right) & = \sum_{\mathbfit{x} \in T_n \cap S_n}\prod_{i = 1}^{n}p_X\left(x_i\right) \\
            & \leq \sum_{\mathbfit{x} \in T_n \cap S_n}2^{-n\left(R + \gamma\right)} \\
            & \leq 2^{-n\left(R + \gamma\right)}\abs{S_n} \\
            & = 2^{-n\gamma}.
        \end{align*}
        Therefore, $\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) \geq \Pr\left(X_1^n \in T_n\right) - 2^{-n\gamma} \geq \Pr\left(X_1^n \in T_n\right) - \e^{-n\gamma}$, and so 
        \begin{equation*}
            \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) \geq \inf_{\gamma > 0}\left\{\Pr\left(\frac{1}{n}\log_2\frac{1}{\prod_{i = 1}^np_X\left(x_i\right)} \geq R + \gamma\right) - \e^{-n\gamma}\right\}.
        \end{equation*}
    \end{proof}
\end{lembox}
Using the above lemma, we can prove that an optimal achievable rate can never be smaller than the entropy of the source.
\begin{thmbox}{Strong Converse of Optimal Rate}{strongConverse}
    If $R < H\left(X\right)$, then $\lim_{n \to \infty}\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) = 1$.
    \tcblower
    \begin{proof}
        For any $\eta > 0$, let $R = H\left(X\right) - \eta$, then by Lemma \ref{lem:infoSpec}, 
        \begin{align*}
            \Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) & \geq \Pr\left(\frac{1}{n}\log_2\frac{1}{\prod_{i = 1}^np_X\left(x_i\right)} \geq R + \frac{\eta}{2}\right) - \e^{-n\frac{\eta}{2}} \\
            & = \Pr\left(\frac{1}{n}\sum_{i = 1}^{n}\log_2\frac{1}{p_X\left(x_i\right)} \geq H\left(X\right) - \frac{\eta}{2}\right) - \e^{-n\frac{\eta}{2}} \\
            & \geq \Pr\left(\abs{H\left(X\right) - \frac{1}{n}\sum_{i = 1}^{n}\log_2\frac{1}{p_X\left(x_i\right)}} \leq \frac{\eta}{2}\right) - \e^{-n\frac{\eta}{2}}.
        \end{align*}
        By Theorem \ref{thm:weakLawLargeNum}, 
        \begin{align*}
            \lim_{n \to \infty}\Pr\left(\abs{H\left(X\right) - \frac{1}{n}\sum_{i = 1}^{n}\log_2\frac{1}{p_X\left(x_i\right)}} \leq \frac{\eta}{2}\right) = 1.
        \end{align*}
        Therefore, 
        \begin{equation*}
            \lim_{n \to \infty}\Pr\left(X_1^n \neq \yhwidehat{\mathcal{X}}_1^n\right) = 1.
        \end{equation*}
    \end{proof}
\end{thmbox}
\section{Stochastic Processes}
In the previous section, we have discussed some results in fixed-to-fixed-length source coding for discrete memoryless sources. However, there are times where the source is not memoryless. A general model we adopt for such cases is \textit{stochastic processes}.
\begin{dfnbox}{Discrete Time Stochastic Process}{stochastic}
    A {\color{red} \textbf{discrete time stochastic process}} is a sequence of random variables $\left\{X_i\right\}_{i \in \N^+}$ such that 
    \begin{equation*}
        \sum_{x_n \in \mathcal{X}_n}p_{X_1^n}\left(x_1, x_2, \cdots, x_n\right) = p_{X_1^{n - 1}}\left(x_1, x_2, \cdots, x_{n - 1}\right)
    \end{equation*}
    for all $n \in \N^+$.
\end{dfnbox}
In a sense, a stochastic process is fully characterised by all of its joint PMFs such that the PMFs stay consistent under marginalisation. In our case, we focus on some stochastic processes satisfying certain special properties.
\begin{dfnbox}{Stationary Stochastic Process}{stationary}
    A stochastic process $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ is {\color{red} \textbf{stationary}} if for all $n \in \N^+$ and all $\ell \in \N$, 
    \begin{equation*}
        p_{X_1^n}\left(x_1, x_2, \cdots, x_n\right) = p_{X_{1 + \ell}^{n + \ell}}\left(x_1, x_2, \cdots, x_n\right)
    \end{equation*}
    for all $x_1, x_2, \cdots, x_n \in \mathcal{X}$.
\end{dfnbox}
Here, $\ell$ is known as the \textit{shift}. A stochastic can become a Markov chain if every random variable $X_i$ in the sequence only depends on the immediate previous random variable.
\begin{dfnbox}{Markov Chain}{MarkovChain}
    A stochastic process $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ is a {\color{red} \textbf{Markov chain}} if 
    \begin{equation*}
        p_{X_{n + 1} \mid X_1^n}\left(x_{n + 1} \mid x_1, x_2, \cdots, x_n\right) = p_{X_{n + 1} \mid X_n}\left(x_{n + 1} \mid x_n\right)
    \end{equation*}
    for all $n \in \N^+$.
\end{dfnbox}
It can be easily verified that for such a Markov chain,
\begin{equation*}
    p_{X_1^n}\left(x_1, x_2, \cdots, x_n\right) = p_{X_1}\left(x_1\right)\prod_{i = 1}^{n - 1}p_{X_{i + 1} \mid X_i}\left(x_{i + 1} \mid x_i\right)
\end{equation*}
for all $n \in \N^+$. Now, the next question is: does the conditional probability 
\begin{equation*}
    \Pr\left(X_{n + 1} = x_{n + 1} \mid X_n = x_n\right)
\end{equation*} 
depend on where we are in the chain? In general, the answer is ``yes''.
\begin{dfnbox}{Time-Invariant Markov Chain}{timeInvariant}
    A markov chain $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ is {\color{red} \textbf{time-invariant}} if 
    \begin{equation*}
        p_{X_{n + 1} \mid X_n}\left(x_{n + 1} \mid x_n\right) = p_{X_2 \mid X_1}\left(x_2 \mid x_1\right)
    \end{equation*}
    for all $n \in \N^+$ and all $x_{n + 1}, x_n \in \mathcal{X}$.
\end{dfnbox}
For time-invariant discrete-time markov chains, we can capture the transition conditional probabilities $\Pr\left(X_{n + 1} = j \mid X_n = i\right)$ into a matrix.
\begin{dfnbox}{Transition Probability Matrix}{TPM}
    Let $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ be a time-invariant discrete-time markov chain. The {\color{red} \textbf{transition probability matrix}} (TPM) $\mathbfit{P}$ of $X$ is defined such that 
    \begin{equation*}
        P_{ij} \coloneqq \Pr\left(X_{n + 1} = j \mid X_{n} = i\right)
    \end{equation*}
    for all $n \in \N^+$ and all $i, j \in \mathcal{X}$.
\end{dfnbox}
Suppose that $\mathcal{X}$ is finite. Then clearly, with respect to the TPM, we can write 
\begin{align*}
    \Pr\left(X_{n + 1} = j\right) & = \sum_{i \in \mathcal{X}}\Pr\left(X_{n + 1} = j \mid X_n = i\right)\Pr\left(X_n = i\right) \\
    & = \sum_{i \in \mathcal{X}}P_{ij}\Pr\left(X_n = i\right)
\end{align*}
for all $n \in \N^+$ and all $j \in \mathcal{X}$. Now, by writing everything in terms of matrix multiplication, we have 
\begin{equation*}
    \begin{bmatrix}
        p_{X_{n + 1}}\left(x_1\right) & p_{X_{n + 1}}\left(x_2\right) & \cdots & p_{X_{n + 1}}\left(x_{\abs{\mathcal{X}}}\right)
    \end{bmatrix} = \begin{bmatrix}
        p_{X_{n}}\left(x_1\right) & p_{X_{n}}\left(x_2\right) & \cdots & p_{X_{n}}\left(x_{\abs{\mathcal{X}}}\right)
    \end{bmatrix}\mathbfit{P}.
\end{equation*}
Let us define 
\begin{equation*}
    \pi_n \coloneqq \begin{bmatrix}
        p_{X_{n}}\left(x_1\right) & p_{X_{n}}\left(x_2\right) & \cdots & p_{X_{n}}\left(x_{\abs{\mathcal{X}}}\right)
    \end{bmatrix},
\end{equation*}
then $\pi_n$ is clearly a distribution for $X_n$. Now, this equation seems suspicious enough that~$\pi_n$ might possess some sort of limiting behaviour, which we define as follows:
\begin{dfnbox}{Stationary Distribution}{stationaryDist}
    Let $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ be a Markov chain with transition probability matrix $\mathbfit{P}$. A distribution $\pi$ on $\mathcal{X}$ is said to be a {\color{red} \textbf{stationary distribution}} if $\pi = \pi\mathbfit{P}$.
\end{dfnbox}
A fact is that not all Markov chains have a well-defined stationary distribution. The following two important properties for Markov chains help guarantee the existence of such a distribution.
\begin{enumerate}
    \item \textit{Irreducible}: a Markov chain is irreducible if it is possible to go from any state to any other state, i.e.,
    \begin{equation*}
        \Pr\left(X_m = a \mid X_n = b\right) > 0, \qquad \Pr\left(X_m = b \mid X_n = a\right) > 0
    \end{equation*}
    for all $a \neq b \in \mathcal{X}$ and $m > n$.
    \item \textit{Aperiodic}: a Markov chain is aperiodic if for any state $i \in \mathcal{X}$, either it is impossible to go back to $i$ from $i$, or the greatest common divisor for the lengths of all paths going from $i$ to itself is $1$. Formally, this means that 
    \begin{equation*}
        \gcd\left\{n \in \N^+ \colon \Pr\left(X_n = i \mid X_0 = i\right) > 0\right\} = 1.
    \end{equation*}
\end{enumerate}
\begin{thmbox}{Characterisation of Irreducible and Aperiodic Markov Chains}{irreducibleAndAperiodic}
    An irreducible and aperiodic Markov chain has a unique stationary distribution.
\end{thmbox}
Suppose the stationary distribution $\pi$ exists, then it is clear that we can find $\pi$ by solving the system 
\begin{equation*}
    \pi\mathbfit{P - I} = \mathbf{0}.
\end{equation*}
Of course, one might also attempt to diagonalise $\mathbfit{P}$ to find $\lim_{n \to \infty}\mathbfit{P}^n$ because $\pi_n = \pi_0\mathbfit{P}^n$. Furthermore, suppose 
\begin{equation*}
    \pi = \begin{bmatrix}
        p_1, p_2, \cdots, p_{\abs{\mathcal{X}}}
    \end{bmatrix}.
\end{equation*}
Notice that
\begin{equation*}
    p_j = \sum_{i \in \mathcal{X}}P_{ij}p_i
\end{equation*}
for all $j \in \mathcal{X}$. It is not difficult to see that $P_{ij} = p_j$ for all $i \in \mathcal{X}$, and so 
\begin{equation*}
    \lim_{n \to \infty}\mathbfit{P}^n = \begin{bmatrix}
        \pi \\
        \pi \\
        \vdots \\
        \pi
    \end{bmatrix}.
\end{equation*}
\begin{dfnbox}{Entropy Rate}{entropyRate}
    Let $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ be a stochastic process. The {\color{red} \textbf{entropy rate}} of $X$ is defined as 
    \begin{equation*}
        H\left(X\right) \coloneqq \lim_{n \to \infty}\frac{1}{n}H\left(X_1, X_2, \cdots, X_n\right)
    \end{equation*}
    if the limit exists.
\end{dfnbox}
There is an alternative way to define the entropy rate:
\begin{dfnbox}{Prime Definition for Entropy Rate}{primeEntropyRate}
    Let $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ be a stochastic process. The {\color{red} \textbf{prime entropy rate}} of $X$ is defined as 
    \begin{equation*}
        H'\left(X\right) \coloneqq \lim_{n \to \infty}H\left(X_{n} \mid X_1^{n - 1}\right)
    \end{equation*}
    if the limit exists.
\end{dfnbox}
In general, the two types of entropy rate may not be equivalent. However, we can check that they are the same for stationary processes.
\begin{thmbox}{Equivalence of Entropy Rates for Stationary Sources}{equivEntropyRate}
    If $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ is a stationary stochastic process, then $H\left(X\right)$ and $H'\left(X\right)$ both exist and~$H\left(X\right) = H'\left(X\right)$.
    \tcblower
    \begin{proof}
        Since $X$ is stationary, 
        \begin{align*}
            H\left(X_n \mid X_1^{n - 1}\right) & \leq H\left(X_n \mid X_2^{n - 1}\right) \\
            & = H\left(X_{n - 1} \mid X_1^{n - 2}\right).
        \end{align*}
        Note that $H\left(X_n \mid X_1^{n - 1}\right) \geq 0$ for all $n \in \N^+$, so by monotone convergence theorem, 
        \begin{equation*}
            H'\left(X\right) = \lim_{n \to \infty}H\left(X_n \mid X_1^{n - 1}\right)
        \end{equation*}
        exists. Notice that 
        \begin{equation*}
            \frac{1}{n}H\left(X_1, X_2, \cdots, X_n\right) = \frac{1}{n}\sum_{i = 1}^{n}H\left(X_n \mid X_1^{n - 1}\right),
        \end{equation*}
        so clearly 
        \begin{equation*}
            H\left(X\right) = \lim_{n \to \infty}\frac{1}{n}H\left(X_1, X_2, \cdots, X_n\right) = \lim_{n \to \infty}H\left(X_n \mid X_1^{n - 1}\right) = H'\left(X\right).
        \end{equation*}
    \end{proof}
\end{thmbox}
We now state a theorem without proof which will generalise Theorem \ref{thm:AEP} to stationary stochastic processes.
\begin{thmbox}{Shannon-McMillian-Breiman Theorem}{SMB}
    For a stationary ergodic process $X$, 
    \begin{equation*}
        \lim_{n \to \infty}-\frac{1}{n}\log_2p\left(X_1, X_2, \cdots, X_n\right) = H\left(X\right).
    \end{equation*}
\end{thmbox}
Now, we apply this theorem to Markov chains. We first state the following fact:
\begin{probox}{Entropy Rate of Stationary Markov Chains}{stationaryMarkovEntropy}
    For any time-invariant stationary Markov chain $X$ with transition probability matrix $\mathbfit{P}$ and stationary distribution $\pi$, the entropy rate is given by 
    \begin{align*}
        H\left(X\right) & = H'\left(X\right) = H\left(X_2 \mid X_1\right)
    \end{align*}
    where $X_1 \sim \pi$ and $X_2 \mid X_1$ follows the distribution induced by $\mathbfit{P}$.
    \tcblower
    \begin{proof}
        By Theorem \ref{thm:equivEntropyRate}, since $X$ is a Markov chain,
        \begin{align*}
            H\left(X\right) & = H'\left(X\right) \\
            & = \lim_{n \to \infty}H\left(X_n \mid X_1^{n - 1}\right) \\
            & = \lim_{n \to \infty}H\left(X_n \mid X_{n - 1}\right) \\
            & = \lim_{n \to \infty}\sum_{i \in \mathcal{X}}p_{X_{n - 1}}\left(i\right)H\left(X_n \mid X_{n - 1} = i\right) \\
            & = \lim_{n \to \infty}\sum_{i \in \mathcal{X}}p_{X_{n - 1}}\left(i\right)\left(-\sum_{j \in \mathcal{X}}p_{X_n \mid X_{n - 1}}\left(j \mid i\right)\log_2p_{X_n \mid X_{n - 1}}\left(j \mid i\right)\right) \\
            & = \lim_{n \to \infty}\sum_{i \in \mathcal{X}}p_{X_{n - 1}}\left(i\right)\left(-\sum_{j \in \mathcal{X}}P_{ij}\log_2P_{ij}\right) \\
            & = \lim_{n \to \infty}\sum_{i \in \mathcal{X}}p_{X_{n - 1}}\left(i\right)H\left(X_2 \mid X_1 = i\right) \\
            & = \sum_{i \in \mathcal{X}}\pi\left(i\right)H\left(X_2 \mid X_1 = i\right) \\
            & = H\left(X_2 \mid X_1\right).
        \end{align*}
    \end{proof}
\end{probox}
Let us discuss a few applications of Markov chains.
\subsection{Application: List Shuffling}
Suppose we have a list containing $k$ distinct items and we wish to shuffle the list such that the end outcome is \textbf{completely random}, i.e., the shuffled list is independent of the starting state of the original list. Let us make a na\"{i}ve attempt first:
\subsubsection{One-at-a-Time Shuffling}
Let $X \coloneqq \left\{X_i\right\}_{i \in \N^+}$ be a stochastic process where $X_i$ represents the permutation of the list after the $i$-th shuffle. First, observe that $\abs{\mathcal{X}} = k!$. Given $X_n$, we obtain $X_{n + 1}$ by selecting any item from $X_n$ uniformly and insert the item to the first index of the list. Notice that this means $X_{n + 1}$ could take $k$ different permutations uniformly. Since the stochastic process is obviously a Markov chain, we have
\begin{equation*}
    H\left(X_{n + 1} \mid X_1^n\right) = H\left(X_{n + 1} \mid X_n\right) = \log_2 k
\end{equation*}
for all $n \in \N^+$. In the long-run, there is an equal probability of obtaining any permutation, so the stationary distribution is uniform. Therefore,
\begin{equation*}
    \lim_{n \to \infty}H\left(X_n\right) = \log_2 k!.
\end{equation*}
Furthermore, one may check that this Markov chain is time-invariant, irreducible and aperiodic (this is because we can always restore a permutation with $2$ or $3$ times of such shuffles). Now we ask the following question:
\begin{quote}
    How many steps of such shuffling process does it take such that the permutation of list obtained is independent of the initial list?
\end{quote}
Suppose that $X_n$ and $X_0$ are independent for some $n \in \N^+$, then necessarily $I\left(X_n ; X_0\right) = 0$. This implies that the sufficient and necessary condition for such independence is 
\begin{equation*}
    H\left(X_n \mid X_0\right) = H\left(X_n\right) - I\left(X_n ; X_0\right) = \log_2 k!.
\end{equation*}
Let $N_i$ be the index of the item selected during the $i$-th shuffle, then clearly $X_n$ is completely determined by $X_0$ and $N_1^{n - 1}$. Since the $N_i$'s are independent and identically distributed, by Corollary \ref{cor:subadd} we have
\begin{equation*}
    H\left(X_0^n\right) = H\left(X_0, N_1^{n}\right) = H\left(X_0\right) + nH\left(N_1\right).
\end{equation*}
On the other hand, we also have 
\begin{align*}
    H\left(X_0^n\right) & = H\left(X_0\right) + H\left(X_1^n \mid X_0\right) \\
    & = H\left(X_0\right) + H\left(X_n \mid X_0\right) + H\left(X_1^{n - 1} \mid X_0, X_n\right).
\end{align*}
Therefore, 
\begin{align*}
    H\left(X_n \mid X_0\right) & = nH\left(N_1\right) - H\left(X_1^{n - 1} \mid X_0, X_n\right) \\
    & \leq nH\left(N_1\right) \\
    & = n\log_2 k.
\end{align*}
Therefore, we necessarily need $n\log_2 k \geq \log_2 k!$, so $n \geq \frac{\log_2 k!}{\log_2 k}$. One can in fact prove further that no matter how many shuffles we perform in this manner, we still have some dependence between $X_n$ and $X_0$, i.e., $H\left(X_n \mid X_0\right) < \log_2 k!$ for all $n \in \N^+$.
\subsubsection{Fisher-Yates Shuffle}
We have demonstrated that the na\"{i}ve way of shuffling is never truly random. Now let us try to improve our algorithm. For $i = 1, 2, \cdots, k$, let $M_i$ uniformly distributed over the index set $\left\{i, i + 1, \cdots, k\right\}$ represent the index of the item selected at random during the $i$-th shuffle such that the $M_i$-th item is then inserted to the first index of the list. Clearly, this shuffling will terminate if and only if we have performed $k$ rounds. Furthermore, notice that by labelling each item with their index in the initial list, after the $n$-th round, one can re-construct the sequence $\left\{M_i\right\}_{i = 1}^n$ by taking the labels of items from the $n$-th item backwards to the first item sequentially. This means that $\left\{M_i\right\}_{i = 1}^{n - 1}$, and thereafter $\left\{X_i\right\}_{i = 1}^{n - 1}$, is completely determined by $X_0$ and $X_n$. Therefore,
\begin{equation*}
    H\left(X_1^{n - 1} \mid X_0, X_n\right) = 0
\end{equation*}
for all $n \in \left\{1, 2, \cdots, k\right\}$. Note that since the $M_i$'s are independent and completely determine the $X_i$'s given $X_0$, by Corollary \ref{cor:subadd} we have
\begin{align*}
    H\left(X_0\right) + H\left(X_k \mid X_0\right) + H\left(X_1^{k - 1} \mid X_0, X_k\right) & = H\left(X_0^k\right) \\
    & = H\left(X_0, M_1^k\right) \\
    & = H\left(X_0\right) + \sum_{i = 1}^{k}H\left(M_i\right).
\end{align*}
Therefore, 
\begin{align*}
    H\left(X_k \mid X_0\right) & = \sum_{i = 1}^{k}H\left(M_i\right) - H\left(X_1^{n - 1} \mid X_0, X_n\right) \\
    & = \sum_{i = 1}^{k}\log_2\left(k - i + 1\right) \\
    & = \log_2 k!.
\end{align*}
Thus, we have showed that this new algorithm is indeed a truly random shuffling!
\subsection{Application: Random Walk}
Consider a simple weighted undirected graph $G$ with no loops. Let $w_{ij}$ denote the weight of the edge $ij \in E\left(G\right)$. Suppose a moving body is traversing the graph randomly and let $X_t$ denote its position at time $t$. For each $u \in V\left(G\right)$, define 
\begin{equation*}
    \Pr\left(X_{t + 1} = v \mid X_t = u\right) = \frac{w_{uv}}{\sum_{w \in N\left(u\right)}w_{wv}}.
\end{equation*}
Now we wish to investigate \textbf{how likely it is for the body to reach $v \in V\left(G\right)$ in the long-run}, i.e., given the stationary distribution $\pi$, what is $\pi\left(v\right)$ for each $v \in V\left(G\right)$?

First, we claim that 
\begin{equation*}
    \pi_i = \frac{\sum_{j \in N\left(i\right)}w_{ij}}{\sum_{ij \in E\left(G\right)}w_{ij}}
\end{equation*}
for all $i \in V\left(G\right)$. For simplicity, let us define 
\begin{equation*}
    \omega_i = \sum_{j \in N\left(i\right)}w_{ij}, \qquad \omega = \sum_{ij \in E\left(G\right)}w_{ij}
\end{equation*}
for all $i \in V\left(G\right)$. Let the transition probability matrix be $\mathbfit{P}$. For each $j \in V\left(G\right)$, notice that 
\begin{align*}
    \sum_{i \in V\left(G\right)}\pi_iP_{ij} & = \sum_{i \in V\left(G\right)}\frac{\omega_i}{\omega} \cdot \frac{w_{ij}}{\sum_{k \in N\left(i\right)}w_{ik}} \\
    & = \sum_{i \in V\left(G\right)}\frac{\omega_i}{\omega} \cdot \frac{w_{ij}}{\omega_i} \\
    & = \sum_{i \in V\left(G\right)}\frac{w_{ij}}{\omega} \\
    & = \frac{\sum_{i \in N\left(j\right)}w_{ij}}{\omega} \\
    & = \pi_j,
\end{align*}
which implies that $\pi = \pi\mathbfit{P}$. Therefore, $\pi$ as defined by us previously is indeed a stationary distribution. This pretty much coincides with our intuition, because the probability of the body reaching vertex $i$ in the long-run is given by the ratio between the total weight of the edges incident to $i$ and that of the entire graph.

Since the process is stationary, by Proposition \ref{pro:stationaryMarkovEntropy}, we can compute the entropy rate as 
\begin{align*}
    H\left(X\right) & = H\left(X_2 \mid X_1\right) \\
    & = -\sum_{i \in V\left(G\right)}\pi_i\sum_{j \in V\left(G\right)}P_{ij}\log_2P_{ij} \\
    & = -\sum_{i \in V\left(G\right)}\pi_i\sum_{j \in N\left(i\right)}P_{ij}\log_2P_{ij} \\
    & = -\sum_{i \in V\left(G\right)}\frac{\omega_i}{\omega}\sum_{j \in N\left(i\right)}\frac{w_{ij}}{\omega_i}\log_2\frac{w_{ij}}{\omega_i} \\
    & = -\sum_{i, j \in V\left(G\right)^2}\frac{w_{ij}}{\omega}\log_2\frac{w_{ij}}{\omega_i} \\
    & = -\left(\sum_{i, j \in V\left(G\right)^2}\frac{w_{ij}}{\omega}\log_2\frac{w_{ij}}{\omega} - \sum_{i, j \in V\left(G\right)^2}\frac{w_{ij}}{\omega}\log_2\frac{w_{i}}{\omega}\right) \\
    & = -\sum_{i, j \in V\left(G\right)^2}\frac{w_{ij}}{\omega}\log_2\frac{w_{ij}}{\omega} + \sum_{i \in V\left(G\right)}\frac{w_{i}}{\omega}\log_2\frac{w_{i}}{\omega},
\end{align*}
where $X_1 \sim \pi$. Let $\left(U, V\right)$ be a pair of vertices in $G$ such that $\Pr\bigl(\left(U, V\right) = \left(i, j\right)\bigr) = \frac{w_{ij}}{\omega}$, then 
\begin{equation*}
    H\left(X\right) = H\left(U, V\right) - H\left(X_1\right).
\end{equation*}
In the case where the graph $G$ is uniformly-weighted, one can check that the Markov chain becomes time-invariant with the stationary distribution given by 
\begin{equation*}
    \pi_i = \frac{d\left(i\right)}{e\left(G\right)}.
\end{equation*}
Clearly, $\left(U, V\right)$ is now uniformly distributed and so 
\begin{equation*}
    H\left(X\right) = \log_2{2e\left(G\right)} - H\left(\pi\right).
\end{equation*}
This means that during a random walk where the choice of the next vertex is uniformly distributed, the entropy rate of the walk is completely determined by the number of edges and the entropy rate of the stationary distribution.
\subsection{Application: Hidden Markov Model}
Let $\left\{X_i\right\}_{i \in \N^+}$ be a Markov chain and define $Y_i = \phi\left(X_i\right)$ for some function $\phi$. Note that in general, $\left\{Y_i\right\}_{i \in \N^+}$ may not still be a Markov chain. However, it can be proved that $Y$ is always a stationary stochastic process. By Theorem \ref{thm:equivEntropyRate}, the entropy rate of $Y$ is well-defined and given by 
\begin{equation*}
    H'\left(Y\right) = \lim_{n \to \infty}H\left(X_n \mid X_1^{n - 1}\right),
\end{equation*}
where $\left\{H\left(X_n \mid X_1^{n - 1}\right)\right\}_{i = 1}^{\infty}$ is a non-increasing sequence. Therefore, we have 
\begin{equation*}
    H'\left(Y\right) \leq H\left(X_n \mid X_1^{n - 1}\right)
\end{equation*}
for all $n \in \N^+$. However, if we were to approximate $H'\left(Y\right)$ with a tolerance of error of $\epsilon$, this alone is not sufficient. Notice that by Proposition \ref{pro:cor:condDoesNotIncrEntropy}, 
\begin{equation*}
    H\left(Y_n \mid Y_1^{n - 1}, X_1\right) \leq H\left(Y_n \mid Y_1^{n - 1}\right)
\end{equation*}
for all $n \in \N^+$, so 
\begin{equation*}
    H\left(Y_n \mid Y_1^{n - 1}, X_1\right) \leq \lim_{n \to \infty}H\left(Y_n \mid Y_1^{n - 1}\right) = H'\left(Y\right).
\end{equation*}
Therefore, we can apply some numerical methods such that 
\begin{equation*}
    \abs{H\left(Y_n \mid Y_1^{n - 1}, X_1\right) - H\left(Y_n \mid Y_1^{n - 1}\right)} < 2\epsilon
\end{equation*}
to ensure that we have a good approximation of $H'\left(Y\right)$.

Alternatively, we can also achieve this bounding condition by considering 
\begin{align*}
    I\left(Y_n ; X_1 \mid Y_1^{n - 1}\right) & = H\left(X_1 \mid Y_1^{n - 1}\right) - H\left(X_1 \mid Y_n, Y_1^{n - 1}\right) \\
    & \leq H\left(X_1 \mid Y_1^{n - 1}\right) \\
    & \leq H\left(X_1\right).
\end{align*}
Furthermore, we have 
\begin{equation*}
    I\left(X_1 ; Y_1^n\right) = H\left(X_1\right) - H\left(Y_1^n \mid X_1\right) \leq H\left(X_1\right)
\end{equation*}
for all $n \in \N^+$. This means that $\left\{I\left(X_1 ; Y_1^n\right)\right\}_{n = 1}^{\infty}$ is a non-decreasing sequence upper bounded by $H\left(X_1\right)$, so by monotone convergence theorem,
\begin{align*}
    H\left(X_1\right) & \geq \lim_{n \to \infty}I\left(X_1 ; Y_1^n\right) \\
    & = \lim_{n \to \infty}\sum_{i = 1}^{n}I\left(X_1 ; Y_i \mid Y_1^{i - 1}\right).
\end{align*}
This means that the series 
\begin{equation*}
    \sum_{i = 1}^{\infty}I\left(X_1 ; Y_i \mid Y_1^{i - 1}\right)
\end{equation*}
converges, and so 
\begin{equation*}
    \lim_{n \to \infty}I\left(X_i ; Y_n \mid Y_1^{n - 1}\right) = \lim_{n \to \infty}\abs{H\left(X_1\right) - H\left(Y_1^n \mid X_1\right)} = 0.
\end{equation*}
\section{Fixed-to-Variable-Length Data Compression}
Note that a fixed-to-fixed-length source code will map a sequence of $n$ random variables to a binary string of length $2^{nR}$. Now, let us consider another approach of coding: instead of encoding the entire sequence, we devise a way to encode every character in the sequence first. The code for the whole sequence then is just the concatenation of the codes of all characters. 
\begin{dfnbox}{Fixed-to-Variable-Length Source Code}{F2V}
    A {\color{red} \textbf{Fixed-to-Variable-Length Source Code}} for a random variable $X$ is a map 
    \begin{equation*}
        C \coloneqq \mathcal{X} \to \left\{0, 1\right\}^*
    \end{equation*}
    where 
    \begin{equation*}
        \left\{0, 1\right\}^*\bigcup_{n \in \N^+}\left\{0, 1\right\}^n
    \end{equation*}
    is the set of all finite-length binary strings. We say that $C\left(x\right)$ is the {\color{red} \textbf{codeword}} corresponding to $x$ and $\ell\left(x\right)$ is the {\color{red} \textbf{length}} of $C\left(x\right)$.
\end{dfnbox}
Notice that now $\ell\left(X\right)$ is a function of $X$, and so it is meaningful to talk about its expectation.
\begin{dfnbox}{Expeted Code Length}{El}
    The {\color{red} \textbf{expected length}} $L\left(C\right)$ of a code $C \colon \mathcal{X} \to \left\{0, 1\right\}^*$ for a random variable $X \sim p_X$ is 
    \begin{equation*}
        L\left(C\right) = \sum_{x \in \mathcal{X}}p_X\left(x\right)\ell\left(x\right) = \mathbb{E}\left[\ell\left(X\right)\right].
    \end{equation*} 
\end{dfnbox}
Given a string consisting of identically distributed random variables $X_1, X_2, \cdots, X_n$, we can form its codeword as $C\left(X_1\right), C\left(X_2\right), \cdots, C\left(X_n\right)$. However, the separation by comma here seems wasteful because it costs $\mathcal{O}\left(n\right)$ memory to store the commas. Can we come up with a code such that we can decode the string from the concatenation of the codewords for each character directly, without facing ambiguity? It is easy to see that a necessary condition for this is that the map $C$ must be injective.
\begin{dfnbox}{Non-singular Code}{nonsingular}
    A code $C$ is {\color{red} \textbf{non-singular}} if $C$ is injective.
\end{dfnbox}
Given a map $C$ to encode over an alphabet, we see that $C$ induces an encoder over all strings formed by the alphabet.
\begin{dfnbox}{Extension}{extension}
    The {\color{red} \textbf{extension}} of a code $C$ is the map 
    \begin{equation*}
        C^* \colon \mathcal{X}^* \to \left\{0, 1\right\}^*
    \end{equation*}
    such that $C^*\left(x_1x_2\cdots x_n\right) = C\left(x_1\right)C\left(x_2\right)\cdots C\left(x_n\right)$.
\end{dfnbox}
Clearly, if $C^*$ is non-singular, then for every code, there is only on unique string which can be encoded into it.
\begin{dfnbox}{Unique Decodability}{UD}
    A code $C$ is {\color{red} \textbf{uniquely decodable}} if $C^*$ is non-singular.
\end{dfnbox}
It is generally hard to decide if a code is uniquely decodable. However, there is a special class for uniquely decodable codes which are relatively easy to check.
\begin{dfnbox}{Prefix-Free Code}{PF}
    A code $C$ is called {\color{red} \textbf{prefix-free}} or {\color{red} \textbf{instantaneous}} if for all $x_1, x_2 \in \mathcal{X}$ with $x_1 \neq x_2$, we have $C\left(x_1\right) \neq C\left(x_2\right)s$ for all $s \in \left\{0, 1\right\}^*$.
\end{dfnbox}
For any random variable $X$, let $\mathcal{C}_X$ be the set of all codes, $\mathcal{S}_X$ be the set of non-singular codes, $\mathcal{U}_X$ be the set of uniquely decodable codes and $\mathcal{F}_X$ be the set of prefix-free codes on $X$ respectively, then 
\begin{equation*}
    \mathcal{F}_X \subseteq \mathcal{U}_X \subseteq \mathcal{S}_X \subseteq \mathcal{C}_X.
\end{equation*}
In fact, we can prove that the above inclusion is strict.

First, it is clear that $C\left(x\right) = 0$ for all $x \in \mathcal{X}$ is a singular code for $X$. Consider $\mathcal{X} = \left\{1, 2, 3\right\}$ and 
\begin{equation*}
    C\left(x\right) = \begin{cases}
        0 & \quad\textrm{if } x = 1 \\
        010 & \quad\textrm{if } x = 2 \\
        01 & \quad\textrm{if } x = 3
    \end{cases}.
\end{equation*}
Clearly, the codeword $010$ is not uniquely decodable and so we have found a non-singular code which is not uniquely decodable. With some work, we can show that the code 
\begin{equation*}
    C\left(x\right) = \begin{cases}
        10 & \quad\textrm{if } x = 1 \\
        00 & \quad\textrm{if } x = 2 \\
        11 & \quad\textrm{if } x = 3 \\
        110 & \quad\textrm{if } x = 4
    \end{cases}
\end{equation*}
is not prefix-free but uniquely decodable over $\left\{1, 2, 3, 4\right\}$.

Let us study the properties of prefix-free codes in more detail.
\begin{thmbox}{Kraft's Inequality}{Kraft}
    For any binary prefix-free code over some alphabet $\mathcal{X} \coloneqq \left\{x_1, x_2, \cdots, x_M\right\}$, if $\ell_i = \ell\left(x_i\right)$ for each $x_i \in \mathcal{X}$, then 
    \begin{equation*}
        \sum_{i = 1}^{M}2^{-\ell_i} \leq 1.
    \end{equation*} 
    Conversely, if there exists positive integers $\ell_1, \ell_2, \cdots, \ell_{M}$ such that 
    \begin{equation*}
        \sum_{i = 1}^{M}2^{-\ell_i} \leq 1,
    \end{equation*}
    then there exists a prefix-free code $C$ over the alphabet $\left\{x_1, x_2, \cdots, x_M\right\}$ with $\ell\left(x_i\right) = \ell_i$.
    \tcblower
    \begin{proof}
        Without loss of generality, let $\ell_i \leq \ell_j$ for all $i \leq j$. Let $A$ be a complete binary tree of depth $\ell_M$ and order $2^{\ell_M + 1} - 1$. Associate every left edge with $0$ and every right edge with $1$, then every code word corresponds to a unique path from the root of $A$ to some vertex in $A$. Therefore, there exists an injection $f \colon \mathcal{X} \to V\left(A\right)$. For any~$i < j$, we claim that $f\left(x_j\right)$ is not in the subtree rooted at $f\left(x_i\right)$. Suppose on contrary that~$f\left(x_j\right)$ is in the subtree rooted at $f\left(x_i\right)$, then since $\ell_j \geq \ell_i$, the codeword for $x_i$ must be a prefix to the codeword for $x_j$, which is a contradiction. Let $A_i$ be the set of leaves in the subtree rooted at $v_i \in V\left(A\right)$, then $A_i \cap A_j = \varnothing$ whenever $i \neq j$. Notice that 
        \begin{equation*}
            \abs{A_i} = 2^{\ell_M - \ell_i}.
        \end{equation*}
        Therefore, 
        \begin{align*}
            2^{\ell_M} & \geq \abs{\bigcup_{i = 1}^{M}A_i} \\
            & = \sum_{i = 1}^{M}2^{\ell_M - \ell_i},
        \end{align*}
        which implies that 
        \begin{equation*}
            \sum_{i = 1}^{M}2^{-\ell_i} \leq 1.
        \end{equation*} 
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        The converse of the theorem can be easily proven by tracing the complete binary tree backwards to re-construct the codeword for each character.
    \end{remark}
\end{notebox}
\subsection{Optimal Code}
With Theorem \ref{thm:Kraft}, we now have a systematic way to construct a prefix-free code. The next question is: how do we construct such a code with minimal code length? Given 
\begin{equation*}
    \mathcal{X} = \left\{x_1, x_2, \cdots, x_M\right\},
\end{equation*}
this becomes an optimisation problem 
\begin{align*}
    \min_{\ell_1, \ell_2, \cdots, \ell_M \in \N^+}&\sum_{i = 1}^{M}p_X\left(x_i\right)\ell_i \\
    \textrm{such that }& \sum_{i = 1}^{M}2^{-\ell_i} \leq 1.
\end{align*}
Notice that this is an integer program. For the sake of simplicity, we may consider dropping the constraint on integrality and loosen the problem into a convex program. We can write out the Lagrangian as
\begin{equation*}
    \mathcal{L}\left(\mathbfit{l}, \lambda\right) = \sum_{i = 1}^{M}p_X\left(x_i\right)\ell_i + \lambda\left(\sum_{i = 1}^{M}2^{-\ell_i} - 1\right).
\end{equation*}
At the optimum, we have 
\begin{equation*}
    \frac{\partial \mathcal{L}}{\partial \ell_i} = p_X\left(x_i\right) - \lambda2^{-\ell_i}\ln 2 = 0,
\end{equation*}
for all $i = 1, 2, \cdots, M$. If $\mathbfit{l}^*$ is an optimum, then $\ell_i^* \propto -\log_2p\left(x_i\right)$ for all $i = 1, 2, \cdots, M$. Notice that if $\ell_i^* = -\log_2p\left(x_i\right)$ for all $i = 1, 2, \cdots, M$, we have 
\begin{equation*}
    \sum_{i = 1}^{M}2^{-\ell_i} = 1.
\end{equation*}
It can be proven that such a choice yields the optimal scenario, where 
\begin{align*}
    L^* & = \sum_{i = 1}^{M}p_X\left(x_i\right)\ell_i^* \\
    & = -\sum_{i = 1}^{M}p_X\left(x_i\right)\log_2p_X\left(x_i\right) \\
    & = H\left(X\right).
\end{align*}
Clearly, the optimum of the corresponding integer program cannot be more optimal than this solution, which motivates the following theorem:
\begin{thmbox}{Lower Bound of Expected Code Length}{lowerBoundCodeLen}
    Let $L^*$ be the expected code length of any prefix-free code for a random variable $X$, then we have $L^* \geq H\left(X\right)$, where the equality holds if and only if $2^{-\ell_i} = p_i$ for all $i = 1, 2, \cdots, \abs{\mathcal{X}}$.
    \tcblower
    \begin{proof}
        Notice that 
        \begin{align*}
            L^* - H\left(X\right) & = \sum_{i = 1}^{\abs{\mathcal{X}}}p_i\ell^*_i - \sum_{i = 1}^{\abs{\mathcal{X}}}p_i\log_2\frac{1}{p_i} \\
            & = -\sum_{i = 1}^{\abs{\mathcal{X}}}p_i\log_2\left(2^{-\ell^*_i}\right) + \sum_{i = 1}^{\abs{\mathcal{X}}}p_i\log_2{p_i}.
        \end{align*}
        By Theorem \ref{thm:Kraft}, take $c \coloneqq \sum_{i = 1}^{\abs{\mathcal{X}}}2^{-\ell^*_i} \leq 1$ and define
        \begin{equation*}
            r_i \coloneqq \frac{2^{-\ell^*_i}}{c},
        \end{equation*}
        then clearly $\mathbfit{r}$ is a probability vector. Observe that we can re-write 
        \begin{align*}
            L^* - H\left(X\right) & = \sum_{i = 1}^{\abs{\mathcal{X}}}p_i\log_2\frac{p_i}{r_i} - \log_2 c \\
            & = D\left(p \parallel r\right) + \log_2\frac{1}{c} \\
            & \geq 0.
        \end{align*}
        Therefore, $L^* \geq H\left(X\right)$. Clearly, the equality is achieved if and only if 
        \begin{equation*}
            \sum_{i = 1}^{\abs{\mathcal{X}}}2^{-\ell^*_i} = 1
        \end{equation*}
        and $p = r$, i.e., $p_i = 2^{-\ell_i^*}$.
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        If $p_i = 2^{-\ell_i^*}$ for some integer $\ell_i^*$, then $p_i$ is known as a \textit{dyadic rational}.
    \end{remark}
\end{notebox}
Notice that to make $L^*$ closer to $H\left(X\right)$, we need $\ell_1^* \approx \log_2\frac{1}{p_i}$. However, based on Theorem \ref{thm:lowerBoundCodeLen} we also know it is impossible to have $\ell_1^* < \log_2\frac{1}{p_i}$. Therefore, a natural choice of the code lengths are the ceiling of the probabilities.
\begin{dfnbox}{Shannon Code}{shannonCode}
    A {\color{red} \textbf{Shannon code}} for a random variable $X$ over $\left\{x_1, x_2, \cdots, x_M\right\}$ is a code satisfying 
    \begin{equation*}
        \ell_i \coloneqq \ell\left(x_i\right) = \left\lceil\log_2\frac{1}{p_i}\right\rceil.
    \end{equation*}
\end{dfnbox}
We can check that for a Shannon code,
\begin{align*}
    \sum_{i = 1}^{M}2^{-\ell_i} & = \sum_{i = 1}^{M}2^{-\left\lceil\log_2\frac{1}{p_i}\right\rceil} \\
    & \leq \sum_{i = 1}^{M}2^{-\log_2\frac{1}{p_i}} \\
    & = \sum_{i = 1}^{M}p_i\\
    & = 1.
\end{align*}
Therefore, a Shannon code can be always made prefix-free and thus uniquely decodable. Furthermore, 
\begin{equation*}
    \log_2\frac{1}{p_i} \leq \ell_i = \left\lceil\log_2\frac{1}{p_i}\right\rceil < \log_2\frac{1}{p_i} + 1,
\end{equation*}
and so 
\begin{align*}
    \sum_{i = 1}^{M}p_i\log_2\frac{1}{p_i} & \leq \sum_{i = 1}^{M}p_i\ell_i \\
    & < \sum_{i = 1}^{M}p_i\log_2\frac{1}{p_i} + \sum_{i = 1}^{M}p_i \\
    & < H\left(X\right) + 1.
\end{align*}
Since $\sum_{i = 1}^{M}p_i\ell_i = L$, we have
\begin{equation*}
    H\left(X\right) \leq L^* < H\left(X\right) + 1.
\end{equation*}
Note that $1$ might induce a huge interval if $X$ is very small. To counter this, we adopt a technique known as \textit{coding over long blocks}.

Consider any $\left(x_1, x_2, \cdots, x_n\right) \in \mathcal{X}^n$ for some $n \in \N^+$ and define the expected codeword length per unit symbol for this vector as 
\begin{equation*}
    L_n \coloneqq \frac{1}{n}\mathbb{E}\left[\ell\left(X_1, X_2, \cdots, X_n\right)\right].
\end{equation*}
By the previous bounds, we have 
\begin{equation*}
    H\left(X_1^n\right) \leq \mathbb{E}\left[\ell\left(X_1, X_2, \cdots, X_n\right)\right] < H\left(X_1^n\right) + 1,
\end{equation*}
and so 
\begin{equation*}
    \frac{H\left(X_1^n\right)}{n} \leq L_n^* < \frac{H\left(X_1^n\right)}{n} + \frac{1}{n}.
\end{equation*}
Since the source is independent and identically distributed, we have 
\begin{equation*}
    H\left(X\right) \leq L_n^* < H\left(X\right) + \frac{1}{n}.
\end{equation*}
Therefore, the longer our block is, the smaller the difference between $L_n^*$ and $H\left(X\right)$ will be.
\end{document}