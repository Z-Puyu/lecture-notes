\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{yhmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}
\DeclareSymbolFont{yhlargesymbols}{OMX}{yhex}{m}{n} \DeclareMathAccent{\yhwidehat}{\mathord}{yhlargesymbols}{"62}

\usepackage{scalerel}[2014/03/10]
\usepackage{stackengine}

\renewcommand\widetilde[1]{\ThisStyle{%
  \setbox0=\hbox{$\SavedStyle#1$}%
  \stackengine{1pt-\LMpt}{$\SavedStyle#1$}{%
    \stretchto{\scaleto{\SavedStyle\mkern.2mu\sim}{.5467\wd0}}{.5\ht0}%
%    .2mu is the kern imbalance when clipping white space
%    .5467++++ is \ht/[kerned \wd] aspect ratio for \sim glyph
  }{O}{c}{F}{T}{S}%
}}
\makeatletter
\let\save@mathaccent\mathaccent
\newcommand*\if@single[3]{%
  \setbox0\hbox{${\mathaccent"0362{#1}}^H$}%
  \setbox2\hbox{${\mathaccent"0362{\kern0pt#1}}^H$}%
  \ifdim\ht0=\ht2 #3\else #2\fi
  }
%The bar will be moved to the right by a half of \macc@kerna, which is computed by amsmath:
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
%If there's a superscript following the bar, then no negative kern may follow the bar;
%an additional {} makes sure that the superscript is high enough in this case:
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
%Use a separate algorithm for single symbols:
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{%
  \begingroup
  \def\mathaccent##1##2{%
%Enable nesting of accents:
    \let\mathaccent\save@mathaccent
%If there's more than a single symbol, use the first character instead (see below):
    \if#32 \let\macc@nucleus\first@char \fi
%Determine the italic correction:
    \setbox\z@\hbox{$\macc@style{\macc@nucleus}_{}$}%
    \setbox\tw@\hbox{$\macc@style{\macc@nucleus}{}_{}$}%
    \dimen@\wd\tw@
    \advance\dimen@-\wd\z@
%Now \dimen@ is the italic correction of the symbol.
    \divide\dimen@ 3
    \@tempdima\wd\tw@
    \advance\@tempdima-\scriptspace
%Now \@tempdima is the width of the symbol.
    \divide\@tempdima 10
    \advance\dimen@-\@tempdima
%Now \dimen@ = (italic correction / 3) - (Breite / 10)
    \ifdim\dimen@>\z@ \dimen@0pt\fi
%The bar will be shortened in the case \dimen@<0 !
    \rel@kern{0.6}\kern-\dimen@
    \if#31
      \overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}%
      \advance\dimen@0.4\dimexpr\macc@kerna
%Place the combined final kern (-\dimen@) if it is >0 or if a superscript follows:
      \let\final@kern#2%
      \ifdim\dimen@<\z@ \let\final@kern1\fi
      \if\final@kern1 \kern-\dimen@\fi
    \else
      \overline{\rel@kern{-0.6}\kern\dimen@#1}%
    \fi
  }%
  \macc@depth\@ne
  \let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
  \mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
  \macc@set@skewchar\relax
  \let\mathaccentV\macc@nested@a
%The following initialises \macc@kerna and calls \mathaccent:
  \if#31
    \macc@nested@a\relax111{#1}%
  \else
%If the argument consists of more than one symbol, and if the first token is
%a letter, use that letter for the computations:
    \def\gobble@till@marker##1\endmarker{}%
    \futurelet\first@char\gobble@till@marker#1\endmarker
    \ifcat\noexpand\first@char A\else
      \def\first@char{}%
    \fi
    \macc@nested@a\relax111{\first@char}%
  \fi
  \endgroup
}
\makeatother

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\zero}{\mathbf{0}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\I}{\mathbfit{I}}
\newcommand{\e}{\mathrm{e}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\im}{\mathrm{i}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\begin{document}
\fancyhead[L]{
    Information and Coding Theory
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Probability}
\section{Probability Spaces}
In an elementary level, we have been viewing probability as the quotient between the number of desired outcomes and the number of all possible outcomes. This definition, though intuitive, is not very solid when it comes to an infinite sample space. In this introductory chapter, we would establish the theories of probability using a more modern and rigorous structure.
\begin{dfnbox}{Set Algebra}{setAlgebra}
    Let $X$ be a set. A {\color{red} \textbf{set algebra}} over $X$ is a family $\mathcal{F} \subseteq \mathcal{P}(X)$ such that 
    \begin{itemize}
        \item $X \backslash F \in \mathcal{F}$ for all $F \in \mathcal{F}$ (closed under complementation);
        \item $X \in \mathcal{F}$;
        \item $X_1 \cup X_2 \in \mathcal{F}$ for any $X_1, X_2 \in \mathcal{F}$ (closed under binary union).
    \end{itemize}
\end{dfnbox}
There are several immediate implications from the above definition. 

First, by closure under complementation, we know that an algebra over any set $X$ must contain the empty set. 

Second, by De Morgan's Law, one can easily check that if the first $2$ axioms hold, the closure under binary union is equivalent to 
\begin{itemize}
    \item $X_1 \cap X_2 \in \mathcal{F}$ for any $X_1, X_2 \in \mathcal{F}$;
    \item $\bigcup_{i = 1}^{n}X_i \in \mathcal{F}$ for any $X_1, X_2, \cdots, X_n \in \mathcal{F}$ for all $n \in \N$;
    \item $\bigcap_{i = 1}^{n}X_i \in \mathcal{F}$ for any $X_1, X_2, \cdots, X_n \in \mathcal{F}$ for all $n \in \N$.
\end{itemize}
$(X, \mathcal{F})$ is known as a \textit{field of sets}, where the elements of $X$ are called \textit{points} and those of $\mathcal{F}$, \textit{complexes} or \textit{admissible sets} of $X$.

In probability theory, what we are interested in is a special type of set algebras known as $\sigma$-\textit{algebras}.
\begin{dfnbox}{$\sigma$-Algebra}{sigmaAlgebra}
    A {\color{red} \textbf{$\sigma$-Algebra}} over a set $A$ is a non-empty set algebra over $A$ that is closed under countable union.
\end{dfnbox}
Of course, by the same argument as above, we known that any $\sigma$-algebra is closed under countable intersection as well.

Now, as we all know, we can take some set $\Omega$ as a \textit{sample space} and denote an \textit{event} by some subset of $\Omega$. Roughly speaking, we could now define the probability of an event $E \subseteq \Omega$ as the ratio between the sets' volumes. The remaining question now is: how do we define the volume of a set properly?
\begin{dfnbox}{Measure}{measure}
    Let $X$ be a set and $\Sigma$ be a $\sigma$-algebra over $X$. A {\color{red} \textbf{measure}} over $\Sigma$ is a function 
    \begin{equation*}
        \mu \colon \Sigma \to \R \cup \{-\infty, +\infty\}
    \end{equation*}
    such that 
    \begin{itemize}
        \item $\mu(E) \geq 0$ for all $E \in \Sigma$ (non-negativity);
        \item $\mu(\varnothing) = 0$;
        \item $\mu\left(\bigcup_{i = 1}^{\infty}E_i\right) = \sum_{i = 1}^{\infty}\mu(E_i)$ for any countable collection of pairwise disjoint elements of $\Sigma$ (countable additivity or $\sigma$-additivity).
    \end{itemize}
    The triple $(X, \Sigma, \mu)$ is known as a {\color{red} \textbf{measure space}} and the pair $(X, \Sigma)$, a {\color{red} \textbf{measurable space}}.
\end{dfnbox}
One thing to note here is that if at least one $E \in \Sigma$ has a finite measure, then $\mu(\varnothing) = 0$ is automatically guaranteed for obvious reasons.
\begin{dfnbox}{Probability Space}{probSpace}
    Let $\Omega$ be a sample space and $\mathcal{F}$ be a $\sigma$-algebra over $\Omega$. A {\color{red} \textbf{probability space}} is a measure space $(\Omega, \mathcal{F}, \mathbb{P})$ where $\mathbb{P} \colon \mathcal{F} \to [0, 1]$, known as a {\color{red} \textbf{probability measure}}, is such that $\mathbb{P}(\Omega) = 1$.
\end{dfnbox}
Obviously, the above definition immediately guarantees that 
\begin{enumerate}
    \item $\mathbb{P}(A^c) = 1 - \mathbb{P}(A)$;
    \item $\mathbb{P}(A) \leq \mathbb{P}(B)$ if $\mathbb{P}(A) \subseteq \mathbb{P}(A)$;
    \item $\mathbb{P}(A \cup B) \leq \mathbb{P}(A) + \mathbb{P}(B)$.
\end{enumerate}
The third result follows from a direct application of the principle of inclusion and exclusion. By induction, one can easily check that 
\begin{equation*}
    \mathbb{P}\left(\bigcup_{i = 1}^{n}E_i\right) \leq \sum_{i = 1}^{n}\mathbb{P}(E_i)
\end{equation*}
for any finitely many events. The following proposition extends this result to countable collections of events:
\begin{probox}{Union Bound of Countable Collections of Events}{unionBound}
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $E_1, E_2, \cdots, E_n, \cdots \in \mathcal{F}$ is any countable sequence of events, then 
    \begin{equation*}
        \mathbb{P}\left(\bigcup_{i = 1}^{\infty}E_i\right) \leq \sum_{i = 1}^{\infty}\mathbb{P}(E_i).
    \end{equation*}
    \tcblower
    \begin{proof}
        Define $F_1 \coloneqq E_1$ and $F_k \coloneqq E_k \backslash \bigcup_{i = 1}^{k - 1}E_i$ for $k \geq 2$. Clearly, the $F_i$'s are pairwise disjoint. By Definition \ref{dfn:sigmaAlgebra}, the $F_i$'s are elements of $\mathcal{F}$. Note that $\mathbb{P}(F_i) \leq \mathbb{E_i}$ for all $i \in \N^+$, so 
        \begin{align*}
            \mathbb{P}\left(\bigcup_{i = 1}^{\infty}E_i\right) & = \mathbb{P}\left(\bigcup_{i = 1}^{\infty}F_i\right) \\
            & = \sum_{i = 1}^{\infty}\mathbb{P}(F_i) \\
            & \leq \sum_{i = 1}^{\infty}\mathbb{P}(E_i).
        \end{align*}
    \end{proof}
\end{probox}
Next, we will introduce the notion of \textit{random variables} formally. For this purpose, we first establish the notion of a \textit{Borel algebra}.
\begin{dfnbox}{Borel Algebra}{borelAlgebra}
    Let $X$ be a topological space. A {\color{red} \textbf{Borel set}} on $X$ is a set which can be formed via countable union, countable intersection and relative complementation of open sets in $X$. The smallest $\sigma$-algebra over $X$ containing all Borel sets on $X$ is known as the {\color{red} \textbf{Borel algebra}} over $X$.
\end{dfnbox}
Clearly, the Borel algebra over $X$ contains all open sets in $X$ according to the above axioms from Definition \ref{dfn:sigmaAlgebra}. This helps us define the following:
\begin{dfnbox}{Random Variable}{RV}
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $(\mathcal{X}, \mathcal{B})$ be a measurable space where $\mathcal{B}$ is the Borel algebra over $\mathcal{X}$. A {\color{red} \textbf{random variable}} is a function $X \colon \Omega \to \mathcal{X}$ such that 
    \begin{equation*}
        \left\{\omega \in \Omega \colon X(\omega) \in B\right\} \in \mathcal{F} 
    \end{equation*}
    for all $B \in \mathcal{B}$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Rigorously, such a random variable $X$ is a \textit{measurable function} or \textit{measurable mapping} from $(\Omega, \mathcal{F})$ to $(\mathcal{X}, \mathcal{B})$.
    \end{remark}
\end{notebox}
The probability measure $\mathbb{P}$ thus induces a probability measure $P_X$ over $(\mathcal{X}, \mathcal{B})$.
\begin{dfnbox}{Distribution}{distribution}
    Let $X \colon \Omega \to \mathcal{X}$ be a random variable over the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and $\mathcal{B}$ be the Borel algebra over $\mathcal{X}$, the {\color{red} \textbf{distribution}} of $X$ is the probability measure $P_X$ on $(\mathcal{X}, \mathcal{B})$ given by 
    \begin{equation*}
        P_X(B) = \mathbb{\left\{\omega \in \Omega \colon X(\omega) \in B\right\}}.
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Often times, we write $\mathrm{Pr}(X \in B) = P_X(B)$.
    \end{remark}
\end{notebox}
In the context of information theory, we mostly are concerned with real-valued random variables only.
\begin{dfnbox}{Real-Valued Random Variable}{RRV}
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, a {\color{red} \textbf{real-valued random variable}} over the space is a mapping $X \colon \Omega \to \R$ such that 
    \begin{equation*}
        \left\{\omega \in \Omega \colon X(\omega) \leq x\right\} \in \mathcal{F}
    \end{equation*}
    for all $x \in \R$.
\end{dfnbox}
Note that the Borel set over $\R$ is just the family of all open intervals. 

Clearly, if $X$ is a real-valued random variable, we have $\left\{\omega \in \Omega \colon X(\omega) > x\right\} \in \mathcal{F}$. Moreover, we claim that 
\begin{equation*}
    \left\{\omega \in \Omega \colon X(\omega) < x\right\} = \bigcup_{y < x}\left\{\omega \in \Omega \colon X(\omega) \leq y\right\}.
\end{equation*}
The proof is quite straightforward and is left to the reader as an exercise. By Definition \ref{dfn:sigmaAlgebra}, this means that 
\begin{equation*}
    \left\{\omega \in \Omega \colon X(\omega) < x\right\} \cup \left\{\omega \in \Omega \colon X(\omega) > x\right\} \in \mathcal{F}.
\end{equation*}
Therefore, $\left\{\omega \in \Omega \colon X(\omega) = x\right\} \in \mathcal{F}$. This argument justifies the probabilities $\mathrm{Pr}(X < x)$ and $\mathrm{Pr}(X = x)$. We give a special name to the range of a random variable in computer science.
\begin{dfnbox}{Alphabet}{alphabet}
    Let $X$ be a random variable, the range of $X$ is called an {\color{red} \textbf{alphabet}}, denoted as $\mathscr{X}$.
\end{dfnbox}
Recall that we have defined expectations for discrete and continuous random variables in elementary probability theory. In terms of measure theory, the two formulae can be unified as 
\begin{equation*}
    \mathbb{E}[X] = \int_{\Omega}\!X(\omega)\,\d\mathbb{P}(\omega).
\end{equation*}
Note that $\mathbb{E}[X]$ is a real number while $\mathbb{E}[X \mid Y]$ is a \textbf{random variable} formed as a function of $Y$. In general, the following result holds:
\begin{thmbox}{Law of Iterated Expectations}{iterExpectations}
    Let $X$ and $Y$ be random variables, then $\mathbb{E}\bigl[\mathbb{E}[X \mid Y]\bigr] = \mathbb{E}[X]$.
\end{thmbox}
\section{Markov Chains}
\begin{thmbox}{Markov Inequality}{MarkovIneq}
    If $X$ is a non-negative random variable, then $\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}[X]}{a}$ for all $a > 0$.
\end{thmbox}
\begin{thmbox}{Chebyshev's Inequality}{ChebyshevIneq}
    For any real-valued random variable $X$ with finite variance, 
    \begin{equation*}
        \mathbb{P}\left(\abs{X - \mathbb{E}[X]} > a\sqrt{\mathrm{Var}(X)}\right) \leq \frac{1}{a^2}
    \end{equation*}
    for all $a > 0$.
\end{thmbox}
A convex function is an overestimate of all linear functions whose values are bounded above by it.
\begin{thmbox}{Jensen's Inequality}{JensenIneq}
    Let $f$ be a convex function and $X$ be a random variable, then $\mathbb{E}[f(x)] \geq f(\mathbb{E}[X])$.
\end{thmbox}
\end{document}