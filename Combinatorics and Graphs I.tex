\documentclass[math]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}
\usetikzlibrary{shapes.geometric}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
\newcommand{\e}{\mathrm{e}}

\newenvironment{solution}
    {\let\oldqedsymbol=\qedsymbol
    \renewcommand{\qedsymbol}{\ }
    \begin{proof}[Solution]
    }
    {\end{proof}
    \renewcommand{\qedsymbol}{\oldqedsymbol}
    }

\begin{document}
\fancyhead[L]{
    Combinatorics and Graphs I 
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Permutations and Combinations}
\section{Basic Counting Principles}
An important motivation to study combinatorics is to count the \textbf{number of ways} in which an event may occur. Intuitively, we have two approaches to count.

The first approach is to categorise the event into \textbf{non-overlapping cases}. This means that we break an event into mutually exclusive sub-events, after which we can count the number of ways for each sub-event to occur. The agregate of these counts is the total number of ways for the original event to occur.

Those familiar with basic set theory may consider $E$ to be the set containing all distinct ways for an event to occur. By breaking up the event, we essentially establish a \textbf{partition} of~$E$, so that the sum of cardinalities of all the elements in that partition equals the cardinality of $E$.

This motivates us to write the following principle using set notations.
\begin{thmbox}{Addition Principle (AP)}{AP}
    Let $k \in \N^+$ and let $A_1, A_2, \cdots, A_k$ be $k$ finite sets which are pairwise disjoint, i.e. $A_i \cap A_j = \varnothing$ whenever~$i \neq j$, then
   \begin{equation*}
        \abs{\bigcup_{i = 1}^k A_i} = \sum_{i = 1}^{k}\abs{A_i}.
   \end{equation*} 
   \tcblower
   \begin{proof}
        The case where $k = 1$ is trivial.

        Suppose that when $k = n$, we have
        \begin{equation*}
            \abs{\bigcup_{i = 1}^n A_i} = \sum_{i = 1}^{n}\abs{A_i}
        \end{equation*} 
        for any $n$ finite sets which are pairwise disjoint. Let $A_{n + 1}$ be an arbitrary finite set which is disjoint with any of the $A_i$'s from the $n$ sets. So we have:
        \begin{align*}
            \abs{\bigcup_{i = 1}^{n + 1} A_i} & = \abs{\left(\bigcup_{i = 1}^n A_i\right) \cup A_{n + 1}} \\
            & = \abs{\bigcup_{i = 1}^n A_i} + \abs{A_{n + 1}} - \abs{\left(\bigcup_{i = 1}^n A_i\right) \cap A_{n + 1}} \\
            & = \left(\sum_{i = 1}^{n}\abs{A_i}\right) + \abs{A_{n + 1}} - \abs{\varnothing} \\
            & = \sum_{i = 1}^{n + 1}\abs{A_i}.
        \end{align*}
        Therefore, the original statement holds for all $k \in \N^+$.
   \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        In more casual language, this means that if an event $E_k$ has $n_k$ distinct ways to occur, then there is $\sum_{i = 1}^{k}n_k$ ways for at least one of the events $E_1, E_2, \cdots, E_k$ to occur, provided that $E_i$ and $E_j$ can never occur concurrently whenever $i \neq j$.
    \end{remark}
\end{notebox}
Given an event $E$, the other approach to count the number of ways for it to occur is to break~$E$ up internally into \textbf{non-overlapping stages}.

With set notations, we can write the $i$-th stage for $E$ to occur as $e_i$, and so a way for $E$ to occur can be represented by an ordered tuple $(e_1, e_2, \cdots, e_k)$, where $k$ is the total number of stages to undergo for $E$ to occur.

Let $E_i$ denote the set of all distinct ways to undergo the $i$-th stage of $E$, then it is easy to see that $E$ is just the \textbf{Cartesian product} of all the $E_i$'s. Hence, we derive the following principle:
\begin{thmbox}{Multiplication Principle (MP)}{MP}
    Let $k \in \N^+$ and let $A_1, A_2, \cdots, A_k$ be $k$ pairwise disjoint finite sets, then
    \begin{equation*}
        \abs{\prod_{i = 1}^{k}A_i} = \prod_{i = 1}^{k}\abs{A_i}.
    \end{equation*}
    \tcblower
    \begin{proof}
        The case where $k = 1$ is trivial.

        Suppose that when $k = n$, we have
        \begin{equation*}
            \abs{\prod_{i = 1}^{n}A_i} = \prod_{i = 1}^{n}\abs{A_i}
        \end{equation*} 
        for any $n$ finite sets which are pairwise disjoint. Let $A_{n + 1}$ be an arbitrary finite set which is disjoint with any of the $A_i$'s from the $n$ sets. Take $a_i, a_j \in A_{n + 1}$. Note that for all $\mathbfit{a} \in \prod_{i = 1}^{n}A_i$, $(\mathbfit{a}, a_i) \neq (\mathbfit{a}, a_j)$ whenever $a_i \neq a_j$. This means that
        \begin{align*}
            \abs{\prod_{i = 1}^{n + 1}A_i} & = \abs{\prod_{i = 1}^{n}A_i \times A_{n + 1}} \\
            & = \abs{\prod_{i = 1}^{n}A_i}\abs{A_{n + 1}} \\
            & = \left(\prod_{i = 1}^{n}\abs{A_i}\right)\abs{A_{n + 1}} \\
            & = \prod_{i = 1}^{n + 1}\abs{A_i}
        \end{align*}
        Therefore, the original statement holds for all $k \in \N^+$.
    \end{proof}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        In more casual language, this means that if an event $E$ requires $k$ stages to be undergone before it occurs and the $i$-th stage has $n_i$ ways to complete, then there is $\prod_{i = 1}^{k}n_k$ ways for $E$ to occur, provided that no two different stages complete concurrently.
    \end{remark}
\end{notebox}
Often times, it is not straight-forward to count directly due to the presence of restrictions. We shall consider the following:

Let $E$ be the set of all possible ways for an event to occur. Let $p$ be some predicate representing some restriction and let $E(p)$ denote the set of all possible ways for the event to occur while $p$ holds. Note that:
\begin{displaymath}
    E(p) \cup E(\neg p) = E \quad \textrm{and} \quad E(p) \cap E(\neg p) = \varnothing,
\end{displaymath}
i.e. $\left\{E(p), E(\neg p)\right\}$ is a partition of $E$. Therefore, to count the number of ways for the event to occur while $p$ holds, it suffices to compute $E(\neg p)$, i.e. find the number of ways for the event to occur while $p$ does not hold.
\begin{thmbox}{Principle of Complementation}{PC}
    Let $U$ be a set and let $E \subseteq U$, then 
    \begin{equation*}
        \abs{E} = \abs{U} - \abs{U - E}.
    \end{equation*}
\end{thmbox}
\begin{notebox}
    \begin{remark}
        It may also help to think the Principle of Complementation as an inverse of the Addition Principle, where
        \begin{displaymath}
            E = \bigcup_{i = 1}^n E_i
        \end{displaymath}
        is the total number of ways for an event to occur and $E_p$ is the number of ways for the event to occur with restriction $p$.
    \end{remark}
\end{notebox}
In some cases, it is difficult to count the objects directly. However, note that the events for which we are counting are just sets, so we can treat ``number of occurrences'' of an event as the \textbf{cardinality} of a set. Therefore, we have the following principles:
\begin{thmbox}{Injection Principle}{injPrinciple}
    Let $A$, $B$ be finite sets. If there exists an injection
    \begin{displaymath}
        f \colon A \hookrightarrow B,
    \end{displaymath}
    then $\abs{A} \leq \abs{B}$.
\end{thmbox}
\begin{thmbox}{Bijection Principle}{bijPrinciple}
    Let $A$, $B$ be finite sets. If there exists a bijection
    \begin{displaymath}
        f \colon A \to B,
    \end{displaymath}
    then $\abs{A} = \abs{B}$.
\end{thmbox}
We will finish this section with two interesting problems.
\begin{exbox}{No Consecutive}{}
    Let $X = \left\{x \in \N^+ \colon x \leq n\right\}$. Find the number of $r$-combinations of $X$ such that~$\abs{x_i - x_j} \geq 2$ whenever $i \neq j$.
    \tcblower   
    \begin{solution}
        Note that for each of the $r$-combinations of $X$, we can re-write it into a strictly increasing sequence $\{x_i\}_{i = 1}^r$ with $x_i \in X$ and $x_{i + 1} - x_i \geq 2$. Let the set of all such sequences be $S_X$. Consider the function
        \begin{displaymath}
            f \colon S_X \to Y
        \end{displaymath}
        where $f\left(\{x_i\}_{i = 1}^r\right) = \{x_i - i + 1\}_{i = 1}^r$ and $Y$ is the set of all strictly increasing sequences whose terms are integers bounded between $1$ and $n - r + 1$ inclusive.
        \\\\
        Take two different sequences $s_1, s_2 \in S_X$. Let the $k$-th terms of $s_1$ and $s_2$ be $k_1$ and $k_2$ respectively such that $k_1 \neq k_2$. Let $t_1 = f(s_1)$ and $t_2 = f(s_2)$, whose $k$-th terms are $h_1$ and $h_2$ respectively. Note that $h_1 = k_1 - k + 1 \neq k_2 - k + 1 = h_2$, so~$t_1 \neq t_2$, which means $f$ is injective.
        \\\\
        Take an arbitrary $\{y_i\}_{i = 1}^r \in Y$ and consider the sequence $\{y_i + i - 1\}_{i = 1}^r$. Note that~$1 \leq y_i \leq n - r + 1$, so $1 \leq y_i + i - 1 \leq n$ for $i = 1, 2, \cdots, r$, which means~$\{y_i + i - 1\}_{i = 1}^r \in S_X$. This means that for all $y \in Y$, there exists some $s \in S_X$ such that $f(s) = y$, so $f$ is surjective.
        \\\\
        Therefore, $f$ is a bijection and so
        \begin{equation*}
            \abs{S_X} = \abs{Y} = C^{n - r + 1}_r.
        \end{equation*}
    \end{solution}
\end{exbox}
\begin{exbox}{Top Secret}{}
    Six scientists are working on a secret project. They wish to lock up the documents in a cabinet so that the cabinet can be opened when and only when three or more of the scientists are present. Suppose that each lock has one and only one key with the locks being pairwise distinct.
    \begin{enumerate}
        \item What is the smallest number of locks needed?
        \item What is the smallest number of keys that each scientist needs to carry?
    \end{enumerate}
    \tcblower   
    \begin{solution}
        Let $S = \left\{s_1, s_2, s_3, s_4, s_5, s_6\right\}$ be the set of scientists and $L$ be the set of locks. Define $T$ to be the set of all subsets of $S$ with $2$ elements, then for any $P \in T$, $P$ is a pair of distinct scientists.
        \\\\
        Let $L_P \subseteq L$ be the set of locks which cannot be opened by the pair of scientists in $P$. Note that for all $P \in T$, $L_P \neq \varnothing$. 
        \\\\
        Consider $P, Q \in T$ with $P \neq Q$, then $\abs{P \cup Q} \geq 3$. Thus, $L_P \cap L_Q = \varnothing$. Now, define the function
        \begin{displaymath}
            f \colon T \to L
        \end{displaymath}
        such that $f(P) = \ell_P$ where $\ell_P$ is an arbitrary element from $L_P$ (note that we can do this without using Choice as each of the $L_P$'s is finite).
        \\\\
        Suppose that there exist $P, Q \in T$ with $P \neq Q$ such that $f(P) = f(Q) = \ell_0$. Notice that this would mean that $\ell_0 \in L_P \cap L_Q$, which is a contradiction. Therefore,~$f$ must be injective. Hence, 
        \begin{equation*}
            \abs{L} \geq \abs{T} = C^6_2 = 15.
        \end{equation*}
        Now, choose an arbitrary element from $S$, say $s_k$, and consider the set $S' = S - \{s_k\}$. Let $T'$ be the set of all subsets of $S'$ with $2$ elements, then for any $P \in T'$, $L_{P \cup \{s_k\}} = \varnothing$, i.e., $s_k$ must have the keys to open all locks which the pair $P$ cannot open with their own keys. Let this set of keys be $K_P$. Note that $L_P \cap L_Q = \varnothing$ whenever $P \neq Q$, so~$K_P \cap K_Q = \varnothing$ whenever $P \neq Q$. Let the set of all keys held by $s_k$ be $H_k$, then
        \begin{equation*}
            \abs{H_k} \geq \abs{S'} = C^5_2 = 10.
        \end{equation*}
    \end{solution}
\end{exbox}
\section{Permutations}
\subsection{Linear Permutations}
A fundamental problem in combinatorics is described as follows: given a set $S$, how many ways are there to arrange $r$ elements in $S$, i.e. how many \textbf{distinct sequences} can be formed using the elements in $S$ without repetition? The process of selecting elements from $S$ and arranging them as a sequence is known as \textbf{permutation}.

Note that forming a sequence using $r$ elements from a set $S$ is an event consisting of $r$ stages, as we need to select an element for each of the $r$ terms of the sequence. Suppose $S$ has $n$ elements. For the first term of the sequence, we can choose any of the elements in $S$, so there is $n$ ways to do it. For the second term, since we cannot repeat the elements, we are left with $n - 1$ choices. 

Continue choosing elements in this way, we realise that if we choose the terms sequentially, when we reach the $k$-th term we will be left with $n - k + 1$ options as the previous~$(k - 1)$ terms have taken away $(k - 1)$ elements. By Theorem \ref{thm:MP}, we know that the number of sequences which can be formed is given by $\prod_{i = 1}^{r}(n - r + i)$.
\begin{dfnbox}{Permutations}{permutations}
    Let $A$ be a finite set such that $\abs{A} = n$, an $r$-permutation of $A$ is a way to arrange $r$ elements of $A$, denoted as $P^n_r$ and given by
    \begin{equation*}
        P^n_r = \prod_{i = 1}^{r}(n - r + i) = \frac{n!}{(n - r)!}.
    \end{equation*}
\end{dfnbox}
With some algebraic manipulations, it is easy to derive the following formula, which we, however, will prove in a combinatorial manner.
\begin{thmbox}{}{permutationId}
    Let $n, r \in \N$ with $r \leq n$, then $P^{n + 1}_r = P^n_r + rP^n_{r - 1}$.
    \tcblower
    \begin{proof}
        Let $S = \left\{x \in \N^+ \colon x \leq n + 1\right\}$ represent $(n + 1)$ distinct objects. Consider a permutation of $S$:
        \\\\
        If $n + 1$ is not inside the permutation, this is equivalent to an $r$-permutation of~$S - \{n + 1\}$, so there are $P^n_r$ such permutations.
        \\\\
    If $n + 1$ is inside the permutation, it means we need to first find an $(r - 1)$-permuation of $S - \{n + 1\}$, which has $P^n_{r - 1}$ ways to do. After that, we need to insert~$n + 1$ into each of these $(r - 1)$-permutations. Note that for each of such permutations, there are $r$ positions into which we can place $n + 1$. Therefore, the total number of $r$-permuations of $S$ derived in this manner is $rP^n_{r - 1}$.
        \\\\
        Therefore, there are $P^n_{r} + rP^n_{r - 1}$ $r$-permutations of $S$, i.e. $P^{n + 1}_r = P^n_r + rP^n_{r - 1}$. 
    \end{proof}
\end{thmbox}

\subsection{Circular Permutations}
Consider arranging $n$ distinct objects around a circle. If the slots around the circle are uniquely labelled, this is exactly the same as permutations along a straight line.

However, if the slots are identical, i.e. we are arranging $n$ distinct objects around a circle with identical slots, only the \textbf{relative positions} of the objects matter.

Let $\mathbfit{x}_i$ be an arbitrary straight-line permutations of the $n$ objects and let $\mathbfit{y}_i$ be the corresponding circular permutation of the $n$ objects.

Note that if we translate every element in $\mathbfit{x}_i$ by $k$ positions, this will result in a different straight-line permutation $\mathbfit{x}_j$ but does not change the corresponding circular permutation because the relative positions of the objects remain unchanged.

Notice that $k$ can take the values $0, 1, 2, \cdots, n - 1$, so for the same set of $n$ distinct objects, every circular permutation is mapped to $n$ straight-line permutations.
\begin{dfnbox}{Circular Permutations}{circPermutation}
    Let $A$ be a finite set such that $\abs{A} = n$, a circular $r$ permutation of $A$ is a way to arrange~$r$ elements of $A$ around a circular locus, denoted as $Q^n_r$ and given by
    \begin{equation*}
        Q^n_r = \frac{P^n_r}{r} = \frac{n!}{r(n - r)!}.
    \end{equation*}
\end{dfnbox}

\subsection{Permutations with Repetitions}
Consider the following scenario: suppose $T_1, T_2, \cdots T_n$ are distinct labels and $a_1, a_2, \cdots, a_r$ are $r$ distinct objects. If we wish to associate each object with one label, then each of the ways to do so is a permutation, the terms of which are taken from the $T_i$'s. Notice that under this setting, the $T_i$'s are allowed to be repeated in a permutation.

To reason such problems, we first introduce the notion of a \textit{multi-set}.
\begin{dfnbox}{Multi-set}{multiset}
    The {\color{red} \textbf{multi-set}} is defined to be
    \begin{equation*}
        M \coloneqq \left\{r_1 \cdot a_1, r_2 \cdot a_2, \cdots, r_n \cdot a_n\right\},
    \end{equation*}
    where $a_i$ has $r_i$ identical copies.
\end{dfnbox}
Observe that in a permutation of such a multi-set $M$, the object $a_i$ is repeated $r_i$ times. This brings us to the following formula:
\begin{probox}{Generalised Formula for Permutations}{genPnr}
    Let $k \in \N^+$ and let $A_1, A_2, \cdots, A_k$ be $k$ distinct objects, where $A_i$ occurs $n_i > 0$ times for~$i = 1, 2, \cdots, k$, then each permutation for these objects, corresponds to a unique permutation of the multi-set
    \begin{displaymath}
        M = \left\{n_1 \cdot A_1, n_2 \cdot A_2, \cdots, n_k \cdot A_k\right\},
    \end{displaymath}
    and the total number of permutations is given by
    \begin{equation*}
        \frac{\left(\sum_{i = 1}^{k}n_i\right)!}{\prod_{i = 1}^{k}\left(n_i!\right)}.
    \end{equation*}
\end{probox}
We may also consider the special case where we want to form an $r$-permutation with $k$ objects, where each object can repeat an arbitrary number of times. Notice that this is equivalent to finding the permutations of the multi-set
\begin{displaymath}
    M = \left\{\infty \cdot a_1, \infty \cdot a_2, \cdots, \infty \cdot a_k\right\}.
\end{displaymath}
In this case, we can simply iterate through each of the slots in the $r$-permutation and choose the object to be placed there. Trivially, there are always $k$ choices for each slot.
\begin{probox}{A Special Case}{pnrSpe}
    Consider the multi-set
    \begin{displaymath}
        M = \left\{\infty \cdot a_1, \infty \cdot a_2, \cdots, \infty \cdot a_n\right\}.
    \end{displaymath}
    The number of $r$-permutations formed using the elements from $M$ is given by $n^r$.
\end{probox}


\section{Combinations}
Beside permutations, there are also occasions where we only care about which elements from a particular set are selected instead of the order of selection.

Note that if we want to find a selection of $r$ elements from a set $A$ where the order of selected elements does not matter, it is equivalent to finding a subset of $A$ containing $r$ elements. This motivates us to give the following definition:
\begin{dfnbox}{Combinations}{combinations}
    Let $A$ be a finite set such that $\abs{A} = n$, an $r$-combination of $A$ is a set $B \subseteq A$ with~$\abs{B} = r$. The number of combinations of $A$ is given by
    \begin{equation*}
        C^n_r = \frac{P^n_r}{P^r_r} = \frac{n!}{r!(n - r)!} = \begin{pmatrix}
            n \\
            r
        \end{pmatrix}.
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Two obvious results:
        \begin{enumerate}
            \item If $r > n$ or $r < 0$, $C^n_r = 0$;
            \item $C^n_r = C^n_{n - r}$ (By Theorem \ref{thm:PC}).
        \end{enumerate}
    \end{remark}
\end{notebox}
Similar to permutations, we have the following important identity:
\begin{thmbox}{Pascal's Triangle}{pascalTri}
    Let $n$ be an integer with $n \geq 2$ and let $r$ be an integer with $0 \leq r \leq n$, then
    \begin{equation*}
        C^{n + 1}_r = C^{n}_{r - 1} + C^{n}_r.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $S = \left\{x \in \N^+ \colon x \leq n + 1\right\}$ represent $(n + 1)$ distinct objects. Consider an $r$-combination $T$ of $S$:
        \\\\
        If $n + 1 \notin T$, this is equivalent to an $r$-combination of $S - \{n + 1\}$, so there are $C^n_r$ such permutations.
        \\\\
        If $n + 1 \in T$, it suffices to find an $(r - 1)$-combination of $S - \{n + 1\}$, which has $C^n_{r - 1}$ ways to do.
        \\\\
        Therefore, there are $C^n_{r} + C^n_{r - 1}$ $r$-combinations of $S$, i.e. $C^{n + 1}_r = C^n_r + C^n_{r - 1}$. 
    \end{proof}
\end{thmbox}
\subsection{Counting Subsets}
A useful application of combinations, derived directly from the definition, is to count the number of subsets for a given set which is finite. In other words, if we are given a set $A$ with $\abs{A} = n \in \N$, we wish to find a general formula for $\abs{\mathcal{P}(A)}$.

Let $A_i$ be the set of all subsets of $A$ whose cardinality is $i$, then clearly
\begin{equation*}
    \abs{\mathcal{P}(A)} = \sum_{i = 0}^{n}\abs{A_i} = \sum_{i = 0}^{n}C^n_i.
\end{equation*}
We can expand the above expression algebraically and realise that it simpifies to $2^n$. However, in a combinatorial perspective, we are able to prove this result in a more succint manner:
\begin{thmbox}{General Formula for $\abs{\mathcal{P}(A)}$}{cardPowerset}
    Let $A$ be a finite set. If $\abs{A} = n$, then $\abs{\mathcal{P}(A)} = 2^n$.
    \tcblower
    \begin{proof}
        Let $S$ be an arbitrary subset of $A$. Consider an arbitrary element $a \in A$, then either $a \in S$ or $a \notin S$. 
        \\\\
        Let $a_i \in A$ for $i = 1, 2, \cdots, n$. For all $S \in \mathcal{P}(A)$, We replace $a_i$ by $1$ if $a_i \in S$, and by $0$ otherwise. Let $B$ be the set of all binary sequences of length $n$. It is clear that there exists a bijection between $\mathcal{P}(A)$ and $B$, and so $\abs{\mathcal{P}(A)} = \abs{B}$.
        \\\\
        For each binary sequence of length $n$, each of its digits is either $0$ or $1$. By Theorem \ref{thm:MP}, this means that there are in total $2^n$ such binary sequences. Therefore,
        \begin{equation*}
            \abs{\mathcal{P}(A)} = \abs{B} = 2^n.
        \end{equation*}
    \end{proof}
\end{thmbox}

\subsection{Combinations with Repetitions}
Given a set of $n$ distinct objects, we wish to find the number of combinations taken from the set where any element is allowed to be selected for multiple times. To reason about such problems, we introduce the following notion of a \textit{multi-subset}.
\begin{dfnbox}{Multi-subset}{multisubset}
    Let $M = \left\{\infty \cdot a_1, \infty \cdot a_2, \cdots, \infty \cdot a_n\right\}$ be a multi-set where $n \in \N$. An $r$-element {\color{red} \textbf{multi-subset}} of $M$ is the set
    \begin{displaymath}
        \left\{m_1 \cdot a_1, m_2 \cdot a_2, \cdots, m_n \cdot a_n\right\}
    \end{displaymath}
    where each of the $m_i$'s is a non-negative integer such that $\sum_{i = 1}^{n}m_i = r$. We denote the number of $r$-element multi-subsets of $M$ by $H^n_r$.
\end{dfnbox}
Intuitively, we can view each of the $a_i$'s as a ``box'' which holds $m_i$ balls. Therefore, if we can find the ways to distribute a total of $r$ balls into the $n$ ``boxes'', each of the distribution will then correspond to a multi-subset of $M$!
\begin{probox}{A Formula for $H^n_r$}{hnrFormula}
    Let $M = \left\{\infty \cdot a_1, \infty \cdot a_2, \cdots, \infty \cdot a_n\right\}$, then the number of $r$-element multi-subsets of $M$ is given by
    \begin{equation*}
        H^n_r = C^{r + n - 1}_r.
    \end{equation*}
    \tcblower
    \begin{proof}
        Consider a binary string formed using $r$ zeros. Note that we can insert a number of ones into the binary string to partition the zeros into different sections. Now, suppose we insert $(n - 1)$ ones into the binary string, it will result in a binary string containing $r$ zeros and $(n - 1)$ ones, such that the zeros are partitioned into $n$ sections.
        \\\\
        We can then number the sections using $1, 2, 3, \cdots, n$. For the $i$-th section, the number of zeros is recorded as $m_i$. Notice that in this case, each of the binary strings will correspond to a multi-subset in the form of 
        \begin{displaymath}
            \left\{m_1 \cdot a_1, m_2 \cdot a_2, \cdots, m_n \cdot a_n\right\}.
        \end{displaymath}
        One can check that this will establish a bijection between the set of all multi-subsets containing $n$ distinct types of objects and the set of all binary strings with $r$ zeros and~$(n - 1)$ ones.
        \\\\
        Therefore, the above has proven that $H^n_r = C^{r + n - 1}_r$.
    \end{proof}
\end{probox}

\section{Basic Distribution Problems}
Another problem in which we are interested is to \textbf{distribute} the objects from a collection into a finite number of sub-collections. Specifically, we consider the number of ways to
\begin{enumerate}
    \item distribute $r$ distinct objects into $n$ identical collections;
    \item distribute $r$ distinct objects into $n$ distinct collections;
    \item distribute $r$ identical objects into $n$ identical collections;
    \item distribute $r$ identical objects into $n$ distinct collections.
\end{enumerate}
These $4$ basic models of distribution problems give rise to many variations. We will only discuss two simple types of distribution problems. More detailed discussion will be given in later chapters.
\subsection{Stirling Numbers of the First Kind}
\begin{dfnbox}{Stirling Numbers of the First Kind}{stirling1}
    Let $r, n \in \N$ such that $0 \leq n \leq r$, then the {\color{red} \textbf{Stirling Number of the First Kind}}, $s(r, n)$, is the number of ways to arrange $r$ {\color{red} \textbf{distinct}} onjects around $n$ {\color{red} \textbf{identical}} circles such that no circle is empty.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Some obvious results:
        \begin{enumerate}
            \item $s(r, 0) = 0$ if $r \geq 1$.
            \item $s(r, r) = 1$ if $r \geq 0$.
            \item $s(r, 1) = (r - 1)!$ if $r \geq 2$.
            \item $s(r, r - 1) = C^r_2$ if $r \geq 2$.
        \end{enumerate}
    \end{remark}
\end{notebox}
One thing to take note of is that there is no general algebraic formula for $s(r, n)$. Instead, all Stirling Numbers of the First Kind follow a recurrence relation in $2$ parameters.
\begin{thmbox}{A Recurrence Relation for $s(r, n)$}{recurrence1}
    For $r, n \in \N^+$ and $r \leq n$, we have
    \begin{equation*}
        s(r, n) = s(r - 1, n - 1) + (r - 1)s(r - 1, n).
    \end{equation*}
    \tcblower   
    \begin{proof}
        Consider the set
        \begin{displaymath}
            \left\{x_1, x_2, \cdots, x_r\right\}
        \end{displaymath}
        to be the $r$ distinct objects. We consider two cases.
        \\\\
        If $x_r$ is distributed to a circle such that it is the only objects around that circle, it suffices to find the number of ways to arrange the rest $(r - 1)$ distinct objects around the rest $(n - 1)$ identical circles, which there are $s(r - 1, n - 1)$ ways to do.
        \\\\
        If $x_r$ is adjacent to some other object, we can first arrange the rest $(r - 1)$ distinct objects around the $n$ identical circles, which there are $s(r - 1, n)$ ways to do. After that, we can choose any one of the $(r - 1)$ spaces between two adjacent objects to slot in $x_r$. Therefore, there are $(r - 1)s(r - 1, n)$ ways to distribute the objects.
        \\\\
        By Theorem \ref{thm:AP}, the total number of distributions is 
        \begin{equation*}
            s(r, n) = s(r - 1, n - 1) + (r - 1)s(r - 1, n).
        \end{equation*}
    \end{proof}
\end{thmbox}

\subsection{Distinct Objects into Distinct Collections}
The easiest case for this distribution problem is that each collection contains at most one object, i.e., each collection either contains an object, or contains no object at all.
\begin{probox}{}{}
    The number of ways to distribute $r$ distinct objects into $n$ distinct collections such that each collection contains at most $1$ object is given by $P^n_r$.
\end{probox}
We then consider the case where each collection can contain any number of objects.
\begin{probox}{}{}
    The number of ways to distribute $r$ distinct objects into $n$ distinct collections such that each collection can contain any number of objects is given by
    \begin{equation*}
        \frac{(n + r - 1)!}{(n - 1)!}.
    \end{equation*}
\end{probox}

\section{Binomial and Multinomial Coefficients}
\subsection{Binomial Coefficients}
In previous sections on combinations, we have already introduced the ways to choose $r$ items from a collection of $n$ distinct items, the number of which is given by
\begin{displaymath}
    C^n_r = \begin{pmatrix}
        n \\
        r
    \end{pmatrix}.
\end{displaymath} 
In this section, we will see that there is a clear relation between the number of $r$-combinations to the \textit{binomial expansion}.
\begin{thmbox}{Binomial Expansion}{binomial}
    Let $n \in \N$, then
    \begin{equation*}
        (x + y)^n = \sum_{i = 0}^{n}\left[\begin{pmatrix}
            n \\
            i
        \end{pmatrix}x^{n - i}y^i\right].
    \end{equation*}
    \tcblower   
    \begin{proof}
        Note that 
        \begin{equation*}
            (x + y)^n = \prod_{i = 1}^{n}(x + y).
        \end{equation*}
        Let the coefficient of $x^{n - r}y^r$ be $k$, where $r$ is an integer with $0 \leq r \leq n$. Notice that $k$ is exactly the number of ways to choose $(n - r)$ copies of $x$'s from the distinct $n$ terms of $(x + y)$, which is given by
        \begin{equation*}
            \begin{pmatrix}
                n \\
                n - r
            \end{pmatrix} = \begin{pmatrix}
                n \\
                r
            \end{pmatrix}.
        \end{equation*}
        Therefore, summing up the terms, we have
        \begin{equation*}
            (x + y)^n = \sum_{i = 0}^{n}\left[\begin{pmatrix}
                n \\
                i
            \end{pmatrix}x^{n - i}y^i\right].
        \end{equation*}
    \end{proof}
\end{thmbox}
There are many nice algebraic properties about the binomial coefficient $\left(\begin{smallmatrix}
    n \\
    r
\end{smallmatrix}\right)$. Beside Theorem \ref{thm:pascalTri}, the following can be proven with a few simple algebraic manipulations:
\begin{enumerate}
    \item $\begin{pmatrix}
        n \\
        r
    \end{pmatrix} = \frac{n}{r}\begin{pmatrix}
        n - 1 \\
        r - 1
    \end{pmatrix}$.
    \item $\begin{pmatrix}
        n \\
        r
    \end{pmatrix} = \frac{n - r + 1}{r}\begin{pmatrix}
        n \\
        r - 1
    \end{pmatrix}$.
    \item $\begin{pmatrix}
        n \\
        m
    \end{pmatrix}\begin{pmatrix}
        m \\
        r
    \end{pmatrix} = \begin{pmatrix}
        n \\
        r
    \end{pmatrix}\begin{pmatrix}
        n - r \\
        m - r
    \end{pmatrix}$.
\end{enumerate}
We will focus on two important identities about the binomial coefficients, one of which is as follows:
\begin{thmbox}{Vandermonde's Identity}{vadermonde}
    Let $m, n, r \in \N$, then
    \begin{equation*}
        \sum_{i = 0}^{r}\left[\begin{pmatrix}
            m \\
            i
        \end{pmatrix}\begin{pmatrix}
            n \\
            r - i
        \end{pmatrix}\right] = \begin{pmatrix}
            m + n \\
            r
        \end{pmatrix}.
    \end{equation*}
    \tcblower   
    \begin{proof}
        Consider the set
        \begin{displaymath}
            X = \left\{a_1, a_2, \cdots, a_m, b_1, b_2, \cdots, b_n\right\}.
        \end{displaymath}
        Clearly, $\abs{X} = m + n$. Let $A \subseteq X$ such that $\abs{A} = r$, we consider the number of such $A$'s.
        \\\\
        Let $i$ be an integer with $0 \leq i \leq r$. If $A$ contains exactly $i$ elements from the $a_j$'s, then it will contain exactly $(r - 1)$ elements from the $b_j$'s. Therefore, the number of $A$'s for each $i = 1, 2, \cdots, r$ is given by
        \begin{equation*}
            \begin{pmatrix}
                m \\
                i
            \end{pmatrix}\begin{pmatrix}
                n \\
                r - i
            \end{pmatrix}.
        \end{equation*}
        However, we know that the total number of such $A$'s is just $C^{m + n}_r$, so we have
        \begin{equation*}
            \sum_{i = 0}^{r}\left[\begin{pmatrix}
                m \\
                i
            \end{pmatrix}\begin{pmatrix}
                n \\
                r - i
            \end{pmatrix}\right] = \begin{pmatrix}
                m + n \\
                r
            \end{pmatrix}.
        \end{equation*}
    \end{proof}
\end{thmbox}
Recall that in Theorem \ref{thm:pascalTri}, we essentially established a recurrence relation for the binomial coefficients. Thus in theory, it is possible to generate any binomial coefficient recursively. The following theorem illustrates this.
\begin{thmbox}{Chu Shih-Chieh (CSC) Identity}{CSC}
    Let $r, n, k \in \N$ with $n \geq r$, then
    \begin{equation*}
        \sum_{i = 0}^{n - r}\begin{pmatrix}
            r + i \\
            r
        \end{pmatrix} = \begin{pmatrix}
            n + 1 \\
            r + 1
        \end{pmatrix}, \qquad \sum_{i = 0}^{k}\begin{pmatrix}
            r + i \\
            i
        \end{pmatrix} = \begin{pmatrix}
            r + k + 1 \\
            k
        \end{pmatrix}.
    \end{equation*}
    \tcblower
    \begin{proof}
        Consider the set
        \begin{displaymath}
            X = \left\{x \in \N^+ \colon x \leq n + 1\right\}.
        \end{displaymath}
        We will count the number of $(r + 1)$-element subsets of $X$ in the following way:
        \\\\
        Fix $k$ to be the smallest element in some $Y \subseteq X$, then it suffices to choose $r$ elements from $X - \left\{x \in \N^+ \colon x \leq k\right\}$. Note that the maximum for $k$ is $n - r + 1$, and so there are $n - r + 1$ disjoint cases. Therefore, it is easy to see that the total number of subsets is given by
        \begin{displaymath}
            \sum_{i = 0}^{n - r}\begin{pmatrix}
                r + i \\
                r
            \end{pmatrix}.
        \end{displaymath}
        However, we know that this is equivalent to choosing $(r + 1)$ elements directly from $X$, so
        \begin{equation*}
            \sum_{i = 0}^{n - r}\begin{pmatrix}
                r + i \\
                r
            \end{pmatrix} = \begin{pmatrix}
                n + 1 \\
                r + 1
            \end{pmatrix}.
        \end{equation*}
        Note that $C^{r + i}_r = C^{r + i}_i$ and $C^{n + 1}_{r + 1} = C^{n + 1}_{n - r}$. Therefore, by setting $k = n - r$, it is obvious by symmetry that
        \begin{equation*}
            \sum_{i = 0}^{k}\begin{pmatrix}
                r + i \\
                i
            \end{pmatrix} = \begin{pmatrix}
                r + k + 1 \\
                k
            \end{pmatrix}.
        \end{equation*}
    \end{proof}
\end{thmbox}
\subsection{Multinomial Coefficients}
\begin{dfnbox}{Multinomial Coefficient}{multinomialCoeff}
    Let $m, n \in \N$, with $m \geq 1$. The {\color{red} \textbf{multinomial coefficient}}
    \begin{displaymath}
        \begin{pmatrix}
            n \\
            n_1, n_2, \cdots, n_m
        \end{pmatrix} = \frac{n!}{\prod_{i = 1}^{m}n_i!}
    \end{displaymath}
    is the number of ways to distribute $n$ distinct objects into $m$ distinct collections such that there are exactly $n_i$ objects in the $i$-th collection.
\end{dfnbox}
Note that the binomial coefficient is just a special case for multinomial coefficients where $m = 2$ and $n_1 + n_2 = n$. Respectively, multinomial coefficients also generalise binomial expansion.
\begin{thmbox}{Multinomial Expansion}{multinomialEx}
    For $m, n \in \N+$, we have
    \begin{equation*}
        \left(\sum_{i = 1}^{m}x_i\right)^n = \sum_{\substack{\sum_{j = 1}^{m}n_j = n}} \begin{pmatrix}
            n \\
            n_1, n_2, \cdots, n_m
        \end{pmatrix}\prod_{i = 1}^{m}x_i^{n_i}.
    \end{equation*}
\end{thmbox}

\chapter{Pigeonhole Principle}
\section{Pigeonhole Principle}
\begin{thmbox}{Pigeonhole Principle}{PP}
    Let $k, n \in \N^+$. If at least $kn + 1$ distinct objects are distributed into $n$ distinct sets, then there exists a set $K$ such that $\abs{K} \geq k + 1$.
    \tcblower   
    \begin{proof}
        Let $a_1, a_2, \cdots, a_{kn + 1}$ be the $kn + 1$ distinct objects to be distributed into the sets~$K_1, K_2, \cdots, K_n$. Suppose there is no $i \in \N^+$ such that $\abs{K_i} \geq k + 1$, i.e. $K_i \leq k$ for all~$i = 1, 2, \cdots, n$.
        \\\\
        This means that $\sum_{i = 1}^{n}\abs{K_i} \leq kn < kn + 1$, which is a contradiction.
    \end{proof}
\end{thmbox}
Note that Pigeonhole Principle only guarantees existence rather than uniqueness. We may also consider the following generalised version:
\begin{thmbox}{Generalised Pigeonhole Principle}{GPP}
    Let $n, k_1, k_2, \cdots, k_n \in \N^+$. If at least $\left[\sum_{i = 1}^{n}k_i + (n - 1)\right]$ distinct objects are distributed into $n$ distinct sets $S_1, S_2, \cdots, S_n$, then there exists at least one $S_i$ for $i = 1, 2, \cdots, n$ such that $\abs{S_i} \geq k_i$.
    \tcblower   
    \begin{proof}
        Suppose there is no $i \in \N^+$ such that $\abs{S_i} \geq k_i$, i.e., $\abs{S_i} \leq k_i - 1$ for all $i = 1, 2, \cdots, n$
        \\\\
        This means that $\sum_{i = 1}^{n}\abs{K_i} \leq \sum_{i = 1}^{n}(k_i - 1) = \sum_{i = 1}^{n}k_i - n$, which is a contradiction.
    \end{proof}
\end{thmbox}
Note that by setting $k_1 = k_2 = \cdots = k_n = 2$ we get from Theorem \ref{thm:GPP} to Theorem \ref{thm:PP}.

The Pigeonhole Principle is closely linked to the \textit{Ramsey Types of Problems}.
\section{Ramsey Types of Problems}
The Ramsey Types of Problems are motivated by the following question statement:
\begin{exbox}{}{sixPts}
    Let $A_1, A_2, A_3, A_4, A_5, A_6$ be six arbitrary points in a plane. We join these points pairwise with either a {\color{red} \textbf{red}} line segment or a {\color{blue} \textbf{blue}} line segment. Prove that after each point has been joined with any other point, there exists at least one {\color{blue} \textbf{blue}} triangle or at least one {\color{red} \textbf{red}} triangle whose vertices are three of the six points.
    \tcblower   
    \begin{proof}
        Consider the line segments joining $A_h$ with the other five points. Note that this is equivalent to colouring $5$ line segments using either red or blue. By Theorem \ref{thm:PP}, this means that at least $3$ of these line segments have the same colour.
        \\\\
        Without loss of generality, suppose the line segments $A_hA_i$, $A_hA_j$ and $A_hA_k$ are red. Consider the line segments $A_iA_j$, $A_jA_k$ and $A_iA_k$. If they are all blue, then we will have a blue triangle. Otherwise, if any one of them is red, then those two vertices together with $A_h$ will form a red triangle.
    \end{proof}
\end{exbox}
We can extend the above question a bit. Suppose we have $17$ arbitrary points $X_1, X_2, \cdots, X_{17}$ in a plane and we join these points in the same way as Example \ref{ex:sixPts} but with three distinct colours: {\color{red} \textbf{red}}, {\color{blue} \textbf{blue}} and {\color{orange} \textbf{orange}}. Can we still guarantee the existence of at least on triangle whose sides are all of the same colour?

It turns out that we can! Again we can consider all line segments joining $X_1$ with the other $16$ vertices. By Theorem \ref{thm:PP}, we must have at least $6$ of them having the same colour. Without loss of generality, assume that these $6$ line segments are all red.

Now, examine these $6$ vertices joined to $X_1$ with the same-coloured line segments. If any line segment between these $6$ vertices is red, then we have a red triangle. Otherwise, the line segments joining these $6$ vertices are coloured with only two colours, and by Example \ref{ex:sixPts} we are guaranteed to have a triangle with all three sides having the same colour as well.

Going back to Example \ref{ex:sixPts}, one might notice that $6$ is the smallest possible number of vertices for us to guarantee the existence of such a triangle. In the case of $5$ vertices, consider:
\begin{center}
    \begin{tikzpicture}[mystyle/.style={draw,shape=circle,fill=green}]
        \def\ngon{5}
        \node[
            regular polygon,
            regular polygon sides=\ngon,
            minimum size=3cm
        ] (p) {};
        \foreach\x in {1,...,\ngon}{
            \node[mystyle] (p\x) at (p.corner \x){};
        }
        \draw [red, very thick] (p1) -- (p2) -- (p3) -- (p4) -- (p5) -- (p1);
        \draw [blue, very thick] (p1) -- (p3);
        \draw [blue, very thick] (p1) -- (p4);
        \draw [blue, very thick] (p2) -- (p4);
        \draw [blue, very thick] (p2) -- (p5);
        \draw [blue, very thick] (p3) -- (p5);
    \end{tikzpicture}
\end{center}
This motivates the following definitions:
\begin{dfnbox}{Clique}{clique}
    A {\color{red} \textbf{clique}} is defined to be a finite set of vertices such that every pair of vertices are adjacent to each other (i.e., there exists a line segment joining any two vertices). A clique with $k$ vertices is called a $k$-clique.
\end{dfnbox}
\begin{dfnbox}{Ramsey Number}{ramseyNum}
    Given $p, q \in \N^+$. The {\color{red} \textbf{Ramsey Number}} $R(p, q)$ is the smallest natural number $n$ such that for any colouring of the edges of an $n$-clique by $2$ colours, there exists either a monochromatic $p$-clique or a monochromatic $q$-clique.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Some obvious results:
        \begin{itemize}
            \item $R(p, q) = R(q, p)$.
            \item $R(1, q) = 1$.
            \item $R(2, q) = q$.
        \end{itemize}
    \end{remark}
\end{notebox}
These develop into what we now know as \textit{Ramsey Theory} which probes into the question:
\begin{quote}
    How many elements of a particular structure $S$ must there exist to guarantee that a certain property $p$ will always hold for $S$?
\end{quote}
It turns out that calculating the Ramsey Numbers can be extremely difficult. However, we do have a way to establish an upper bound for each of the Ramsey Numbers.
\begin{thmbox}{Recurrence Relation of Ramsey Numbers}{recurRamsey}
    For all integers $p, q \geq 2$,
    \begin{equation*}
        R(p, q) \leq R(p - 1, q) + R(p, q - 1).
    \end{equation*}
    In particular, if both $R(p - 1, q)$ and $R(p, q - 1)$ are even, then
    \begin{equation*}
        R(p, q) \leq R(p - 1, q) + R(p, q - 1) - 1.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $n = R(p - 1, q) + R(p, q - 1)$. To prove that $R(p, q) \leq n$, it suffices to prove that for any $n$-clique, any colouring of its edges using blue and red will result in either a blue $p$-clique or a red $q$ clique.
        \\\\
        Consider an arbitrary vertex $V$ of any $n$-clique. Note that $V$ is incident with $\left(R(p - 1, q) + R(p, q - 1) - 1\right)$ vertices and edges. By Theorem \ref{thm:GPP} this means that either at least $R(p - 1, q)$ edges are blue or at least $R(p, q - 1)$ edges are red.
        \\\\ 
        Without loss of generality, consider the $R(p - 1, q)$-clique whose vertices are joined to $V$ by blue edges. By Definition \ref{dfn:ramseyNum}, this clique contains either a blue $(p - 1)$-clique or a red $p$-clique. If it contains a red $p$-clique, then we are done. If it contains a blue $(p - 1)$-clique, this clique together with $V$ will form a blue $p$-clique. Either way, it is guaranteed that
        \begin{equation*}
            R(p, q) \leq n = R(p - 1, q) + R(p, q - 1).
        \end{equation*}
        Now we proceed to proving the case where $R(p - 1, q)$ and $R(p, q - 1)$ are both even.
        \\\\
        Let $m = R(p - 1, q) + R(p, q - 1) - 1$. Fix an arbitrary vertex $W$, then it suffices to prove that among the $(m - 1)$ edges incident to $W$, either at least $R(p - 1, q)$ are blue or at least $R(p, q - 1)$ are red.
        \\\\
        Suppose on contrary that there are exactly $\left(R(p - 1, q) - 1\right)$ blues edges and exactly $\left(R(p, q - 1) - 1\right)$ red edges for all vertices in the $m$-clique. This means that the total number of blue edges in the $m$-clique is
        \begin{equation*}
            N = \frac{m\left(R(p - 1, q) - 1\right)}{2}.
        \end{equation*}
        However, $m$ and $R(p - 1, q) - 1$ are both odd, this means that $N$ is not an integer, which is impossible.
    \end{proof}
\end{thmbox}
The above recurrence allows us to prove the following theorem:
\begin{thmbox}{Ramsey's Theorem}{RamseyThm}
    For all $p, q \in N^+$, the Ramsey Number $R(p, q)$ always exists.
\end{thmbox}

\chapter{Principle of Inclusion and Exclusion}
\section{Principle of Inclusion and Exclusion (PIE)}
Consider the following naïve question:
\begin{quote}
    If $\abs{A} = a$ and $\abs{B} = b$, what is $\abs{A \cup B}$?
\end{quote}
The solution is simple: we first count how many elements are in $A$ and then count how many elements are in $B$, but note that there are some elements which might be in both sets and are thus counted twice, so we need to subtract these elements away.
\begin{probox}{Cardinality of the Union of Two Finite Sets}{2PIE}
    Let $A$ and $B$ be two finite sets, then
    \begin{equation*}
        \abs{A \cup B} = \abs{A} + \abs{B} - \abs{A \cap B}.
    \end{equation*}
    In particular, if $A$ and $B$ are disjoint, then $\abs{A \cup B} = \abs{A} + \abs{B}$.
\end{probox}
Proposition \ref{pro:2PIE} can be generalised for a finite sequence of finite sets.
\begin{thmbox}{Principle of Inclusion and Exclusion}{PIE}
    Let $E_1, E_2, \cdots, E_n$ be a sequence of finite sets. In general, we have
    \begin{equation*}
        \abs{\bigcup_{i = 1}^n E_i} = \sum_{j = 1}^{n} \left[(-1)^{j + 1}\left(\sum_{1 \leq k_1 \leq k_2 \leq \cdots \leq k_j \leq n}\abs{\bigcap_{r = 1}^{j}E_{k_r}}\right)\right].
    \end{equation*}
    \tcblower
    \begin{proof}
        Define a function $f_S \colon S \to \{0, 1\}$ by
        \begin{equation*}
            f_S(x) = \begin{cases}
                1 \quad \textrm{if } x \in S \\
                0 \quad \textrm{if } x \notin S
            \end{cases}.
        \end{equation*}
        Let $E = \bigcup_{i = 1}^n E_i$. Consider
        \begin{equation}\label{eq:1}
            \prod_{i = 1}^{n}\left(f_E(x) - f_{E_i}(x)\right). \tag{*}
        \end{equation}
        For any $x$, if $x \in E$, then $x \in E_k$ for some $k \in \left\{x \in \N \colon x \leq n\right\}$, which means that~$f_E(x) - f_{E_k}(x) = 0$; if $x \notin E$, then $f_E(x) = f_{E_i}(x) = 0$ for all $i \in \left\{x \in \N \colon x \leq n\right\}$. In either case, (\ref{eq:1}) is identically $0$.
        \\\\
        Note that for any sets $A$, $B$, $f_{A \cap B}(x) = f_A(x)f_B(x)$. In particular, $f_{E \cap E_i}(x) = f_{E_i}(x)$ for all $i = 1, 2, \cdots, n$. Expanding (\ref{eq:1}), we have
        \begin{equation*}
            f_E(x) = \bigl[f_E(x)\bigr]^n = \sum_{j = 1}^{n} \left\{(-1)^{j + 1}\left[\sum_{1 \leq k_1 \leq k_2 \leq \cdots \leq k_j \leq n}\left(\prod_{r = 1}^{j}f_{E_{k_r}}(x)\right)\right]\right\}.
        \end{equation*}
        Summing up $f_E(x)$ over all $x$ leads to the identity in the original statement.
    \end{proof}
\end{thmbox}
We can generalise this further. Note that every set is uniquely determined by some formula~$\phi$ with some parameter $p$. This means that the statement $x \in X$ is logically equivalent to ``$x$ satisfies a property $\phi(x, p)$''. This leads to the general principle of inclusion and exclusion.
\begin{thmbox}{General Principle of Inclusion and Exclusion}{GenPIE}
    Let $S$ be a finite set with $\abs{S} = n$ and let $\left\{P_1, P_2, \cdots P_q\right\}$ be a set of properties. Suppose that~$E(m)$ is the number of elements in $S$ that satisfy exactly $m$ of the $q$ properties and that~$\omega(P_{i_1}P_{i_2}\cdots P_{i_m})$ is the number of elements in $S$ that satisfy exactly $P_{i_1}, P_{i_2},\cdots, P_{i_m}$, such that
    \begin{equation*}
        \omega(m) = \sum \omega(P_{i_1}P_{i_2}\cdots P_{i_m}),
    \end{equation*}
    then
    \begin{equation*}
        E(m) = \sum_{k = m}^{q}\left[(-1)^{k - m}\begin{pmatrix}
            k \\
            m
        \end{pmatrix}\omega(k)\right].
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $x \in S$ be an arbitrary element which satisfies exactly $t$ properties. If $t < m$ or $q < t$, then $x$ contributes a count of $0$ to both sides. If $t = m$, then $x$ contributes a count of $1$ to $E(m)$ and to $\omega(m)$ and contributes a count of $0$ to $\omega(k)$ for all $k > m$.
        \\\\
        If $q \geq t > m$, then $x$ contributes a count of $0$ to the left-hand side. On the right-hand side, $x$ is counted $\left(\begin{smallmatrix}
            t \\
            k
        \end{smallmatrix}\right)$ times for $k = m, m + 1, \cdots, t$. So the total contribution of $x$ to the right-hand side is
        \begin{align*}
            \sum_{k = m}^{t}\left[(-1)^{k - m}\begin{pmatrix}
                t \\
                k
            \end{pmatrix}\begin{pmatrix}
                k \\
                m
            \end{pmatrix}\right] & = \sum_{k = m}^{t}\left[(-1)^{k - m}\begin{pmatrix}
                t \\
                m
            \end{pmatrix}\begin{pmatrix}
                t - m\\
                k - m
            \end{pmatrix}\right] \\
            & = \sum_{k = 0}^{t - m}\left[(-1)^{k}\begin{pmatrix}
                t \\
                m
            \end{pmatrix}\begin{pmatrix}
                t - m\\
                k
            \end{pmatrix}\right] \\
            & = \begin{pmatrix}
                t \\
                m
            \end{pmatrix}\sum_{k = 0}^{t - m}\left[(-1)^{k}\begin{pmatrix}
                t - m\\
                k
            \end{pmatrix}\right] \\
            & = 0.
        \end{align*}
        Therefore, every $x \in S$ contributes an equal count to both sides of the identity, so the identity holds.
    \end{proof}
\end{thmbox}
In particular, we see that 
\begin{equation*}
    E(0) = \sum_{i = 0}^{q}\left[(-1)^i\omega(i)\right].
\end{equation*}
We will introduce some related applications of Theorem \ref{thm:GenPIE}.
\section{Stirling Numbers of the Second Kind}
\begin{dfnbox}{Stirling Number of the Second Kind}{2ndStirling}
    The {\color{red} \textbf{Stirling Number of the Second Kind}}, denoted as $S(r, n)$, is the number of ways to distribute $r$ distinct objects into $n$ indentical collections such that no collection is empty.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Some trivial results:
        \begin{itemize}
            \item $S(0, 0)$ = 1.
            \item $S(r, 0) = S(0, n) = 0$ for all $r, n \in \N^+$.
            \item $S(r, n) > 0$ for all $r, n \in \N$ with $r \geq n \geq 1$.
            \item $S(r, n) = 0$ if $n > r \geq 1$.
            \item $S(r, 1) = S(r, r) = 1$ for all $r \in \N^+$.
        \end{itemize}
    \end{remark}
\end{notebox}
Based on the above trivial results, we can use the following recurrence relation to derive $S(r, n)$ for any $r$ and $n$.
\begin{thmbox}{Recurrence Relation of $S(r, n)$}{SrnRecur}
    For all $r, n \in \N$ with $r \geq n$,
    \begin{equation*}
        S(r, n) = S(r - 1, n - 1) + nS(r - 1, n).
    \end{equation*}
    \tcblower
    \begin{proof}
        Consider the first object. If the first object is alone in a collection, it is equivalent to distributing $(r - 1)$ distinct objects into $(n - 1)$ identical collections such that no collection is empty, which can be done in $S(r - 1, n - 1)$ ways. If the first object is not alone, we first distribute the rest $(r - 1)$ distinct objects into the $n$ identical collections in $S(r - 1, n)$ ways, and then choose a collection to place the first object into in $n$ ways, which can be done in $nS(r - 1, n)$ ways. Therefore, 
        \begin{equation*}
            S(r, n) = S(r - 1, n - 1) + nS(r - 1, n).
        \end{equation*}
    \end{proof}
\end{thmbox}
The Stirling numbers of the second kind are closely related to the problem of set partition.
\begin{dfnbox}{Partition}{partition}
    Let $A$ be a set, an $n$-partition of $A$ is a set $S \subset \mathcal{P}(A)$ such that for all $S_1, S_2 \in S$ with $S_1 \neq S_2$, $S_1 \cap S_2 = \varnothing$ and $\bigcup S = A$.
\end{dfnbox}
Note that in a set, the elements are pairwise distinct. Therefore, a partition of $A$ into $i$ disjoint subsets is equivalent to a distribution of $\abs{A}$ distinct objects into $i$ identical collections. Therefore, the number of different partitions of $A$ is just
\begin{equation*}
    \sum_{i = 1}^{\abs{A}}S(\abs{A}, i).
\end{equation*}
To find an exact formula for $S(r, n)$, we consider the following proposition:
\begin{probox}{Number of Surjective Mappings}{numSurj}
    Let $F(r, n)$ where $r, n \in \N^+$ deonote the number of surjective mappings
    \begin{equation*}
        f \colon [r] - \{0\} \to [n] - \{0\},
    \end{equation*}
    then
    \begin{equation*}
        F(r, n) = \sum_{k = 0}^{n}\left[(-1)^k \begin{pmatrix}
            n \\
            k
        \end{pmatrix}(n - k)^r\right].
    \end{equation*}
    \tcblower
    \begin{proof}
        Note that $f$ is a mapping from a domain of $r$ elements to a co-domain of $n$ elements. Let $M_k$ be the set of such mappings by which at least $k$ elements in the co-domain do not have pre-images. Note that for each element in the domain, its image can be any of the remaining $(n - k)$ elements. Therefore, 
        \begin{equation*}
            \abs{M_k} = \begin{pmatrix}
                n \\
                k
            \end{pmatrix}(n - k)^r.
        \end{equation*}
        By Theorem \ref{thm:GenPIE}, 
        \begin{equation*}
            F(r, n) = \abs{\left(\bigcup_{k = 0}^nM_k\right)'} = \sum_{k = 0}^{n}\left[(-1)^k \begin{pmatrix}
                n \\
                k
            \end{pmatrix}(n - k)^r\right].
        \end{equation*}
    \end{proof}
\end{probox}
Note that $F(r, n)$ is just the number of ways to distribute $r$ distinct elements into $n$ collections where the order of the collections matters, so $F(r, n) = n!S(r, n)$. This gives rise to some corollaries, for example:
\begin{align*}
    S(r, n) =\frac{1}{n!} \sum_{k = 0}^{n}(-1)^k \begin{pmatrix}
        n \\
        k
    \end{pmatrix}(n - k)^r
\end{align*}
Since $S(r, n) = 0$ if $n > r \geq 1$, we have
\begin{equation*}
    \sum_{k = 0}^{n}(-1)^k \begin{pmatrix}
        n \\
        k
    \end{pmatrix}(n - k)^r = 0 \quad \textrm{if }n > r \geq 1.
\end{equation*}
\section{Derangement}
The concept of derangements arises from the following question:
\begin{quote}
    Given an increasing sequence of consecutive integers from $1$ to $n$ inclusive. How many permutations of the sequence are there such that the integer at index $i$ in the permuted sequence is not $i$?
\end{quote}
Here we give a rigorous definition along with a generalised notion of \textit{fixed points}.
\begin{dfnbox}{Derangement}{derangement}
    Let $N_n = [n] - \{0\}$. A {\color{red} \textbf{derangement}} of $N_n$, denoted by $D_n$, is the permutation $a_1a_2\cdots a_n$ of $N_n$ such that $a_i \neq i$ for all $i = 1, 2, \cdots, n$.
    \\\\
    In general, we also say that an $r$-permutation of $N_n$ has a {\color{red} \textbf{fixed point}} at $i$ if $a_i = i$. The number of $r$-permutations of $N_n$ with $k$ fixed points is $D(n, r, k)$.
\end{dfnbox}
\begin{thmbox}{A Formula for $D(n, r, k)$}{dnrk}
    For $n \geq r \geq k \geq -$ and $r \geq 1$,
    \begin{equation*}
        D(n, r, k) = \frac{C^r_k}{(n - r)!}\sum_{i = 0}^{r - k}(-1)^i \begin{pmatrix}
            r - k \\
            i
        \end{pmatrix}(n - k - i)!.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $a_1a_2\cdots a_r$ be a permutation and let $P_i$ be a property satisfied by a permutation with $a_i = i$, then $D(n, r, k) = E(k)$. Suppose $A$ is a permutation with at least $t$ fixed points, then the rest $(r - t)$ digits form an $(r - t)$-permutation of $(n - t)$ integers. Therefore, 
        \begin{equation*}
            \omega(t) = \begin{pmatrix}
                r \\
                t
            \end{pmatrix}\frac{(n - t)!}{(n - r)!}.
        \end{equation*}
        By Theorem \ref{thm:GenPIE},
        \begin{align*}
            D(n, r, k) & = \sum_{i = 0}^{r - k}\left[(-1)^i\begin{pmatrix}
                k + i \\
                k
            \end{pmatrix}\omega(k + i)\right] \\
            & = \sum_{i = 0}^{r - k}\left[(-1)^i\begin{pmatrix}
                k + i \\
                k
            \end{pmatrix}\begin{pmatrix}
                r \\
                k + i
            \end{pmatrix}\frac{(n - k - i)!}{(n - r)!}\right] \\
            & = \frac{1}{(n - r)!}\sum_{i = 0}^{r - k}\left[(-1)^i\begin{pmatrix}
                r \\
                k
            \end{pmatrix}\begin{pmatrix}
                r - k \\
                i
            \end{pmatrix}(n - k - i)!\right] \\
            & = \frac{C^r_k}{(n - r)!}\sum_{i = 0}^{r - k}(-1)^i \begin{pmatrix}
                r - k \\
                i
            \end{pmatrix}(n - k - i)!.
        \end{align*}
    \end{proof}
\end{thmbox}
Using Theorem \ref{thm:dnrk}, we see that the number of derangements is just
\begin{equation*}
    D_n = D(n, n, 0) = n!\sum_{i = 0}^{n}\frac{(-1)^i}{i!}.
\end{equation*}
Relating this to probability, we see that given a sequence of $n$ integers, the probability of us having a derangement is given by:
\begin{equation*}
    \lim_{n \to \infty}P(\textrm{Having a derangement}) = \lim_{n \to \infty}\frac{D_n}{n!} = \mathrm{e}^{-1}.
\end{equation*}

\section{Counting Co-prime Numbers}
In the last section, we introduce an application of the general principle of inclusion and exclusion in the context of prime numbers. In particular, we wish to investigate a standardised method to count the number of prime numbers between $2$ and $n$ inclusive.

We will first introduce an ancient algorithm discovered by Greek mathematician Eratosthenes.
\begin{tecbox}{Sieve of Eratosthenes}{sieve}
    Let $n \in \N$ and $n \geq 2$.
    \begin{itemize}
        \item Define $L$ to be the list of all integers from $2$ to $n$ inclusive, arranged in ascending order.
        \item For every $i \in L$, while $i \leq \sqrt{n}$:
        \begin{itemize}
            \item If $i$ is a prime, remove all multiples of $i$ from $L$.
        \end{itemize}
        \item All remaining integers in $L$ are primes.
    \end{itemize}
\end{tecbox}
To extend the problem further, we can instead consider counting the number of positive integers up to $n$ inclusive which are \textbf{co-prime} to $n$.
\begin{thmbox}{Euler $\varphi$-Function}{eulerPhi}
    Let $n \in \N^+$ and $\varphi(n)$ be the number of positive integers which are smaller than $n$ and co-prime to $n$. If $p_1, p_2, \cdots, p_k$ are distinct prime numbers such that
    \begin{equation*}
        n = \prod_{i = 1}^{k}p_i^{m_i}
    \end{equation*} 
    for positive $m_i$'s, then
    \begin{equation*}
        \varphi(n) = n\prod_{i = 1}^{k}\left(1 - \frac{1}{p_i}\right).
    \end{equation*}
    \tcblower
    \begin{proof}
        Let $S_t$ be the set of positive integers at most $n$ which are divisible by at least $t$ of the $k$ primes. For $t = 1, 2, \cdots, k$, consider
        \begin{equation*}
            \abs{S_t} = \sum_{i_1 < i_2 < \cdots < i_t}\left\lfloor\frac{n}{\prod_{j = 1}^{t}p_{i_j}}\right\rfloor = \sum_{i_1 < i_2 < \cdots < i_t}\frac{n}{\prod_{j = 1}^{t}p_{i_j}}
        \end{equation*}
        By Theorem \ref{thm:GenPIE},
        \begin{align*}
            \varphi(n) & = n - \abs{\left(\bigcup_{t = 1}^{k}S_t\right)} \\
            & = n\prod_{i = 1}^{k}\left(1 - \frac{1}{p_i}\right).
        \end{align*}
    \end{proof}
\end{thmbox}

\chapter{Generating Functions}
\section{Ordinary Generating Functions}
\begin{dfnbox}{Ordinary Generating Function (OGF)}{OGF}
    Let $(a_r)$ be a sequence in $\R$. The {\color{red} \textbf{ordinary generating function}} of $(a_r)$ is defined to be the series
    \begin{equation*}
        A(x) = \sum_{i = 0}^{\infty}a_ix^i.
    \end{equation*}
\end{dfnbox}
Note that the set of generating functions is a vector space. In particular, if $C(x) = A(x) + B(x)$, then $c_r = a_r + b_r$ and if $C(x) = A(x)B(x)$, then $c_r = \sum_{i = 0}^{r}a_ib_{r - i}$.
\begin{dfnbox}{Generalised Binomial Coefficients}{}
    Let $\alpha \in \R$ and $r \in \N$, then 
    \begin{equation*}
        \begin{pmatrix}
            \alpha \\
            r
        \end{pmatrix} = \frac{\prod_{i = 0}^{r - 1}(\alpha - i)}{r!}.
    \end{equation*}
\end{dfnbox}

\begin{dfnbox}{Generalised Binomial Expansion}{}
    Let $\alpha \in \R$ then 
    \begin{equation*}
        (1 \pm x)^\alpha = \sum_{r = 0}^{\infty}\begin{pmatrix}
            \alpha \\
            r
        \end{pmatrix}(\pm x)^r.
    \end{equation*}
\end{dfnbox}
\subsection{Basic Ordinary Generating Functions}
We now examine a few basic ordinary generating functions. First, consider
\begin{equation*}
    a_r = \begin{cases}
        1, & \quad\textrm{if } r = n \\
        0, & \quad\textrm{otherwise}
    \end{cases}.
\end{equation*}
It is easy to see that $A(x) = x^n$ generates this sequence.

Suppose we wish to generate the number of ways to choose $r$ objects from $n$ objects, i.e., consider
\begin{equation*}
    a_r = \begin{cases}
        \begin{pmatrix}
            n \\
            r
        \end{pmatrix}, & \quad\textrm{if } 0 \leq r \leq n \\
        0, & \quad\textrm{otherwise}
    \end{cases}.
\end{equation*}
Note that the generating function is just the binomial expansion
\begin{equation*}
    (1 + x)^n = \sum_{i = 0}^{n}\begin{pmatrix}
        n \\
        i
    \end{pmatrix}x^i.
\end{equation*}
Now consider a special case where $a_r = 1$ forms a constant sequence. In this case we have
\begin{equation*}
    A(x) = \sum_{i = 0}^{\infty}x^i = \frac{1}{1 - x}.
\end{equation*}
More generally, one can verify with generalised binomial expansion that the generating function for $\left(1, k, k^2, \cdots\right)$ is $\frac{1}{1 - kx}$.

Notice that
\begin{equation*}
    (1 - x)^{-n} = \sum_{i = 0}^{\infty}\begin{pmatrix}
        n - 1 + i \\
        i
    \end{pmatrix}x^i.
\end{equation*}
This means that the series $(1 - x)^{-n}$ generates the number of ways to choose $r$ objects from $(n - 1 + r)$ objects. In particular, if $a_r = r + 1$, note that $a_r = C^{r + 1}_r$, so substituting $n = 2$ yields
\begin{equation*}
    A(x) = (1 - x)^{-2} = \frac{1}{(1 - x)^2}
\end{equation*}
as the generating function for $(a_r)$.
\subsection{Operations on Ordinary Generating Functions}
Note that an ordinary generating function is an infinite polynomial, so the set of all ordinary generating functions form a vector space. 

Additionally, the following conclusions hold:
\begin{enumerate}
    \item For any $\alpha, \beta \in \R$, $C(x) = \alpha A(x) + \beta B(x)$ generates $c_r = \alpha a_r + \beta b_r$.
    \item $C(x) = A(x)B(x)$ generates $c_r = \sum_{i = 0}^{r}a_ib_{r - i}$.
    \item For $m \in \N^+$, $C(x) = x^mA(x)$ generates 
    \begin{equation*}
        c_r = \begin{cases}
            0, & \quad\textrm{if } 0 \leq r \leq m - 1 \\
            a_{r - m}, & \quad\textrm{if } r \geq m
        \end{cases}.
    \end{equation*}
    \item $C(x) = A(kx)$ generates $c_r = k^ra_r$.
    \item $C(x) = (1 - x)A(x)$ generates
    \begin{equation*}
        c_r = \begin{cases}
            a_0, & \quad\textrm{if } r = 0 \\
            a_r - a_{r - 1} & \quad\textrm{if } r \geq 1
        \end{cases}.
    \end{equation*}
    \item $C(x) = \frac{A(x)}{1 - x}$ generates $c_r = \sum_{i = 0}^{r}a_i$.
    \item $C(x) = A'(x)$ generates $c_r = (r + 1)a_{r + 1}$.
    \item $C(x) = xA'(x)$ generates $c_r = ra_r$.
\end{enumerate}
\subsection{Modelling with Generating Functions}
\begin{tecbox}{\small Modelling Combinations with Ordinary Generating Functions}{}
    Let $S = \left\{s_1, s_2, \cdots, s_n\right\}$ be $n$ distinct objects and let $a_r$ be the number of ways to select $r$ objects from $S$.
    \\\\
    To model this problem using ordinary generating functions, let $x^0$ denote the event that $s_i$ is not selected and $s_ix$ denote the event that $s_i$ is selected. Consider
    \begin{equation*}
        \prod_{i = 1}^{n}(1 + s_ix).
    \end{equation*}
    Note that the number of $r$-combinations is essentially the number of terms in the coefficient of $x^r$ in the expansion. Therefore, setting $s_i = 1$ for $i = 1, 2, \cdots, n$, the generating function for $(a_r)$ is
    \begin{equation*}
        (1 + x)^n = \sum_{i = 0}^{n}\begin{pmatrix}
            n \\
            i
        \end{pmatrix}x^i,
    \end{equation*}
    which is essentially the binomial expansion.
\end{tecbox}
While the above modelling seems an over-kill, it becomes more useful when dealing with multi-sets. Suppose $S$ is now a multi-set instead with $s_i$ having $c_i$ copies. Then, we can let $s_i^jx^j$ denote the event that $s_i$ is selected $j$ times. Following a similar process we can obtain the corresponding generating function.

In general, let 
\begin{equation*}
    M = \left\{n_1 \cdot b_1, n_2 \cdot b_2, \cdots, n_k \cdot b_k\right\}
\end{equation*}
be a multi-set and let $a_r$ be the number of ways to select $r$ objects from $M$. The generating function of $(a_r)$ is
\begin{equation*}
    A(x) = \prod_{i = 1}^{k}\left(\sum_{j = 0}^{n_i}x^j\right).
\end{equation*}
The above method enables us to also model distribution problems of $n$ identical objects into $k$ distinct collections. 

However, the above method will not work if the collections are also identical. Let $x^i$ denote a collection which contains $i$ of the identical objects and let $a_r$ denote the number of ways to distribute $n$ identical objects into $r$ non-empty identical collections of size at most $k$, then $\left(x^i\right)^j$ denotes the event that there are $j$ collections of size $i$ in the distribution. As such, the generating function for $(a_r)$ is
\begin{equation*}
    A(x) = \prod_{i = 1}^{k}\left(\sum_{j = 0}^{\infty}x^{ij}\right) = \frac{1}{\prod_{i = 1}^{k}(1 - x^i)}.
\end{equation*}
This allows us to model distribution problems with various constriants.

\subsection{Ferrers Diagram}
A common application of distribution problems with $n$ identical objects and $m$ identical collections is the partition of an integer.
\begin{dfnbox}{Integer Partition}{intPart}
    Let $n \in \N^+$. A {\color{red} \textbf{partition}} of $n$ is a non-increasing sequence of positive integers whose sum is $n$. 
\end{dfnbox}
A useful tool to study such partitions is \textit{Ferrers diagram}.
\begin{dfnbox}{Ferrers Diagram}{ferrers}
    Let $n \in \N^+$ and consider a partition $P = \left\{n_1, n_2, \cdots, n_k\right\}$. The {\color{red} \textbf{Ferrers diagram}} of the partition, denoted as $\mathcal{F}(P)$, is an array of left-justified asterisks in which the $i$-th row has the length of $n_i$.
\end{dfnbox}
Just like matrices, a Ferrers diagram can be transposed.
\begin{dfnbox}{Conjugate Partition}{conjPart}
    Let $\mathcal{F}(P)$ be the Ferrers diagram of an integer partition $P$. $Q$ is known as the {\color{red} \textbf{conjugate partition}} of $P$ if $\mathcal{F}(Q) = \mathcal{F}(P)^{\mathrm{T}}$.
\end{dfnbox}
Intuitively, conjugate partition is a \textbf{symmetric relation}, and if $P$ and $Q$ are conjugate partitions, the number of parts of $Q$ is just the size of the largest part of $P$. This leads to the following theorem:
\begin{thmbox}{Size of Conjugate Partitions}{sizeConj}
    Let $k \leq r \in \N^+$. The number of partitions of $r$ into $k$ parts equals the number of partitions of $r$ with the largest part having size $k$.
    \tcblower   
    \begin{proof}
        Let $\mathcal{P}$ be the family of partitions of $r$ into $k$ parts and $\mathcal{Q}$ be the family of partitions of $r$ whose largest part has size $k$.
        \\\\
        Define $f \colon \mathcal{P} \to \mathcal{Q}$ such that $\mathcal{F}\bigl(f(X)\bigr) = \mathcal{F}(X)^{\mathrm{T}}$, i.e., $f(X)$ and $X$ are conjugates.
        \\\\
        It is easy to see that $f$ is well-defined and bijective, so $\abs{\mathcal{P}} = \abs{\mathcal{Q}}$.
    \end{proof}
\end{thmbox}
Theorem \ref{thm:sizeConj} leads to the following corollary:
\begin{corbox}{}{}
    Let $k \leq r \in \N^+$. The number of partitions of $r$ into at most $k$ parts equals the number of partitions of $r$ with the largest part having size at most $k$.
\end{corbox}
The following formula can de derived:
\begin{equation*}
    \sum_{k = 1}^{m}p(n, k) = p(n + m, m).
\end{equation*}
\section{Exponential Generating Functions}
\begin{dfnbox}{Exponential Generating Function (EGF)}
    Let $(a_r)$ be a sequence in $\R$. The {\color{red} \textbf{exponential generating function}} of $(a_r)$ is defined to be the series
    \begin{equation*}
        A(x) = \sum_{i = 0}^{\infty}a_i\frac{x^i}{i!}.
    \end{equation*}
\end{dfnbox}
Here we introduce three basic exponential generating functions:
\begin{enumerate}
    \item $a_r = 1$: $A(x) = \sum_{i = 0}^{\infty}\frac{x^i}{i!} = \e^x$.
    \item $a_r = r!$: $A(x) = \sum_{i = 0}^{\infty}x_i = \frac{1}{1 - x}$.
    \item $a_r = k^r$ for $k \neq 0$: $A(x) = \sum_{i = 0}^{\infty}\frac{(kx)^i}{i!} = \e^{kx}$.
\end{enumerate}
Let $a_r = P^n_r$ be the number of $r$-permutations of $n$ distinct objects. The exponential generating function for $(a_r)$ is just 
\begin{align*}
    A(x) & = \sum_{i = 0}^{n}P^n_r\frac{x^i}{i!} \\
    & = \sum_{i = 0}^{n}i!C^n_r\frac{x^i}{i!} \\
    & = \sum_{i = 0}^{n}C^n_rx^i \\
    & = (1 + x)^n.
\end{align*}
In general, let 
\begin{equation*}
    M = \left\{n_1 \cdot b_1, n_2 \cdot b_2, \cdots, n_k \cdot b_k\right\}
\end{equation*}
be a multi-set and let $a_r$ be the number of $r$-permutations of $M$. The generating function of $(a_r)$ is
\begin{equation*}
    A(x) = \prod_{i = 1}^{k}\left(\sum_{j = 0}^{n_i}\frac{x^j}{j!}\right).
\end{equation*}

\chapter{Recurrence Relations}
In this chapter, we will study how to utilise sequences, particularly recurrence relations, to help solve some counting problems. 
\section{Recurrence Relations}
A recurrence relation, informally, is an equation that relates the $n$-th term of a sequence to its previous $k$ terms.
\begin{dfnbox}{Recurrence Relation}{RR}
    Let $(a_n)$ be a sequence. A {\color{red} \textbf{$k$-th order recurrence relation}} on $(a_n)$ is defined to be the equation
    \begin{equation*}
        a_n = f(a_{n - 1}, a_{n - 2}, \cdots, a_{n - k})
    \end{equation*}
    for some $k \in \Z$. An expression $a_n = g(n)$ which satisfies the recurrence relation for all $n$ is known as the {\color{red} \textbf{solution}}. 
\end{dfnbox}
Note that to explicitly define a sequence, we need to define the first $k$ terms of the sequence given a recurrence relation. These are known as the \textit{initial condition}.

We will first study a most basic type of recurrence relations --- \textit{linear homogeneous} recurrecne relations.
\begin{dfnbox}{Linear Homogeneous Recurrence Relations}{linHomoRR}
    A {\color{red} \textbf{$k$-th order linear homogeneous recurrence relation}} of a sequence $(a_n)$ is defined as
    \begin{equation*}
        \sum_{i = 0}^{k}c_ia_{n - i} = 0
    \end{equation*}
    where $c_0, c_k \neq 0$.
\end{dfnbox}
A \textit{characteristic equation} of a recurrence relation of order $k$ is obtained by replacing $a_{n - i}$ with $x^{k - i}$. In the linear homogeneous case, the characteristic equation is 
\begin{equation*}
    \sum_{i = 0}^{k}c_ix^{k - i} = 0.
\end{equation*} 
Let $\alpha_1, \alpha_2, \cdots, \alpha_p$ be the distinct roots to the equation with multiplicity $m_1, m_2, \cdots, m_p$ respectively, then 
\begin{equation*}
    a_n = \sum_{i = 1}^{p}\left[\left(\sum_{j = 1}^{m_i}A_{ij}n^{j - 1}\right)\alpha_i^n\right]
\end{equation*}
is the general solution to the recurrence relation, where the $A_{ij}$'s are constants. We can further generalise this to a non-homogeneous case.
\begin{dfnbox}{General Linear Recurrence Relation}{genLinRR}
    A {\color{red} \textbf{$k$-th order linear recurrence relation}} of a sequence $(a_n)$ is defined as
    \begin{equation*}
        \sum_{i = 0}^{k}c_ia_{n - i} = f(n)
    \end{equation*}
    where $c_0, c_k \neq 0$ and $f$ is a function in $n$.
\end{dfnbox}
To solve a general linear recurrence relation, we first introduce the notion of a \textit{particular solution}.
\begin{dfnbox}{Particular Solution}{particularSoln}
    Let $a_n = r(a_{n - 1}, a_{n - 2}, \cdots, a_{n - k})$ be a recurrence relation. A {\color{red} \textbf{particular solution}} is defined as a function $p(n)$ that satisfies the recurrence relation.
\end{dfnbox}
It turns out that to solve a general linear recurrence relation, we can first convert it to the homogeneous case and find its solution known as the \textit{complementary solution}. Augmenting the complementary solution with a particular solution will yield the general solution!
\begin{thmbox}{Solution of Linear Recurrence Relations}{linRRSoln}
    Let
    \begin{equation*}
        \sum_{i = 0}^{k}c_ia_{n - i} = f(n)
    \end{equation*}
    be a linear recurrence relation. Suppose $p(n)$ is a particular solution and $c(n)$ is the solution to the homogeneous recurrence relation
    \begin{equation*}
        \sum_{i = 0}^{k}c_ia_{n - i} = 0,
    \end{equation*}
    then $a_n = p(n) + c(n)$ is the general solution to the original linear recurrence relation.
    \tcblower
    \begin{proof}
        Since $p(n)$ is a particular solution, we have 
        \begin{equation*}
            \sum_{i = 0}^{k}c_ip(n - i) = f(n).
        \end{equation*}
        Furthermore, we have
        \begin{equation*}
            \sum_{i = 0}^{k}c_ic(n - i) = 0.
        \end{equation*}
        Therefore,
        \begin{align*}
            \sum_{i = 0}^{k}c_i\bigl(p(n - i) + c(n - i)\bigr) = f(n),
        \end{align*}
        which means $a_n = p(n) + c(n)$ is the general solution to the original linear recurrence relation.
    \end{proof}
\end{thmbox}
There is no general way to derive the particular solution of a recurrence relation. Under some special cases, however, we can make a reasonable guess of the particular solution based on the form of $f(n)$. This is summarised as follows
\begin{center}
    \begin{tabular}{|c|l|}
        \hline
        $f(n)$ &  $a_n^{(p)}$ \\
        \hline
        $Ak^n$ & $\begin{cases}
            Bk^n & \quad\textrm{if } k \textrm{ is not a characteristic root} \\
            Bn^mk^n & \quad\textrm{if } k \textrm{ is a characteristic root with multiplicity } m
        \end{cases}$ \\
        \hline
        $\sum_{i = 0}^{t}p_in^i$ & $\begin{cases}
            \sum_{i = 0}^{t}q_in^i & \quad\textrm{if } 1 \textrm{ is not a characteristic root} \\
            n^m\sum_{i = 0}^{t}p_in^i & \quad\textrm{if } 1 \textrm{ is a characteristic root with multiplicity } m
        \end{cases}$ \\
        \hline
        $An^tk^n$ & $\begin{cases}
            \left(\sum_{i = 0}^{t}q_in^i\right)k^n & \quad\textrm{if } k \textrm{ is not a characteristic root} \\
            n^m\left(\sum_{i = 0}^{t}p_in^i\right)k^n & \quad\textrm{if } k \textrm{ is a characteristic root with multiplicity } m
        \end{cases}$ \\
        \hline
    \end{tabular}
\end{center}
\section{System of Recurrence Relations}
Just like systems of equations, we can have two recurrence relations on sequences $(a_n)$ and $(b_n)$, which form a system of equations. We can solve the system by substitution to solve one of the recurrence relation first, just like solving a system of equations.
\section{Recurrence Relations and Generating Functions}
Note that for a sequence $(a_n)$, we can define
\begin{equation*}
    A(x) = \sum_{i = 0}^{\infty}a_ix^i
\end{equation*}
as the generating function. Recall that we can transform $A(x)$ to generate a linear combination of $a_i$'s. Therefore, we could use generating functions to solve a recurrence relation.

\chapter{Graph Theory}
\section{Graph Structures}
\subsection{Multigraph}
Intuitively, we would describe a \textit{graph} as a collection of nodes (or vertices) plus some lines (or edges) joining some nodes together. This can be rigorously defined as follows:
\begin{dfnbox}{Multigraph}{multigraph}
    A {\color{red} \textbf{multigraph}} $G$ consists of a non-empty finite set of vertices denoted by $V(G)$ and a finite set of edges denoted by $E(G)$. $\abs{V(G)}$ is known as the {\color{red} \textbf{order}} of $G$, denoted by~$v(G)$ and $\abs{E(G)}$ is known as the {\color{red} \textbf{size}} of $G$, denoted by $e(G)$.
    \\\\
    In particular, if $v(G) = m$ and $e(G) = n$, we say that $G$ is an {\color{red} \textbf{$(m, n)$-graph}}.
    \\\\
    $G$ is said to be {\color{red} \textbf{trivial}} if $v(G) = 1$ and {\color{red} \textbf{non-trivial}} otherwise.
\end{dfnbox}
Note that in a multigraph, by default there can be a plural number of edges between any two vertices. We would define the notion of \textit{simple graph} as a special multigraph.
\begin{dfnbox}{Simple Graph}{simpleGraph}
    A multigraph $G$ is said to be {\color{red} \textbf{simple}} if there is at most one edge between any two distinct vertices.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        Note that if $G$ is simple and undirected, then
        \begin{displaymath}
            E(G) \subseteq \left\{(v_j, v_i) \colon v_j, v_i \in V(G), j \geq i\right\}.
        \end{displaymath}
    \end{remark}
\end{notebox}
Notice that if $(v_i, v_i)$ is an edge, it connects a vertex to itself. This is known as a \textit{loop}.

In a graph, it is important to know the layout of the vertices and edges. For this purpose, we define the notion of \textit{adjacency} in a multigraph.
\begin{dfnbox}{Adjacency and Neighbourhood}{adjacent}
    Let $v_i, v_j \in V(G)$, we say that they are {\color{red} \textbf{adjacent}} if $v_iv_j \in E(G)$. Alternatively, we say that $v_i$ and $v_j$ are {\color{red} \textbf{neighbours}} to each other. The edge $v_iv_j$ is said to be {\color{red} \textbf{incident}} with~$v_i$ and $v_j$. Two edges $e$ and $f$ are said to be {\color{red} \textbf{adjacent}} if there exists some~$v \in V(G)$ such that both $e$ and $f$ are incident with $v$. 
    \\\\
    The set of all neighbours to some $v_i \in V(G)$ is called the {\color{red} \textbf{neighbourhood set}} of $v_i$, denoted by $N_G(v_i)$. In particular, the set $N_G[v_i] \coloneqq N_G(v_i)\cup\{v_i\}$ is known as the {\color{red} \textbf{closed neighbourhood set}} of $v_i$.
\end{dfnbox}
We can further add on to the definition by discussing the size of the neighbourhood of a vertex.
\begin{dfnbox}{Degree}{deg}
    The {\color{red} \textbf{degree}} of $v$, denoted by $d_G(v)$ is defined as the number of edges incident to $v$. If~$d_G(v)$ is even (repectively, odd), then we say that $v$ is an even (respectively, odd) vertex. If $d_G(v) = 0$, we say that $v$ is {\color{red} \textbf{isolated}}; if $d_G(v) = 1$, we say that $v$ is an {\color{red} \textbf{end}} vertex. 
    \\\\
    In particular, we define
    \begin{equation*}
        \Delta(G) = \max_{v \in G}d_G(v), \qquad \delta(G) = \min_{v \in G}d_G(v).
    \end{equation*}
\end{dfnbox}
With the notion of degree established, we can now define a \textit{regular graph}.
\begin{dfnbox}{Regular Graph}{regGraph}
    A {\color{red} \textbf{regular graph}} is a graph in which every vertex has the same degree. In particular, if $d_G(v) = k$ for all $v \in V(G)$, $G$ is known as a {\color{red} \textbf{$k$-regular graph}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        A graph $G$ is regular if and only if $\Delta(G) = \delta(G)$.
    \end{remark}
\end{notebox}
Since a graph essentially consists of two sets, it is natural to consider the notion of graph complementation. We now proceed to introducing the notion of \textit{complement}.
\begin{dfnbox}{Complement}{comp}
    Let $G$ be a graph of order $n$, the {\color{red} \textbf{complement}} of $G$, denoted by $\overline{G}$, is the graph of order $n$ where
    \begin{equation*}
        V\left(\overline{G}\right) = V(G), \qquad E\left(\overline{G}\right) = \left\{(u, v) \colon (u, v) \notin E(G)\right\}.
    \end{equation*}
\end{dfnbox}
\subsection{Handshaking Lemma}
In this section, we discuss the following interesting question:
\begin{quote}
    $15$ students went to a party. During the party some of them shook hands with each other. At the end of the party, the number of handshakes made by each student was recorded and it was reported that the sum was $39$. Was this possible?
\end{quote}
Note that if we represent each student as a vertex, then we can use $V(G) = \left\{v_1, v_2, \cdots, v_{15}\right\}$ to construct a graph, in which an edge $v_iv_j \in E(G)$ if and only if students $i$ and $j$ shook hands. As such, $d_G(v_i)$ is the number of persons student $i$ shook hands with.

Note that if we sum up $d_G(v_i)$ for all the vertices, every edge will be counted exactly twice! This means that
\begin{equation*}
    \sum_{i = 1}^{15}d_G(v_i) = 39
\end{equation*}
is impossible since the left-hand side must be even. In fact, by the above reasoning, we see that in any graph, the sum of degrees of its vertices must be even.
\begin{lembox}{Handshaking Lemma}{handshake}
    If $G$ is a graph of order $n$ and size $m$, then
    \begin{equation*}
        \sum_{i = 1}^{n}d_G(v_i) = 2m.
    \end{equation*}
\end{lembox}
\begin{notebox}
    \begin{remark}
        It can be easily deduced from the above lemma that in any graph, the number of vertices with odd degrees must be even.
    \end{remark}
\end{notebox}
As an extension of Lemma \ref{lem:handshake}, the minimum size of any graph is obviously $0$, and the maximum size of a simple graph occurs when there is an edge between any two vertices.
\begin{dfnbox}{Empty and Complete Graphs}{empCom}
    Let $G$ be a simple graph of order $n$. $G$ is said to be an {\color{red} \textbf{empty graph}} or {\color{red} \textbf{null graph}}, denoted by $0_n$, if $e(G) = 0$, and a {\color{red} \textbf{complete graph}}, denoted by $K_n$ if for all $u, v \in V(G)$, we have $(u, v) \in E(G)$. 
\end{dfnbox}
\subsection{Subgraph}
Since a multigraph is just two sets, we can define a ``subset'' relation between graphs.
\begin{dfnbox}{Subgraph}{subgraph}
    Let $G, H$ be graphs, then $H$ is a {\color{red} \textbf{subgraph}} of $G$ if $V(H) \subseteq V(G)$ and $E(H) \subseteq E(G)$. In particular, a subgraph $H$ is a {\color{red} \textbf{proper subgraph}} of $G$ if $V(H) \neq V(G)$ or $E(H) \neq E(G)$.
\end{dfnbox}
Note that we a graph can possibly be reproduced by connecting its vertices correctly, so a subgraph containing all vertices of a graph ``spans'' the original graph.
\begin{dfnbox}{Spanning Subgraph}{spanSubgraph}
    Let $H$ be a subgraph of $G$. $H$ is called a {\color{red} \textbf{spanning subgraph}} of $G$ if $V(H) = V(G)$.
\end{dfnbox}
Note that by definition, a spanning subgraph retains all vertices of the original graph. Therefore, a way to quickly generate a subgraph of a given graph $G$ is to keep the vertex set and delete some edges from the edge set. We denote such a graph by $H = G - F$ for some $F \subset E(G)$. Observe that 
\begin{equation*}
    V(H) = V(G), \qquad E(H) = E(G) - F.
\end{equation*}
On the other hand, by deleting some vertices together with edges incident to them, we can produce a subgraph from any given graph.
\begin{dfnbox}{Induced Subgraph}{induceSubgraph}
    Let $H$ be a subgraph of $G$. $H$ is called a {\color{red} \textbf{induced subgraph}} of $G$ if
    \begin{equation*}
        E(H) = \left\{uv \in E(G) \colon u, v \in V(H)\right\}.
    \end{equation*}
    Let $S \subseteq V(G)$, the subgraph of $G$ induced by $S$ is denoted by $[S]$.
    \\\\
    Alternatively, let $F \subseteq E(G)$. Define
    \begin{equation*}
        V' \coloneqq \bigcup \bigl\{\left\{u, v\right\} \colon uv \in F\bigr\},
    \end{equation*}
    then $(V', F)$ is the subgraph of $G$ induced by $F$, denoted by $G[F]$.
\end{dfnbox}
Intuitively, an induced subgraph pf $G$ consists of a selected subset of the vertices of $G$ together with all edges in $G$ connecting any vertices in this subset.

Using the idea of deletion, we can quickly generate an induced subgraph $H$ of $G$ as follows: first, set $V(H) = V(G) - A$ for some $A \subseteq V(G)$, i.e., remove some vertices; then, we will remove all edges from $E(G)$ which are incident to some vertices in $A$, i.e., set
\begin{equation*}
    E(H) = \left\{e \in E(G) \colon e \textrm{ is not incident to any } v \in A\right\}.
\end{equation*}
Then, $H = \bigl(V(H), E(H)\bigr)$ is an induced subgraph of $G$. In fact, we can denote $H$ as
\begin{equation*}
    H = G - A = [V(G) - A].
\end{equation*}
This leads to the following proposition:
\begin{probox}{}{}
    Let $G$ be a graph. If $A \subseteq V(G)$, then $G - A = [V(G) - A]$. Suppose $H$ is a subgraph of $G$, then $H$ is an induced subgraph of $G$ if and only if $H = G - \bigl(V(G) - V(H)\bigr)$.
\end{probox}
\subsection{Graph Isomorphism}
Given two graphs $G$ and $H$, are they the same graph? This seemingly innocent question proves to be extremely hard to answer. Two graphs can look drastically different but be structurally identical in reality. For example, we could shift around the vertices of a graph without changing any edge to alter the shape of the graph dramatically. Therefore, to compare the structures of graphs, we require some rigorous definition.
\begin{dfnbox}{Graph Isomorphism}{graphIsomorph}
    Two graphs $G$ and $H$ are said to be {\color{red} \textbf{isomorphic}}, denoted by $G \cong H$, if there exists a bijection $f \colon V(G) \to V(H)$ such that
    \begin{equation*}
        uv \in E(G) \quad\textrm{ if and only if } f(u)f(v) \in E(H).
    \end{equation*}
\end{dfnbox}
Such a bijection $f$ is said to \textbf{preserve adjacency}, i.e., if $u$ and $v$ are neighbours in $G$, then their images are also neighbours in $H$.

It is very hard to determine whether two specific graphs are isomorphic, but there are some considerations in the general case. For example, some trivial conclusions include:
\begin{itemize}
    \item Two graphs with different orders cannot be isomorphic.
    \item Two graphs with different sizes cannot be isomorphic.
    \item Two graphs with different numbers of components cannot be isomorphic.
    \item If the numbers of vertices with degree $k$ are different in two graphs, they cannot be isomorphic.
\end{itemize}
Notice that by Definition \ref{dfn:graphIsomorph}, we can define a function $g$ such that $uv \notin E(G)$ if and only if $g(u)g(v) \notin E(H)$ and relate this function to the complement graphs of $G$ and $H$. Here we introduce a way to determine isomorphism by considering complement graphs, the proof of which is left to the reader as an exercise.
\begin{thmbox}{Complementation Preserves Isomorphism}{compIso}
    Let $G$ and $H$ be two graphs of the same order. $G \cong H$ if and only if $\overline{G} \cong \overline{H}$.
\end{thmbox}
Now let us think the reverse: if it is not easy to prove isomorphism, can we find a way to quickly determine that two graphs are not isomorphic? Here we present the necessary conditions for isomorphism:
\begin{thmbox}{Necessary Conditions for Isomorphism}{necessaryCondIso}
    If $G \cong H$, then
    \begin{enumerate}
        \item $G$ and $H$ must have the same order and size.
        \item $\delta(G) = \delta(H)$ and $\Delta(G) = \Delta(H)$.
        \item The number of vertices with degree $i$ in $G$ and $H$ is the same for all $i \in \N$.
    \end{enumerate}
\end{thmbox}
The first two conditions are easy to observe. For the third condition, we introduce a tool known as \textit{degree sequences}.
\begin{dfnbox}{Degree Sequence}{degSeq}
    Let $G$ be a graph of order $n$. If we label its vertices by $v_1, v_2, \cdots, v_n$ such that
    \begin{equation*}
        d(v_1) \geq d(v_2) \geq \cdots \geq d(v_n),
    \end{equation*}
    then the non-increasing sequence $\bigl(d(v_n)\bigr)$ is known as the {\color{red} \textbf{degree sequence}} of $G$.
\end{dfnbox}
Now we consider the following question:
\begin{quote}
    Let $(d_n)$ be a non-increasing sequence, is there some graph whose degree sequence is $(d_n)$?
\end{quote}
We will make use of the following definition:
\begin{dfnbox}{Graphic Sequence}{graphicSeq}
    Let $(d_n)$ be a sequence of non-negative integers at most $n - 1$. $(d_n)$ is said to be {\color{red} \textbf{graphic}} if there exists a graph $G$ whose degree sequence is $(d_n)$.
\end{dfnbox}
To determine whether a sequence is graphic by eye power is difficult. Fortunately, we have the following recursive approach:
\begin{thmbox}{Graphic Sequence Characterisation}{graphicSeqChar}
    Let $(d_n)$ be a non-increasing sequence of non-negative integers at most $n - 1$. Define
    \begin{equation*}
        d^*_m = \begin{cases}
            d_{m + 1} - 1 & \quad\textrm{if } 1 \leq m \leq d_1 \\
            d_{m + 1} & \quad\textrm{if } d_1 + 1 \leq m \leq n - 1
        \end{cases}.
    \end{equation*}
    $(d_n)$ is graphic if and only if $\left(d^*_m\right)$ is graphic.
    \tcblower
    \begin{proof}
        Suppose $(d_n)$ is graphic,we consider the following lemma:
        \begin{lembox}{}{graphicLemma}
            For any graphic sequence $(d_n)$, there is some graph $G$ of order $n$ with $d_G(v_i) = d_i$ for $i = 1, 2, \cdots, n$ such that $v_1$ is adjacent to $v_j$ for $j = 2, 3, \cdots, d_1 + 1$.
            \tcblower
            \begin{proof}
                Suppose on contrary there is no such a graph $G$, then for any graph $G$ with degree sequence $(d_n)$, $v_1$ is adjacent to at most $d_1 - 1$ vertices in
                \begin{equation*}
                    A \coloneqq \left\{v_j \colon j = 2, 3, \cdots, d_1 + 1\right\}.
                \end{equation*}
                Thus, there exists some $v_j \in A$ such that $v_1v_j \notin E(G)$. However,~$d_G(v_1) = d_1$, so there exists some $v_k \in V(G) - A$ with $v_k \neq v_1$ such that $v_1v_k \in E(G)$.
                \\\\
                Notice that since $v_k \notin A \cup \{v_1\}$, $k > j$ and so $d_G(v_k) = d_k \leq d_j = d_G(v_j)$. Therefore, $v_j$ has at least as many neighbours as $v_k$, which means there must exists some $v_r$ with $v_jv_r \in E(G)$ such that $v_kv_r \notin E(G)$.
                \\\\
                Define a graph $G'$ by
                \begin{equation*}
                    V(G') = V(G), \qquad E(G') = E(G) - v_jv_r - v_1v_k + v_jv_k + v_1v_j.
                \end{equation*}
                Note that $d_{G'}(v_i) = d_G(v_i)$ for all $i = 1, 2, \cdots, n$, so $(d_n)$ is also a degree sequence for $G'$. However, now $v_1$ in $G'$ is adjacent to all vertices in $A$, which is a contradiction.
            \end{proof}
        \end{lembox}
        By Lemma \ref{lem:graphicLemma}, we can choose some graph $G$ whose degree sequence is $(d_n)$ such that $v_1$ is adjacent to $d_1$ vertices $v_2, v_3, \cdots, v_{d_1 + 1}$. Let $H = G - v_1$, then for any $u_i \in H$,
        \begin{equation*}
            d_H(u_i) = \begin{cases}
                d_G(v_{i + 1}) - 1 & \quad\textrm{if } 1 \leq i \leq d_1 \\
                d_G(v_{i + 1}) & \quad\textrm{if } d_1 + 1 \leq i \leq n - 1
            \end{cases}.
        \end{equation*}
        Therefore, $\left(d^*_m\right)$ is graphic.
        \\\\
        Conversely, suppose $\left(d^*_m\right)$ is graphic, then there is some graph $H$ of order $n - 1$ such that $d_H(u_i) = d^*_i$ for $i = 1, 2, \cdots, d_1$. Construct a graph $G = H + v_1$ such that~$v_i = u_{i - 1}$ for $i = 2, 3, \cdots, n$ and $v_1$ is adjacent to $v_2, v_3, \cdots, v_{d_1 + 1}$. Therefore,
        \begin{equation*}
            d_G(v_i) = \begin{cases}
                d_1 & \quad\textrm{if } i = 1 \\
                d^*_{i - 1} + 1 & \quad\textrm{if } 2 \leq i \leq d_1 + 1 \\
                d^*_{i} & \quad\textrm{if } d_1 + 2 \leq i \leq n
            \end{cases}.
        \end{equation*}
        Note that $d_G(v_i) = d_i$, so $(d_n)$ is graphic.
    \end{proof}
\end{thmbox}
By Theorem \ref{thm:graphicSeqChar}, we can reduce a sequence recursively until we reach some obvious case. This obvious case is graphic if and only if the original sequence is graphic. We can also build the graph corresponding to the original sequence from a simple graphic sequence.

Lastly, we introduce a special type of graphs known as \textit{self-complementary} graphs.
\begin{dfnbox}{Self-Complementary Graph}{selfComp}
    A graph $G$ is said to be {\color{red} \textbf{self-complementary}} if $G \cong \overline{G}$.
\end{dfnbox}
It turns out that a self-complementary graph satisfies some special properties.
\begin{probox}{Order of Self-Complementray Graphs}{selfCompOrder}
    If $G$ is a self-complementary graph of order $n$, then $n = 4k$ or $n = 4k + 1$ for some~$k \in \Z^+$.
    \tcblower
    \begin{proof}
        Since $G$ is self-complementary, $G \cong \overline{G}$ and so $e(G) = e\left(\overline{G}\right)$. Note that
        \begin{equation*}
            e(G) + e\left(\overline{G}\right) = \begin{pmatrix}
                n \\
                2
            \end{pmatrix} = \frac{n(n - 1)}{2},
        \end{equation*}
        so we have $e(G) = \frac{n(n - 1)}{4}$. Since $e(G) \in \Z^+$, either $n$ or $n - 1$ is divisible by $4$, so~$n = 4k$ or $n = 4k + 1$ for some $k \in \Z^+$.
    \end{proof}
\end{probox}
\section{Graph Traversal}
In this chapter, we study the movement within a graph, that is, how to traverse the vertices in a graph along its edges.
\subsection{Path and Connectedness}
\begin{dfnbox}{Walk, Trail, Path}{wtp}
    Let $G$ be a graph and $x, y \in V(G)$, then an {\color{red} \textbf{$x$-$y$ walk}} is an alternating sequence
    \begin{equation*}
        W \coloneqq v_0e_1v_1e_2v_2\cdots v_{n - 1}e_nv_n
    \end{equation*}
    where $v_i \in V(G)$, $e_i = v_{i - 1}v_i \in E(G)$, $x = v_0$ and $y = v_n$. In particular, $W$ is {\color{red} \textbf{open}} if $v_0 \neq v_n$ and {\color{red} \textbf{closed}} otherwise. The {\color{red} \textbf{length}} of a walk is defined as the number of edges in it.
    \\\\
    If $e_i \neq e_j$ whenever $i \neq j$, then $W$ is called a {\color{red} \textbf{trail}}. A closed trail is called a {\color{red} \textbf{circuit}}. If $v_i \neq v_j$ whenever $i \neq j$, then $W$ is called a {\color{red} \textbf{path}}. A closed path is called a {\color{red} \textbf{cycle}}.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        All paths are trails and all trails are walks, but the converses are not true.
    \end{remark}
\end{notebox}
In more informal terms, a trail is a walk with no repeated edge and a path is a trail with no repeated vertex.

Note that a path is essentially a gragh. We denote a path of order $n$ by $P_n$ and a cycle of order $n$ by $C_n$.

A common tool in proofs of graph theory is the \textbf{extremal approach}, which can be demonstrated in the proposition below.
\begin{probox}{}{}
    If a graph $G$ conatins a $u$-$v$ walk of length $k$, then $G$ contains a $u$-$v$ path of length at most $k$.
    \tcblower
    \begin{proof}
        Let $S$ be the set of all $u$-$v$ walks in $G$, Note that $S \neq \varnothing$ since there is a $u$-$v$ walk of length $k$. Let $P$ be a walk of the shortest length. We claim that $P$ must be a path.
        \\\\
        Suppose $P$ is not a path, then there exists some vertices $w_1 = w_2$ in $P$. Suppose $Q$ is obtained by removing the $w_1$-$w_2$ walk from $P$, then $Q$ is a walk. However,~$Q$ is shorter than $P$ which is not possible. Therefore, $P$ must be a path.
        \\\\
        Note that the length of $P$ is at most $k$, so $G$ contains a $u$-$v$ path of length at most $k$.
    \end{proof}
\end{probox}
An important question we are interested in with graphs is the reachability of a vertex, i.e., given two vertices $u$ and $v$, we want to know whether we can reach one vertex from another. Intuitively, we can traverse between two vertices if there is a path between them. From here we define the notion of a \textit{connected graph}.
\begin{dfnbox}{Connected Graph}{connected}
    Let $G$ be a graph. Two vertices $u$ and $v$ are {\color{red} \textbf{connected}} if there is a path betweem them. $G$ is said to be {\color{red} \textbf{connected}} if for any $u, v \in V(G)$, there exists a path from $u$ to $v$. For any $u \in V(G)$, we denote the set of all vertices connected to $u$ (inclusive of $u$) as $c(u)$.
\end{dfnbox}
Note that connectedness is an equivalence relation. Let $P$ be a partition formed by the equivalence classes of $G$ under the connectedness relation, then for any $u, v \in V(G)$, $u$ and~$v$ are in the same equivalence class if and only if $u$ and $v$ are connected. Any $c(u) \in P$ induces a subgraph $[c(u)]$.
\begin{dfnbox}{Connected Component}{CC}
    Let $G$ be a graph and $R$ be a relation such that for any $u, v \in V(G)$, $uRv$ if and only if~$u$ and $v$ are connected. Let
    \begin{equation*}
        C \coloneqq V(G)/R
    \end{equation*}
    be the quotient set, then any $c(u) \in C$ is known as a (connected) {\color{red} \textbf{component}} of $G$.
    \\\\
    The number of components of $G$ is denoted by $\omega(G)$.
\end{dfnbox}
It can be easily seen that $G$ is connected if and only if $\omega(G) = 1$.

We will now establish a relationship between connectedness and complementation.
\begin{thmbox}{Connectedness of Complement}{connectedComp}
    If $G$ is disconnected, then $\overline{G}$ is connected.
    \tcblower
    \begin{proof}
        Let $u, v \in V\left(\overline{G}\right)$ be two arbitrary vertices. If $uv \notin E(G)$, then $uv \in \left(\overline{G}\right)$ and so $u$ and $v$ are connected in $\overline{G}$.
        \\\\
        If $uv \in E(G)$, since $G$ is disconnected, there exists some $w \in V(G)$ such that~$uw, wv \notin E(G)$. Therefore, $uw, wv \in E\left(\overline{G}\right)$. This means that there is a~$u$-$v$ path in $\overline{G}$ and so $u$ and $v$ are connected in $\overline{G}$.
        \\\\
        Therefore, $\overline{G}$ is connected.
    \end{proof}
\end{thmbox}
Intuitively, we can transform a connected graph into a disconnected one by deleting some vertices or edges. It can be easily observed that in certain graphs, deleting one particular vertex or edge will immediately disconnect the graph.
\begin{dfnbox}{Cut-Vertex, Bridge}{cutNBridge}
    Let $G$ be a non-trivial graph. $v \in V(G)$ is called a {\color{red} \textbf{cut-vertex}} of $G$ if $\omega(G - v) > \omega(G)$. $e \in E(G)$ is called a {\color{red} \textbf{bridge}} of $G$ if $\omega(G - e) > \omega(G)$.
\end{dfnbox}
Note that a graph does not have to be connected to have cut vertices and bridges. Essentially, a cut-vertex (or bridge) is just a vertex (or edge) which disconnects a component upon removal.

Now the next question is how do we identify a cut-vertex or a bridge in a graph? Intuitively, a cut-vertex divides a graph into two portions such that traversal between the two portions has to pass through it.
\begin{thmbox}{Cut-Vertex Characterisation}{cutVertex}
    Let $G$ be a graph. $v \in V(G)$ is a cut-vertex if and only if there exists $a, b \in V(G)$ such that $v$ is in every path between $a$ and $b$.
    \tcblower
    \begin{proof}
        Let $v \in V(G)$ be a cut-vertex and consider $H = G - v$, then we can find two non-empty disjoint connected components of $H$, say $H_1$ and $H_2$. Now, take $a \in V(H_1)$ and $b \in V_(H_2)$, then they are disconnected. However, $a$ and $b$ are connected in $G$, which means $v$ is in every $a$-$b$ path.
        \\\\
        We will prove the converse by contrapositive. Suppose $v$ is not a cut-vertex of $G$, then $G - v$ is connected. Therefore, for any $u, w \in V(G)$, there is a path between them which does not contain $v$.
    \end{proof}
\end{thmbox}
A similar argument can be established for bridges. Furthermore, since we have to pass through the bridge when traversing between the two portions, it can be easily seen that we have to reuse the bridge in order to traverse back.
\begin{thmbox}{Bridge Characterisation}{bridge}
    Let $G$ be a graph. $e \in E(G)$ is a bridge if and only if $e$ is not contained by any cycle in $G$.
    \tcblower
    \begin{proof}
        We will prove the forward direction by contrapositive. Suppose $e = xy$ is contained in some cycle in $G$, then there is some $x$-$y$ path in $G - e$. Suppose $a, b \in V(G)$ are connected in $G$ via a path containing $e$, this implies that there is some $a$-$b$ path in $G - e$ and so $G - e$ is connected, which means that $e$ is not a bridge.
        \\\\
        We will prove the converse also by contrapositive. Suppose $e = xy$ is not a bridge, the $G - e$ is connected and so there is some $x$-$y$ path in $G - e$. Let this path be $P$, then $P \cup \{e\}$ is a cycle in $G$.
    \end{proof}
\end{thmbox}
Combining the two characterisations, we can relate cut-vertices to bridges in the following proposition, the proof of which is left to the reader as an exercise:
\begin{probox}{}{cutVertexNBridge}
    Let $G$ be a graph. If $uv \in E(G)$ is a bridge and $u$ is not an end vertex, then $u$ is a cut-vertex.
\end{probox}
A direct consequence of this is the following corollary:
\begin{corbox}{}{}
    If $G$ is a graph with order at least $3$ and $G$ contains a bridge, then $G$ contains a cut-vertex.
\end{corbox}
\section{Weighted Graphs}
Given a graph $G$ with $u$, $v$ being two connected vertices, we are interested in the notion of distance between them. Intuitively, if there are multiple paths between $u$ and $v$, we would take the length of the shortest one to represent their distance.
\begin{dfnbox}{Distance, Eccentricity, Diameter}{distEDia}
    Let $G$ be a connected graph and let $u, v \in V(G)$. The {\color{red} \textbf{distance}} between $u$ and $v$, denoted by $d(u, v)$, is the length of the shortest path between $u$ and $v$. The {\color{red} \textbf{eccentricity}} of $u$ is defined to be
    \begin{equation*}
        e(u) = \max_{v \in V(G)}\{d(u, v)\}.
    \end{equation*}
    The {\color{red} \textbf{diameter}} of $G$ is defined by
    \begin{equation*}
        \mathrm{diam}(G) = \max_{u \in V(G)}\{e(u)\}.
    \end{equation*}
    The {\color{red} \textbf{radius}} of $G$ is defined by
    \begin{equation*}
        \mathrm{rad}(G) = \min_{u \in V(G)}\{e(u)\}.
    \end{equation*}
    A vertex $v$ is called a {\color{red} \textbf{central}} vertex if
    \begin{equation*}
        e(v) = \mathrm{rad}(v).
    \end{equation*}
    The subgraph induced by the set of central vertices of $G$ is known as the {\color{red} \textbf{centre}} of $G$.
\end{dfnbox}
This also justifies the use of words ``diameter'' and ``radius'' in a circle. We can view a circle as a graph consisting of a centre and infinitely many vertices in the circumference, with edges of length $r$ connecting the centre to the circumference. Indeed, in this definition, the farthest vertices will be any two on the circumference with a distance of $2r$, and the nearest vertices will be the centre and any vertex on the circumference with a distance of $r$.

For readers with knowledge in real analysis or topology, it is easy to see that an undirected unweighted connected graph is a metric space with distance between vertices as its metric. Thus it is natural to have the following result:
\begin{thmbox}{Triangle Inequality}{triIneq}
    If $G$ is connected, then for any $u, v, w \in V(G)$,
    \begin{equation*}
        d(u, v) \leq d(u, w) + d(w, v).
    \end{equation*}
    \tcblower   
    \begin{proof}
        Let $U$, $V$ be the shortest $u$-$w$ path and shortest $w$-$v$ path respectively, then~$U + V$ is a $u$-$v$ walk with length $d(u, w) + d(w, v)$. Since the shortest $u$-$v$ path has length $d(u, v)$, we have
        \begin{equation*}
            d(u, v) \leq d(u, w) + d(w, v).
        \end{equation*}
    \end{proof}
\end{thmbox}
An application of Theorem \ref{thm:triIneq} allows us to establish the following:
\begin{thmbox}{Boundedness of Diameter}{boundedDiam}
    If $G$ is connected, then
    \begin{equation*}
        \mathrm{rad}(G) \leq \mathrm{diam}(G) \leq 2\mathrm{rad}(G).
    \end{equation*}
    \tcblower
    \begin{proof}
        $\mathrm{rad}(G) \leq \mathrm{diam}(G)$ is immediate from Definition \ref{dfn:distEDia}. Take $u, v \in V(G)$ with~$d(u, v) = \mathrm{diam}(G)$ and take $w \in V(G)$ such that $e(w) = \mathrm{rad}(G)$. Notice that this implies
        \begin{align*}
            d(u, w) & \leq e(w) = \mathrm{rad}(G) \\
            d(w, v) & \leq e(w) = \mathrm{rad}(G).
        \end{align*}
        By Theorem \ref{thm:triIneq}, 
        \begin{equation*}
            \mathrm{diam}(G) = d(u, v) \leq d(u, w) + d(w, v) = 2\mathrm{rad}(G).
        \end{equation*}
    \end{proof}
\end{thmbox}
The notion of distance between vertices can be abstracted into a concept known as edge (vertex) weight. So far we have been dealing with graphs in which edges do not differ from one another except the vertices they are incident to. However, in real life, there are many scenarios where some egdes in a graph are ``more important'' than others. For example, when building roads to connect villages, we wish to choose edges with the lowest cost. Therefore, we introduce the notion of \textit{weighted graphs}
\begin{dfnbox}{Weighted Graph}{weightedGraph}
    A {\color{red} \textbf{weighted graph}} is a graph $G$ with a function $w \colon E(G) \to \R$. For any $e \in E(G)$,~$w(e)$ is called the {\color{red} \textbf{weight}} of $e$. If $H$ is a subgraph of $G$, the {\color{red} \textbf{weight}} $H$ is defined to be
    \begin{equation*}
        \sum_{e \in E(H)}w(e).
    \end{equation*}
\end{dfnbox} 
One classic problem on weighted graphs is the (single-source) shortest path problem.
\subsection{(Single-Source) Shortest Path}
In an unweighted graph, we define the length of a path to be the number of edges it contains. However, in a weighted graph we need a different definition to distinguish edges of different weights.
\begin{dfnbox}{Length of a Weighted Path}{weightedPathLen}
    Let $G$ be a weighted graph and $P$ be a path in $G$. The {\color{red} \textbf{length}} of $P$ is defined as $w(P)$.
\end{dfnbox}
Similarly, we can define the shortest path in a weighted graph as the path with the lowest weight, and the distance between two vertex as the weight of the shortest path between them. The (single-source) shortest path problem asks:
\begin{quote}
    Let $G$ be a weighted graph and $s$ be the source vertex. For any $v \in V(G)$, what is the distance between $s$ and $v$?
\end{quote}
Let us consider the following idea: suppose we are already certain that the distance from $s$ to $u$ cannot be further reduced, then we can safely take the shortest edge from $u$ to $u'$ and compute the weight of the $s$-$u'$ path containing $u$. If this path is shorter than the known distance from $s$ to $u'$, we will ``relax'' this distance. A common and famous algorithm to solve a (single-source) shortest path problem is the \textit{Dijkstra's algorithm}.
\begin{tecbox}{Dijkstra's Algorithm}{dijkstra}
    Let $G$ be a weighted graph and $u_0$ be the source.
    \begin{itemize}
        \item Define $L \colon V(G) \to \R$: A function which maps a vertex in $G$ to the current distance between it and the source vertex. 
        \item Initialise $L(u_0) = 0$ and $L(u_i) = \infty$ for all other vertices.
        \item Declare all vertices as temporary.
        \item Let $v$ be the vertex such that $L(v)$ is the lowest among all temporary vertices. Declare $v$ as final.
        \item For each neighbour $u_i$ of $v$:
        \begin{itemize}
            \item If $w(vu_i) + L(v) < L(u_i)$, relax $L(u_i)$.
        \end{itemize}
        \item Repeat the process until all vertices have become final.
    \end{itemize} 
\end{tecbox}
In real programming, the notion of ``temporary'' and ``final'' vertices can be easily implemented with a priority queue to achieve logarithmic complexity in distance update.

\section{Graphs and Matrices}
So far we have been using hand-drawn illustrations to aid us in graph visualisation. However, for large graphs with many vertices and egdes, drawing out the graph structures becomes nearly impossible. Therefore, we need to use some algebraic tools to help us represent a large graph.
\begin{dfnbox}{Adjacency Matrix}{adMat}
    Let $G$ be a multigraph with $v(G) = n$, then the {\color{red} \textbf{adjacency matrix}} denoted by $\mathbfit{A}(G)$ is an $n \times n$ matrix such that $a_{ij}$ is the number of edges incident to both $v_i, v_j \in V(G)$.
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        For an undirected multigraph, $\mathbfit{A}$ is always symmetric.
    \end{remark}
\end{notebox}
Note that an adjacency matrix does not tell us which edges are incident to a particular vertex, so we may consider the following alternative:
\begin{dfnbox}{Incidence Matrix}{inMat}
    Let $G$ be a multigraph with $v(G) = n$ and $e(G) = m$, then the {\color{red} \textbf{incidence matrix}} denoted by $\mathbfit{M}(G)$ is an $n \times m$ matrix such that
    \begin{equation*}
        m_{ij} = \begin{cases}
            1, &\textrm{if } e_j \textrm{ is incident with } v_i \\
            0, &\textrm{otherwise}
        \end{cases}.
    \end{equation*}
\end{dfnbox}
\begin{notebox}
    \begin{remark}
        For an undirected multigraph, consider an arbitrary $k$-th column of $\mathbfit{M}$. Note that any column will have exactly $2$ non-zero entries. Suppose $m_{ik} = m_{jk} = 1$, it means that $e_k = v_iv_j$.
    \end{remark}
\end{notebox}
For readers who have learnt about basic data structures in computer science, an incidence matrix is just an adjacency list with each list of neighbours converted to a (non-compact) boolean array.
\\\\
Note that the sum of the $i$-th row for both $\mathbfit{A}$ and $\mathbfit{M}$ is the degree of vertex $i$. 
\\\\
Since $\mathbfit{A}$ and $\mathbfit{M}$ carries information about adjacencies between vertices, we can use them to draw some conclusions regarding graph traversal.
\begin{probox}{}{numWalks}
    Let $G$ be a simple graph of order $n$ with adjacency matrix $\mathbfit{A}$, then the $(i, j)$ entry of $\mathbfit{A}^k$ is the number of $v_i$-$v_j$ walks of length $k$.
    \tcblower   
    \begin{proof}
        The case where $k = 1$ is trivially true since there is a $v_i$-$v_j$ walk of length $1$ if and only if $v_i$ and $v_j$ are neighbours.
        \\\\
        Let the $(i, j)$ entry of $\mathbfit{A}^m$ be $m_{ij}$. Suppose $m_{ij}$ is the number of $v_i$-$v_j$ walks of length $m$. Then the $(i, j)$ entry of $\mathbfit{A}^{m + 1}$ is given by
        \begin{align*}
            \sum_{r = 1}^{n}m_{ir}a_{rj}.
        \end{align*}
        Notice that $m_{ir}$ is the number of $v_i$-$v_r$ walks of length $m$ and $a_{rj}$ is the number of $v_r$-$v_j$ walks of length $1$, so by Theorem \ref{thm:MP}, $m_{ir}a_{rj}$ is the number of $v_i$-$v_j$ walks of length~$n$ that passes through $v_r$. Therefore, summing up $m_{ir}a_{rj}$ for $r = 1, 2, \cdots, n$ gives the total number of $v_i$-$v_j$ walks of length $n + 1$.
    \end{proof}
\end{probox}
Additionally, we have the following corollary:
\begin{corbox}{}{}
    Let $G$ be a graph with $v(G) \geq 2$ and adjacency matrix $\mathbfit{A}$. Let
    \begin{equation*}
        \mathbfit{B} = \sum_{i = 1}^{n - 1}\mathbfit{A}^i,
    \end{equation*}
    then $G$ is connected if and only if $b_{ij} \neq 0$ for all $i \neq j$.
\end{corbox}

\section{Bipartite Graphs}
\begin{dfnbox}{Bipartite Graph}{bipartite}
    A graph $G$ is {\color{red} \textbf{bipartite}} if $V(G)$ can be partitioned into two disjointed subsets $V_1$ and~$V_2$ such that every edge of $G$ joins a vertex in $V_1$ to a vertex in $V_2$. $(V_1, V_2)$ is known as a {\color{red} \textbf{bipartition}} of $G$.
\end{dfnbox}
Note that the size of a bipartite graph $G$ with partite sets $V_1$ and $V_2$ is just
\begin{equation*}
    e(G) = \sum_{x \in V_1}d_G(x) = \sum_{y \in V_2}d_G(y).
\end{equation*}
It can be deduced that the maximal size of a bipartite graph is achieved when every vertex in $V_1$ is connected to every vertex in $V_2$. To define this rigorously, we first introduce the notion of \textit{join}.
\begin{dfnbox}{Join}{join}
    Let $G_1$ and $G_2$ be two disjoint graphs of order $n_1$ and $n_2$ respectively. The {\color{red} \textbf{join}} of $G_1$ and $G_2$, deonoted by $G_1 + G_2$ is the graph such that
    \begin{align*}
        V(G_1 + G_2) & = V(G_1) \cup V(G_2) \\
        E(G_1 + G_2) & = E(G_1) \cup E(G_2) \cup \left\{uv \colon u \in V(G_1), v \in V(G_2)\right\}.
    \end{align*}
\end{dfnbox}
Informally, we can describe the join of $G_1$ and $G_2$ as a graph which contains all the vertices from $G_1$ and $G_2$, all existing edges from~$G_1$ and $G_2$, and new edges connecting every vertex in $G_1$ to every vertex in $G_2$. Thus, we can define a complete bipartite graph using a join.
\begin{dfnbox}{Complete Bipartite Graph}{completeBipartite}
    A {\color{red} \textbf{complete bipartite graph}} is defined as $K_{p, q} = 0_p + 0_q$.
\end{dfnbox}
Sometimes, it is not easy to identify the bipartition in a bipartite graph. Therefore, to quickly determine whether a graph is bipartite, we consider the following theorem:
\begin{thmbox}{Bipartite Graph Characterisation}{bipartiteIff}
    A graph is bipartite if and only if it contains no odd cycles.
    \tcblower
    \begin{proof}
        Suppose $G$ is bipartite with bipartition $(U, V)$. If $G$ contains no cycle then we are done. Suppose $G$ contains some cycle. Let $u_i$ be the $i$-th visited vertex from $U$ in the cycle. Note that between $u_i$ and $u_{i + 1}$ there exists one and only one vertex from $V$. Suppose $k$ vertices from $U$ are contained in the cycle, then it is easy to see that there are also $k$ vertices from $V$ contained in the cycle. Therefore, every cycle in $G$ must be even.
        \\\\
        Suppose conversely that $G$ contains no odd cycles. Note that it suffices to prove that every component of $G$ is bipartite. Let $C$ be a component of $G$, then $C$ contains no odd cycles. Take some $w \in V(C)$ and define the sets
        \begin{align*}
            V_1 & \coloneqq \left\{v \colon d(v, w) \textrm{ is even}\right\} \\
            V_2 & \coloneqq \left\{v \colon d(v, w) \textrm{ is odd}\right\}.
        \end{align*}
        We will prove $(V_1, V_2)$ is a bipartition. Suppose it is not a bipartition, then without loss of generality, let $a, b \in V_1$ be neighbours. Note that there is a path $A$ between $w$ and $a$ of even length and a path $B$ between $w$ and $b$ of even length, so $A \cup B \cup \{ab\}$ is a closed walk of odd length. Consider the following lemma:
        \begin{lembox}{}{existOddCycle}
            A closed walk of odd length in a graph contains an odd cycle.
            \tcblower
            \begin{proof}
                Let the walk have a length of $p$. The case where $p = 3$ is trivially true since the only closed walk of length $3$ is $C_3$. 
                \\\\
                Suppose any closed walk of odd length less than $p$ contains an odd cycle. Let $P$ be a closed walk of length $p$. If $P$ is a cycle then we are done. If $P$ is not a cycle, then there exist some $w_1 = w_2$ in $P$. Note that $w_1 = w_2$ is a cut-vertex. So there exists two closed walks $Q_1$ and $Q_2$ such that $Q_1 \cap Q_2 = \{w_1\}$. Notice that one of $Q_1$ and $Q_2$ must have an odd length and so contains an odd cycle, so $P$ contains an odd cycle.
            \end{proof}
        \end{lembox}
        By Lemma \ref{lem:existOddCycle}, $A \cup B \cup \{ab\}$ contains an odd cycle, which is a contradiction. 
    \end{proof}
\end{thmbox}

\section{Trees}
\begin{dfnbox}{Tree}{tree}
    A {\color{red} \textbf{tree}} is a {\color{red} \textbf{connected}} graph with {\color{red} \textbf{no cycles}}.
\end{dfnbox}
Consider the following question:
\begin{quote}
    What is the minimal number of edges needed to construct a connected graph of order $n$?
\end{quote}
Intuitively, we need at least $(n - 1)$ edges. On the other hand, it is also easy to see that a tree of order $n$ is of the smallest size among all order-$n$ graphs.
\begin{thmbox}{Equivalent Definitions for Trees}{equivDfnTree}
    Let $T$ be a connected graph of order $n$ and size $m$, then the following statements are equivalent:
    \begin{enumerate}
        \item $T$ is a tree;
        \item Every two vertices in $T$ are joined by a unique path;
        \item $m = n - 1$.
    \end{enumerate}
\end{thmbox}
We introduce $2$ interesting properties of trees.
\begin{probox}{}{numDeg1}
    Let $T$ be a tree with $\Delta(T) = k$. Let $n_i$ be the number of vertices in $T$ of degree $i$, then
    \begin{equation*}
        n_1 = 2 + \sum_{i = 1}^{k - 2}in_{i + 2}.
    \end{equation*}
    \tcblower
    \begin{proof}
        Let the order of $T$ be $n$, then $n = \sum_{i = 1}^{k}n_i$. By Lemma \ref{lem:handshake}, we have
        \begin{equation*}
            \sum_{v \in V(T)}d(v) = 2(n - 1).
        \end{equation*}
        However, since there are $n_i$ vertices with degree $i$, we have
        \begin{equation*}
            \sum_{v \in V(T)}d(v) = \sum_{i = 1}^{k}in_i.
        \end{equation*}
        Therefore,
        \begin{align*}
            2\sum_{i = 1}^{k}n_i - 2 & = \sum_{i = 1}^{k}in_i \\
            n_1 & = \sum_{i = 3}^{k}in_i \\
            n_1 & = \sum_{i = 1}^{k - 2}in_{i + 2}.
        \end{align*}
    \end{proof}
\end{probox}
The following corollary is an immediate consequence of Proposition \ref{pro:numDeg1}:
\begin{corbox}{}{}
    A tree of order at least $2$ contains at least $2$ end vertices.
\end{corbox}
A tree with exactly $2$ end vertices is a doubly linked list.

A collection of disjoint trees are vividly named as a \textit{forest}.
\begin{dfnbox}{Forest}{forest}
    A {\color{red} \textbf{forest}} is a graph $G$ in which each component is a tree.
\end{dfnbox}

\subsection{Spanning Tree}
\begin{dfnbox}{Spanning Tree}{spanTree}
    Let $G$ be a multigraph, a tree $T$ which is a spanning graph of $G$ is known as the {\color{red} \textbf{spanning tree}} of $G$.
\end{dfnbox}
Intuitively, one can traverse along a spanning tree of a graph to reach every vertex of the graph from any source vertex. Therefore, a spanning tree essentially gives all paths between any pair of vertices, and so the existence of a spanning tree is an indicator of connectedness.
\begin{probox}{}{}
    A graph $G$ is connected if and only if it contains a spanning tree.
    \tcblower
    \begin{proof}
        If $G$ contains a spanning tree, then every pair of vertices in $G$ are connected, and so $G$ is connected.
        \\\\
        Suppose $G$ is connected. If $G$ is a tree, then it is a spanning tree of itself. Otherwise, $G$ contains some cycle $C$. Let $e$ be any edge in $C$, then $e$ is not a bridge so~$G - e$ is connected. Set $G' = G - e$ and repeat the process for $G'$. Since $G$ is finite, we will eventually remove all cycles in $G$ and derive a tree containing all vertices in $G$, i.e., a spanning tree.
    \end{proof}
\end{probox}
Notice that a connected graph cannot be smaller in size than its spanning tree, so we have the following corollary:
\begin{corbox}{}{}
    If $G$ is a connected graph of order $n$, then $e(G) \geq n - 1$.
\end{corbox}
Now, we wish to answer the following question:
\begin{quote}
    Let $G$ be a connected multigraph of order $n$ and size $m$. Suppose the vertices of $G$ are labelled and duplicated egdes between any pair of vertices distinguished, how many different spanning trees of $G$ there are?
\end{quote}
Let $G$ be a connected multigraph, we denote \textbf{the number of spanning trees} of $G$ by $\tau(G)$. It is not an easy task to count directly by observation. We introduce a useful concept known as \textit{contraction}.
\begin{dfnbox}{Contraction}{contraction}
    Let $G$ be a connected multigraph of order $n$ and let $e = uv$ be an egde of $G$. A {\color{red} \textbf{contraction}} of $G$ by $e$, denoted by $G \circ e$, is produced by removing all edges joining $u$ and~$v$ and identifying $u$ and $v$ into a single vertex.
\end{dfnbox}
We may make use of contractions to compute $\tau(G)$ in a recursive manner.
\begin{thmbox}{Recursive Algorithm for $\tau(G)$}{recursiveTau}
    Let $G$ be a connected multigraph, then
    \begin{equation*}
        \tau(G) = \tau(G - e) + \tau(G \circ e)
    \end{equation*}
    for all $e \in E(G)$.
    \tcblower
    \begin{proof}
        Let $G$ be a multigraph and $e \in E(G)$. Define $A$ to be the set of spanning trees of $G$ which contains $e$ aand $B$ to be that which does not contain $e$.
        \\\\
        Take any $T_A \in A$, then $e \in E(T_A)$ is a bridge. Notice that if $T_1, T_2 \in A$, then~$T_1 \neq T_2$, so they have at least one different edge which is not $e$. Define
        \begin{equation*}
            A' \coloneqq \left\{T'_A = T_A - e \colon T_A \in A\right\}.
        \end{equation*}
        Clearly, for $T'_1, T'_2 \in A'$, $T'_1 \neq T'_2$ are different spanning trees.Observe that there is a bijection between $A$ and $A'$, so $\abs{A} = \abs{A'} = \tau(G \circ e)$.
        \\\\
        Take any $T_B \in B$, since $e \notin E(T_B)$, $T_B$ must be a spanning tree of $G - e$. Therefore,~$\abs{B} = \tau(G - e)$. Hence,
        \begin{equation*}
            \tau(G) = \abs{B} + \abs{A} = \tau(G - e) + \tau(G \circ e).
        \end{equation*}
    \end{proof}
\end{thmbox}
Theorem \ref{thm:recursiveTau} allows us to reduce $\tau(G)$ for a complex graph $G$ into a linear combination of~$\tau$'s for simpler graphs. So now we will discuss some basic cases where $\tau$ is easy to compute.
\begin{enumerate}
    \item A simple cycle of order $n$:
    
    Note that for any $e \in E(C_n)$, $C_n - e$ is a spanning tree, so $\tau(C_n) = e(C_n) = n$.
    \item A connected graph $G$ with a cut-vertex $v$:
    
    Let $G_1$ and $G_2$ be two components of $G - v$. A spanning tree of $G$ consists of a spanning tree of $\bigl[V(G_1) \cup \{v\}\bigr]$ and a spanning tree of $\bigl[V(G_2) \cup \{v\}\bigr]$, so 
    \begin{equation*}
        \tau(G) = \tau\Bigl(\bigl[V(G_1) \cup \{v\}\bigr]\Bigr)\tau\Bigl(\bigl[V(G_2) \cup \{v\}\bigr]\Bigr).
    \end{equation*}
    \item A connected graph $G$ with a bridge $e$:
    
    By a similar argument as the previous case, let $G_1$ and $G_2$ be the two components of $G - e$, then $\tau(G) = \tau(G_1)\tau(G_2)$.
    \item A connected graph $G$ consisting of two simple cycles $C_p$ and $C_q$ sharing a common edge $e$:
    
    Note that $G - e$ is a simple cycle $C_{p + q - 2}$ and $G \circ e$ is two simple cycles $C_{p - 1}$ and $C_{q - 1}$ sharing a cut-vertex. By Theorem \ref{thm:recursiveTau},
    \begin{align*}
        \tau(G) & = \tau(G - e) + \tau(G \circ e) \\
        & = p + q - 2 + (p - 1)(q - 1).
    \end{align*}
    \item A simple cycle $C_p$ with exactly one edge $uv$ duplicated.
    
    Let $e$ and $f$ be the duplicated edges. Observe that there is only $1$ spanning tree which contains neither $e$ nor $f$. If exactly one of $e$ and $f$ is part of the spanning tree, that spanning tree also spans a simple cycle $C_p$. Therefore, $\tau(G) = 2(p - 1) + 1 = 2p - 1$. 
    \item A connected graph $G$ consisting of two simple cycles $C_p$ and $C_q$ sharing a pair of duplicated edges $e$ and $f$:
    
    Note that $G - e$ is just the graph in case $4$ and $G \circ e$ is $C_{p - 1}$ and $C_{q - 1}$ joined by a common cut-vertex, so by Theorem \ref{thm:recursiveTau},
    \begin{align*}
        \tau(G) & = \tau(G - e) + \tau(G \circ e) \\
        & = p + q - 2 + 2(p - 1)(q - 1).
    \end{align*}
\end{enumerate}
When dealing with large graphs, we can make use of matrices just like what we did for graph traversal. In particular, we shall state the following useful theorem without proof.
\begin{thmbox}{Matrix Tree Theorem}{matTree}
    Let $\mathbfit{A}$ be the adjacency matrix of a multigraph $G$ and $\mathbfit{C}$ be the $n \times n$ diagonal matrix defined by
    \begin{equation*}
        c_{ij} = \begin{cases}
            d(v_i) & \quad\textrm{if } i = j \\
            0 & \quad\textrm{otherwise}
        \end{cases}.
    \end{equation*}
    Then, $\tau(G)$ is the cofactor of any entry of $\mathbfit{C - A}$. 
\end{thmbox}
Using Theorem \ref{thm:matTree}, we can prove the following propositions:
\begin{probox}{Number of Spanning Trees for Complete Graphs}{completeTau}
    Let $n \in \N$ and $n \geq 2$ and let $K_n$ be a complete graph, then $\tau(K_n) = n^{n - 2}$.
    \tcblower
    \begin{proof}
        Note that since $K_n$ is complete, $\mathbfit{C}(K_n)$ is defined by
        \begin{equation*}
            c_{ij} = \begin{cases}
                n - 1 & \quad\textrm{if } i = j \\
                0 & \quad\textrm{otherwise}
            \end{cases}
        \end{equation*}
        and the adjacency matrix $\mathbfit{A}(K_n)$ is such that
        \begin{equation*}
            a_{ij} = \begin{cases}
                1 & \quad\textrm{if } i \neq j \\
                0 & \quad\textrm{otherwise}
            \end{cases}.
        \end{equation*}
        Therefore, the cofactor of $\mathbfit{C}(K_n) - \mathbfit{A}(K_n)$ will be the $(n - 1) \times (n - 1)$ determinant
        \begin{equation*}
            \begin{vmatrix}
                n - 1 & -1 & -1 & \cdots & -1 \\
                -1 & n - 1 & -1 & \cdots & -1 \\
                -1 & -1 & n - 1 & \cdots & -1 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                -1 & -1 & -1 & \cdots & n - 1
            \end{vmatrix} \longrightarrow \begin{vmatrix}
                1 & 1 & 1 & \cdots & 1 \\
                -1 & n - 1 & -1 & \cdots & -1 \\
                -1 & -1 & n - 1 & \cdots & -1 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                -1 & -1 & -1 & \cdots & n - 1
            \end{vmatrix},
        \end{equation*}
        where the latter is obtained by adding all other rows to the first row. Performing row operations by adding the first row to every other row, the determinant becomes
        \begin{equation*}
            \begin{vmatrix}
                1 & 1 & 1 & \cdots & 1 \\
                0 & n & 0 & \cdots & 0 \\
                0 & 0 & n & \cdots & 0 \\
                \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & 0 & \cdots & n
            \end{vmatrix} = n^{n - 2}.
        \end{equation*}
        Since such row operations do not change the determinant, $\tau(K_n) = n^{n - 2}$.
    \end{proof}
\end{probox}
\begin{probox}{Number of Spanning Trees for Complete Bipartite Graphs}{completeBipartiteTau}
    For all $r \in \Z^+$, let $K_{2, r}$ be a complete bipartite graph with partite sets of sizes $2$ and $r$, then $\tau\left(K_{2, r}\right) = 2^{r - 1}r$.
    \tcblower
    \begin{proof}
        Note that
        \begin{equation*}
            \mathbfit{C}(K_{2, r}) - \mathbfit{A}(K_{2, r}) = \begin{bmatrix}
                r & 0 & -1 & -1 & \cdots & -1 \\
                0 & r & -1 & -1 & \cdots & -1 \\
                -1 & -1 & 2 & 0 & \cdots & 0 \\
                -1 & -1 & 0 & 2 & \cdots & 0 \\
                \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                -1 & -1 & 0 & 0 & \cdots & 2
            \end{bmatrix}.
        \end{equation*}
        Consider the $(r + 2, r + 2)$ cofactor:
        \begin{equation*}
            \begin{vmatrix}
                r & 0 & -1 & -1 & \cdots & -1 \\
                0 & r & -1 & -1 & \cdots & -1 \\
                -1 & -1 & 2 & 0 & \cdots & 0 \\
                -1 & -1 & 0 & 2 & \cdots & 0 \\
                \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                -1 & -1 & 0 & 0 & \cdots & 2
            \end{vmatrix} \longrightarrow \begin{vmatrix}
                1 & 1 & 0 & 0 & \cdots & 0 \\
                0 & r & -1 & -1 & \cdots & -1 \\
                -1 & -1 & 2 & 0 & \cdots & 0 \\
                -1 & -1 & 0 & 2 & \cdots & 0 \\
                \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                -1 & -1 & 0 & 0 & \cdots & 2
            \end{vmatrix},
        \end{equation*}
        where the latter is obtained by adding every other row to the first row. Performing row operations by adding the first row to every row starting from the third row, the determinant becomes
        \begin{equation*}
            \begin{vmatrix}
                1 & 1 & -1 & \cdots & -1 \\
                0 & r & -1 & \cdots & -1 \\
                0 & 0 & 2 & \cdots & 0 \\
                \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & 0 & 0 & \cdots & 2
            \end{vmatrix} = 2^{r - 1}r.
        \end{equation*}
        Since such row operations do not change the determinant, so $\tau(K_{2, r}) = 2^{r - 1}r$.
    \end{proof}
\end{probox}
\subsection{Minimum Spanning Trees}
The minimum spanning tree problem asks the following:
\begin{quote}
    Given a connected graph $G$, what is a spanning tree of $G$ with the smallest possible weight, and what is this smallest weight?
\end{quote}
More formally, we wish to find a spanning tree $T$ of $G$ such that $w(T) \leq w(T')$ for any spanning tree $T'$ of $G$.

Intuitively, if we already have a partial minimum spanning tree of $G$, we can greedily choose the edge with the smallest weight from $E(G)$ such that the spanning tree is maintained after adding this edge. We introduce two famous algorithms for this purpose.

\begin{tecbox}{Kruskal's Algorithm}{kruskal}
    Let $G$ be a graph of order $n$. 
    \begin{itemize}
        \item Define 
        \begin{itemize}
            \item $i$: Current size of the partial spanning tree.
            \item $E_i$: The set of edges contained in the partial spanning tree of size $i$.
        \end{itemize}
        \item Initialise $i = 0$ and $E_0 = \varnothing$.
        \item Select $e \in E(G) - E_i$ such that $w(e)$ is the minimum and $\bigl[E_i \cup \{e\}\bigr]$ contains no cycle.
        \item Set $E_{i + 1} = E_i \cup \{e\}$ and increment $i$.
        \item Repeat the above processes until $i = n - 1$, and $\bigl[E_i\bigr]$ is a minimum spanning tree of $G$.
    \end{itemize}
\end{tecbox}
\begin{tecbox}{Prim's Algorithm}{prim}
    Let $G$ be a graph of order $n$. 
    \begin{itemize}
        \item Define 
        \begin{itemize}
            \item $i$: Current size of the partial spanning tree.
            \item $S_i$: The set of vertices contained in the partial spanning tree of size $i$.
            \item $E_i$: The set of edges contained in the partial spanning tree of size $i$.
        \end{itemize}
        \item Initialise $i = 1$, $S_1 = \{u_1, v_1\}$ and $E_1 = \{u_1v_1\}$ such that $w(u_1v_1) \leq w(e)$ for all~$e \in E(G)$.
        \item Select $uv \in E(G) - E_i$ such that $u \in S_i$, $v\notin S_i$ and $w(uv)$ is the minimum.
        \item Set $S_{i + 1} = S_i \cup \{v\}$, $E_{i + 1} = E_i \cup \{uv\}$ and increment $i$.
        \item Repeat the processes until $i = n$, and $\bigl[E_i\bigr]$ is a minimum spanning tree of $G$.
    \end{itemize}
\end{tecbox}

\end{document}