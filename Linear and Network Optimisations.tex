\documentclass[math, code]{amznotes}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{etoolbox}

\graphicspath{ {./images/} }
\geometry{
    a4paper,
    headheight = 1.5cm
}

\patchcmd{\chapter}{\thispagestyle{plain}}
{\thispagestyle{fancy}}{}{}

\theoremstyle{remark}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{case}{Case}

\newcommand{\map}[3]{#1: #2 \rightarrow #3} % Mapping
\newcommand{\image}[2]{#2\left[#1\right]} % Image
\newcommand{\preimage}[2]{#2\left[#1\right]^{-1}} % Pre-image
\newcommand{\eval}[3]{\left. #1\right\rvert_{#2 = #3}} % Evaluated at
%\newcommand\bigO[1]{\mathcal{O}\left(#1\right)}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\fancyhead[L]{
    Linear and Network Optimisations
}
\fancyhead[R]{
    Lecture Notes
}
\tableofcontents

\chapter{Linear Programming}
\section{Linear Programming}
Recall that in general, an optimisation problem can be formulated as
\begin{align*}
    \min_{\mathbfit{x} \in \R^n} & f(\mathbfit{x}) \\
    \textrm{s.t. } & \mathbfit{x} \in P,
\end{align*}
where $P \subseteq \R^n$ is called the \textit{feasible set} (or \textit{feasible region}).
\begin{dfnbox}{Linear Programming Problem}{LP}
    A {\color{red} \textbf{linear programming}} (LP) problem is an optimisation problem where the objective function $f$ is linear and the feasible set $P$ is a polyhedron.
\end{dfnbox}
We can therefore formulate a linear programming problem as
\begin{align*}
    \min_{\mathbfit{x} \in \R^n} & \mathbfit{c}^{\mathrm{T}}\mathbfit{x} \\
    \textrm{s.t. } & \mathbfit{a}_i^{\mathrm{T}}\mathbfit{x} \leq b_i \quad \textrm{for } i = 1, 2, \cdots, p \\
    & \mathbfit{a}_j^{\mathrm{T}}\mathbfit{x} = b_j \quad \textrm{for } i = 1, 2, \cdots, m,
\end{align*}
where $\mathbfit{c} \in \R^n$ is called the \textit{cost} or \textit{profit} vector, $\mathbfit{a}_i^{\mathrm{T}}\mathbfit{x}$ and $\mathbfit{a}_j^{\mathrm{T}}\mathbfit{x}$ are called the \textit{constraints} and~$\mathbfit{x}$ is known as \textit{decision variables}.

In particular, the following is known as the \textit{standard form} of a linear program:
\begin{align*}
    \min_{\mathbfit{x} \in \R^n} & \mathbfit{c}^{\mathrm{T}}\mathbfit{x} \\
    \textrm{s.t. } & \mathbfit{Ax = b} \\
    & x_i \geq 0, \quad \textrm{for } i = 1, 2, \cdots, m.
\end{align*}
One should realise that a linear program in the standard form can be more easily solved by using linear algebra to find the optimal solution. Note that not every optimisation problem is given in the standard form. Fortunately, we can always convert a linear program into the standard form.

For example, consider the constraint $a_ix_i \leq b_i$. Notice that this is essentially equivalent to $a_ix_i + s_i = b_i$ for some $s_i \geq 0$ known as the \textit{slack variable}. Similarly, $a_jx_j \geq b_j$ can be re-written as $a_jx_j - s_j = b_j$ for ome $s_j \geq 0$.

Note that some of the $x_i$'s may be free variables. In this case, we can convert it to $x_i^+ - x_i^-$ for some $x_i^+, x_i^- \geq 0$. For instance, we can take $x_i^+ = 0$ and $x_i^- > 0$ whenever $x_i < 0$ and vice versa for $x_i > 0$. Note that this correspondence is not unique.
\section{Convex Sets and Functions}
\begin{probox}{}{}
    Let $f_1, f_2, \cdots, f_m \colon \R^n \to \R$ be convex functions, then the function
    \begin{equation*}
        f(\mathbfit{x}) \coloneqq \max_{i = 1, 2, \cdots, m}f_i(\mathbfit{x})
    \end{equation*}
    is convex.
    \tcblower
    \begin{proof}
        Take any $\mathbfit{x} \neq \mathbfit{y} \in \R^n$ and consider $\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}$ for some $\lambda \in [0, 1]$. Note that for each of the $f_i$'s, we have
        \begin{equation*}
            f_i\bigl(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\bigr) \leq \lambda f_i(\mathbfit{x}) + (1 - \lambda)f_i(\mathbfit{y}),
        \end{equation*}
        and so
        \begin{equation*}
            \max_{i = 1, 2, \cdots, m}f_i\bigl(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\bigr) \leq \max_{i = 1, 2, \cdots, m}\left[\lambda f_i(\mathbfit{x}) + (1 - \lambda)f_i(\mathbfit{y})\right].
        \end{equation*}
        Therefore,
        \begin{align*}
            f\bigl(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\bigr) & = \max_{i = 1, 2, \cdots, m}f_i\bigl(\lambda\mathbfit{x} + (1 - \lambda)\mathbfit{y}\bigr) \\
            & \leq \max_{i = 1, 2, \cdots, m}\left[\lambda f_i(\mathbfit{x}) + (1 - \lambda)f_i(\mathbfit{y})\right] \\
            & = \lambda\max_{i = 1, 2, \cdots, m}f_i(\mathbfit{x}) + (1 - \lambda)\max_{i = 1, 2, \cdots, m}f_i(\mathbfit{y}) \\
            & = \lambda f(\mathbfit{x}) + (1 + \lambda)f(\mathbfit{y}).
        \end{align*}
    \end{proof}
\end{probox}

\chapter{The Simplex Method}
\section{Geometry of Linear Programming}
\begin{dfnbox}{Polyhedron}{polyhedron}
    A {\color{red} \textbf{polyhedron}} is defined as the set
    \begin{equation*}
        P \coloneqq \left\{\mathbfit{x} \in \R^n \colon \mathbfit{Ax \leq b}\right\}
    \end{equation*}
    where $\mathbfit{A} \in \R^{m \times n}$ and $\mathbfit{b} \in \R^m$.
\end{dfnbox}
Geometrically, a polyhedral set $P$ can be defined alternatively as a finite intersection of half-planes:
\begin{equation*}
    P \coloneqq \bigcap_{i = 1}^m\left\{\mathbfit{x} \in \R^n \colon \mathbfit{a_i}^{\mathrm{T}}\mathbfit{x} \leq b_i\right\}.
\end{equation*}
\begin{thmbox}{Basic Solution Characterisation}{basicSolnChar}
    A vector $\mathbfit{x}^* \in \R^n$ is a basic solution if and only if
    \begin{itemize}
        \item $\mathbfit{Ax}^* = \mathbfit{b}$, and
        \item There exists an index set $B \subseteq \left\{1, 2, \cdots, n\right\}$ such that the set
        \begin{equation*}
            \left\{\mathbfit{A}_i \colon i \in B\right\}
        \end{equation*}
        is linearly independent and $x^*_j = 0$ for all $j \notin B$, where
        \begin{equation*}
            \mathbfit{A} = \begin{bmatrix}
                \mathbfit{A_1} & \mathbfit{A_2} & \cdots & \mathbfit{A_n}
            \end{bmatrix}.
        \end{equation*}
    \end{itemize}
    \tcblower
    \begin{proof}
        Suppose $B = \left\{B(1), B(2), \cdots, B(m)\right\}$ and let $N = \{1, 2, \cdots, n\} - B$. For each $i \in N$, since $x^*_i = 0$, we have $\mathbfit{e}^{\mathrm{T}}_i\mathbfit{x}^* = 0$. Therefore, the matrix representation for the active constraints is
        \begin{equation*}
            \begin{bmatrix}
                \mathbfit{A} \\
                \mathbfit{e}^{\mathrm{T}}_{N(1)} \\
                \mathbfit{e}^{\mathrm{T}}_{N(2)} \\
                \vdots \\
                \mathbfit{e}^{\mathrm{T}}_{N(n - m)}
            \end{bmatrix}.
        \end{equation*}
        Re-arranging the columns, the above matrix can be re-written as
        \begin{equation*}
            \begin{bmatrix}
                \mathbfit{A}_B & \mathbfit{A}_N \\
                \mathbf{0} & \mathbfit{I}_N
            \end{bmatrix}.
        \end{equation*}
        Note that the columns of $\mathbfit{A}_B$ is linearly independent, so $\det(\mathbfit{A}_B) \neq 0$. Therefore, 
        \begin{equation*}
            \begin{vmatrix}
                \mathbfit{A}_B & \mathbfit{A}_N \\
                \mathbf{0} & \mathbfit{I}_N
            \end{vmatrix} = \det(\mathbfit{A}_B)\det(\mathbfit{I}_N) \neq 0,
        \end{equation*}
        and so the matrix is invertible. Therefore, the rows of the matrix are linearly independent. This means that the set 
        \begin{equation*}
            \left\{\nabla h_i \colon i = 1, 2, \cdots, n\right\}
        \end{equation*}
        is linearly independent. Therefore, $\mathbfit{x}^*$ is a basic feasible solution.
        \\\\
        Suppose conversely that $\mathbfit{x}^*$ is a basic feasible solution, then clearly $\mathbfit{Ax}^* = \mathbfit{b}$. Suppose there are $m$ equality constraints, then we must have $(n - m)$ active active inequality constraints at $\mathbfit{x}^*$, indexed by $N = \left\{N(1), N(2), \cdots, N(n - m)\right\}$, such that the constraints are linearly independent. Therefore, the matrix
        \begin{equation*}
            \begin{bmatrix}
                \mathbfit{A} \\
                \mathbfit{e}^{\mathrm{T}}_{N(1)} \\
                \mathbfit{e}^{\mathrm{T}}_{N(2)} \\
                \vdots \\
                \mathbfit{e}^{\mathrm{T}}_{N(n - m)}
            \end{bmatrix}
        \end{equation*}
        is invertible. Let $B \coloneqq \left\{B(1), B(2), \cdots, B(m)\right\} = \{1, 2, \cdots, n\} - N$ be an index set, then the above matrix can be re-arranged as
        \begin{equation*}
            \begin{bmatrix}
                \mathbfit{A}_B & \mathbfit{A}_N \\
                \mathbf{0} & \mathbfit{I}_N
            \end{bmatrix},
        \end{equation*}
        which is invertible. Therefore, $\left\{\mathbfit{A}_{B(1)}, \mathbfit{A}_{B(2)}, \cdots, \mathbfit{A}_{B(m)}\right\}$ is linearly independent. Note that $x^*_i = 0$ for all $i \in N$.
    \end{proof}
\end{thmbox}
\begin{thmbox}{\small Equivalent Conditions for the Existence of Basic Feasible Solution}{}
    Let $\mathbfit{A} \in \R^{m \times n}$ where $m \geq n$ and 
    \begin{equation*}
        P = \left\{\mathbfit{x} \in \R^n \colon \mathbfit{Ax} \geq \mathbfit{b}\right\} \neq \varnothing.
    \end{equation*}
    The following statements are equivalent:
    \begin{enumerate}
        \item $P$ does not contain any straight line.
        \item $P$ has a basic feasible solution.
        \item $P$ has $n$ linearly independent constraints.
    \end{enumerate}
\end{thmbox}

\section{The Simplex Method Algorithm}
\begin{dfnbox}{Feasible Direction}{feasibleDir}
    Let $P$ be a polyhedron and $\mathbfit{x} \in P$ be a feasible point. A vector $\mathbfit{d}$ is a {\color{red} \textbf{feasible direction}} if $\mathbfit{x} + \lambda\mathbfit{d} \in P$ for some $\lambda > 0$.
\end{dfnbox}
\end{document}